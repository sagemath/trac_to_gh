# Issue 34613: sage is slowing down cypari2 a lot

Issue created by migration from https://trac.sagemath.org/ticket/34850

Original creator: vdelecroix

Original creation time: 2022-12-14 20:10:39

CC:  edgarcosta mkoeppe cremona fredrik.johansson


```
 #!python
import timeit

prog="""
from cypari2 import Pari
pari = Pari(size={size})
pari({cmd})
"""

prog = prog.format(size=2**24, cmd="\"quadclassunit(1 - 2^100)\"")

print("python environment with cypari2:          ",
      timeit.timeit(prog, number=3))

setup="""
from sage.ext.memory import init_memory_functions
init_memory_functions()
"""
print("with sage custom GMP allocation functions:",
      timeit.timeit(prog, setup=setup, number=3))

setup="""
import sage.all
"""
print("with the whole sagemath library lodaded:  ",
      timeit.timeit(prog, setup=setup, number=3))
```

gives

```
$ python pari_timing.py 
python environment with cypari2:           4.871978694998688
with sage custom GMP allocation functions: 4.941096214999561
with the whole sagemath library lodaded:   7.39602135700261
```



---

Comment by dimpase created at 2022-12-15 00:09:04

Could it be just a different memory layout as soon as `sage.all` is imported, causing the memory consumption of the process to increase about 6-fold ?


---

Comment by mmezzarobba created at 2022-12-15 10:22:44

FWIW, I don't see the issue on my system:

```
python environment with cypari2:           3.9759535739940475
with sage custom GMP allocation functions: 4.250082928003394
with the whole sagemath library lodaded:   3.8536450660030823
```

(edit: I previously posted a version where I was running the test code under sage, not ipython, but the results above are with ipython)


---

Comment by vdelecroix created at 2022-12-15 10:26:05

Thanks Marc for running the test. What is your configuration? eg what is the system and does sage use system pari?


---

Comment by mmezzarobba created at 2022-12-15 10:33:35

I was going to add more details after doit `sage -f pari`—which is still running at the moment. But the example above is already using sage's pari (I also have a systemwide pari, but it is a different version). This is on Debian sid with an Intel cpu. Note that I am using Python 3.11 via #33842.


---

Comment by mantepse created at 2022-12-15 11:30:00

on ubuntu 22.04.01 LTS:

```
┌────────────────────────────────────────────────────────────────────┐
│ SageMath version 9.8.beta5, Release Date: 2022-12-11               │
│ Using Python 3.10.6. Type "help()" for help.                       │
└────────────────────────────────────────────────────────────────────┘
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Warning: this is a prerelease version, and it may be unstable.     ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
sage: import timeit
....: 
....: prog="""
....: from cypari2 import Pari
....: pari = Pari(size={size})
....: pari("{cmd}")
....: """
....: 
....: prog = prog.format(size=2**24, cmd="quadclassunit(1 - 2^100)")
....: 
....: print("python environment with cypari2:          ",
....:       timeit.timeit(prog, number=3))
....: 
....: setup="""
....: from sage.ext.memory import init_memory_functions
....: init_memory_functions()
....: """
....: print("with sage custom GMP allocation functions:",
....:       timeit.timeit(prog, setup=setup, number=3))
....: 
....: setup="""
....: import sage.all
....: """
....: print("with the whole sagemath library lodaded:  ",
....:       timeit.timeit(prog, setup=setup, number=3))
python environment with cypari2:           5.757997545006219
with sage custom GMP allocation functions: 5.887209259002702
with the whole sagemath library lodaded:   5.1330207900027744
```



---

Comment by dimpase created at 2022-12-15 12:01:31

If you replace

```
    import sage.all
```

with

```
    import sympy
```

you'll get the same slowdown. Bingo! Sympy probably changes FPU status, it seems, according to
`build/pkgs/sympow/email-exchange.txt`


---

Comment by mmezzarobba created at 2022-12-15 12:35:30

Replying to [comment:9 Dima Pasechnik]:
> Sympy probably changes FPU status, it seems, according to
> `build/pkgs/sympow/email-exchange.txt`

Did you accidentally look in `sympow/` instead of `sympy/`, or do you mean sympow also has something to do with it?

I (obviously) don't see any slowdown here with `import sympy` either, even after reinstalling sympy with `sage -f`.


---

Comment by dimpase created at 2022-12-15 12:49:19

You know that the greatest discoveries are made by accident ;-) 

Indeed, I went looking for FPU in sage source repo, and found that `sympow` thing, which got swapped to `sympy` in my brain... Anyhow, for me `sympy` causes the same slowdown:

```
$ ./sage --python p.py
python environment with cypari2:           4.7912631159997545
with sage custom GMP allocation functions: 4.886824406014057
with sympy loaded:   6.885674357006792
$ cat p.py
import timeit

prog="""
from cypari2 import Pari
pari = Pari(size={size})
pari("{cmd}")
"""

prog = prog.format(size=2**24, cmd="quadclassunit(1 - 2^100)")

print("python environment with cypari2:          ",
      timeit.timeit(prog, number=3))

setup="""
from sage.ext.memory import init_memory_functions
init_memory_functions()
"""
print("with sage custom GMP allocation functions:",
      timeit.timeit(prog, setup=setup, number=3))

setup="""
import sympy
"""
print("with sympy loaded:  ",
      timeit.timeit(prog, setup=setup, number=3))
```



---

Comment by dimpase created at 2022-12-15 13:06:45

I see this `sympy` difference on the latest Sage beta on 3 boxes: Fedora 34 with a newish CPU, and on Gentoo with a much older CPU, and on Debian Bullseye (stable) with an even older and slower CPU.

I see a bit of a **speedup**, by about 15%,  with either `sage.all` or `sympy` imported on macOS 13 (x86_64, as well as arm64, aka M1)


---

Comment by vdelecroix created at 2022-12-15 15:15:04

Replying to [comment:12 Dima Pasechnik]:
> I see this `sympy` difference on the latest Sage beta on 3 boxes: Fedora 34 with a newish CPU, and on Gentoo with a much older CPU, and on Debian Bullseye (stable) with an even older and slower CPU.
> 
> I see a bit of a **speedup**, by about 15%,  with either `sage.all` or `sympy` imported on macOS 13 (x86_64, as well as arm64, aka M1)

Interesting.
- on system sage on archlinux : no difference between no setup and `import sympy`
- on the compiled sage : same slowdown with `import sympy` and `import sage.all`


---

Comment by dimpase created at 2022-12-15 16:56:02

narrowing it down further, with taking all of Sage out:

```python
# p1.py
import timeit

prog="""
from cypari2 import Pari
pari = Pari(size={size})
pari("{cmd}")
"""

prog = prog.format(size=2**24, cmd="quadclassunit(1 - 2^100)")

print("python environment with cypari2:         ",
      timeit.timeit(prog, number=3))

setup="""
import sympy
"""
print("with sympy loaded:                       ",
      timeit.timeit(prog, setup=setup, number=3))
```


On Gentoo with system `python3.10.8`, system-wide `pari 2.15.1`, running this `p1.py`

  * "naked" system python,  pip-installed `cypari2` and `sympy` - no time difference
  * venv'ed system python,  pip-installed `cypari2` and `sympy` - no time difference
  * Sage's-venv wrapped system python, Sage-installed `cypari` and `sympy` - `sympy` produces a slowdown.  

So this is down to interaction between python, sympy, and cypari2 (and Sage's venv?)


---

Comment by dimpase created at 2022-12-15 17:04:39

Matthias, there is something funny in Sage's venv, causing such a slowdown on Linux.


---

Comment by mkoeppe created at 2022-12-15 17:48:59

To narrow it down further, you can try installing the Sage-built wheels (from venv/var/lib/sage/wheels/) in a separate venv. Specifically, take cypari2 from the sage wheel rather than from pypi.


---

Comment by dimpase created at 2022-12-15 20:53:20

Replying to [comment:16 Matthias Köppe]:
> To narrow it down further, you can try installing the Sage-built wheels (from venv/var/lib/sage/wheels/) in a separate venv. Specifically, take cypari2 from the sage wheel rather than from pypi.

This way, no difference in timing, so the wheels themselves are OK.


---

Comment by mkoeppe created at 2022-12-15 21:15:54

Next you could check whether it makes a difference to run the separate venv inside or outside of the sage environment set up by `sage -sh`.


---

Comment by dimpase created at 2022-12-15 22:02:07

if I start `sage -sh`, cd somewhere outside Sage directory, run `python -m venv swheels`,
activate `swheels`, and try doing things there, `pip` does not look too healthy:

```
(swheels) (sage-sh) dima@hilbert:swheels$ pip cache purge
An error occurred during configuration: option format: invalid choice: 'columns' (choose from 'human', 'abspath')
```


regardless, `pip install` works there, and I can run the test - again, no difference in time.
So, somehow, putting Sage's python into venv cures this time discrepancy.


```
(swheels) (sage-sh) dima@hilbert:swheels$ python p.py
python environment with cypari2:          5.116253892017994
with sympy loaded:                        5.216335576027632
(swheels) (sage-sh) dima@hilbert:swheels$ deactivate 
(sage-sh) dima@hilbert:swheels$ which python
/mnt/opt/Sage/sage-dev/local/var/lib/sage/venv-python3.10/bin/python
(sage-sh) dima@hilbert:swheels$ python p.py 
python environment with cypari2:          5.446458007965703
with sympy loaded:                        8.667378643993288
```



---

Comment by mkoeppe created at 2022-12-15 22:05:31

Replying to [comment:19 Dima Pasechnik]:
> if I start `sage -sh`, cd somewhere outside Sage directory, run `python -m venv swheels`,
> activate `swheels`, and try doing things there, `pip` does not look too healthy:
> {{{
> (swheels) (sage-sh) dima`@`hilbert:swheels$ pip cache purge
> An error occurred during configuration: option format: invalid choice: 'columns' (choose from 'human', 'abspath')
> }}}

Indeed. I've opened #34853 for this unrelated issue.


---

Comment by mkoeppe created at 2022-12-15 22:07:39

I'd suggest to look whether mpmath is somehow involved, see #25445


---

Comment by dimpase created at 2022-12-15 22:20:29

Replying to [comment:21 Matthias Köppe]:
> I'd suggest to look whether mpmath is somehow involved, see #25445

In case, it's exactly the same wheels, `mpmath` included, that are involved here. As to #25445, indeed, `import mpmath` has the same side effect on performance as `import sympy`.

So we can now test instead the following:

```python
# p1.py
import timeit

prog="""
from cypari2 import Pari
pari = Pari(size={size})
pari("{cmd}")
"""

prog = prog.format(size=2**24, cmd="quadclassunit(1 - 2^100)")

print("python environment with cypari2:         ",
      timeit.timeit(prog, number=3))

setup="""
import mpmath
"""
print("with mpmath loaded:                       ",
      timeit.timeit(prog, setup=setup, number=3))
```



---

Comment by mkoeppe created at 2022-12-15 22:26:31

Try if the various `MPMATH_...` environment variables (https://github.com/mpmath/mpmath/blob/master/mpmath/libmp/backend.py#L66) have an effect


---

Comment by dimpase created at 2022-12-15 22:42:47

Oops, we're back to `import sage.all` here, as your link points few lines below at

```python
if ('MPMATH_NOSAGE' not in os.environ and 'SAGE_ROOT' in os.environ or
        'MPMATH_SAGE' in os.environ):
    try:
        import sage.all
        import sage.libs.mpmath.utils as _sage_utils
        sage = sage.all
        sage_utils = _sage_utils
        BACKEND = 'sage'
        MPZ = sage.Integer
    except:
        pass
```

and skipping it indeed cures the discrepancy.

```
(sage-sh) dima@hilbert:swheels$ MPMATH_NOSAGE=yes python p.py
python environment with cypari2:          5.126071550010238
with sympy loaded:                        5.132636028982233
(sage-sh) dima@hilbert:swheels$ python p.py
python environment with cypari2:          5.0600332279573195
with sympy loaded:                        8.450407178024761
```


sorry for the bogus lead :-)

The question is now - can we import less than `sage.all` pieces?


---

Comment by mkoeppe created at 2022-12-15 23:08:24

Can you reproduce it if you replace `import mpmath` by `import sage.libs.pari.all` or `import.sage.cpython.all` etc?


---

Comment by dimpase created at 2022-12-15 23:43:34

Replying to [comment:25 Matthias Köppe]:
> Can you reproduce it if you replace `import mpmath` by `import sage.libs.pari.all` or `import.sage.cpython.all` etc?

Success with `sage.libs.eclib.mat` or `sage.libs.eclib.homspace` - they are still a serious chunk of `sage.all`, about 40%, by some measure, but not all of it:


```
$ ./sage --python
>>> import sys
>>> import sage.libs.eclib.mat
>>> len(sys.modules.keys())
673
>>> 
```

vs

```
$ ./sage --python
Python 3.10.8 (main, Nov 29 2022, 20:55:11) [GCC 12.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> len(sys.modules.keys())
74
>>> import sage.all
>>> len(sys.modules.keys())
1712
```


for `sage.libs.eclib.homspace` one has `674` keys
(but these for `sage.libs.eclib.mat` are a subset of these, so we can go with the latter)


---

Comment by dimpase created at 2022-12-15 23:52:00


```
>>> import sage.libs.eclib.mwrank
>>> len(sys.modules.keys())
346
```

and `import sage.libs.eclib.mwrank` does not reproduce. so we're down to ~330 modules to check  :-)


---

Comment by dimpase created at 2022-12-16 00:04:31

`eclib` certainly uses `libpari`, so it potentially could be that loading it changes some `pari` defaults.
But I don't know anything about this.
Maybe John C. can comment...


---

Comment by cremona created at 2022-12-16 08:52:13

Yes, eclib calls some functions in libpari and so must make sure that libpari has been initialised.  This is done in the eclib function `eclib_pari_init` which detects whether it has already been initialised (by testing the value of pari variable "avma") and if necessary calls `pari_init()`.  The first parameter  to `pari_init` is taken from the environment variable `PARI_SIZE` and defaults to (currently) `10^8` if that variable is not set or has a value which cannot be converted to a long int.

The function `eclib_pari_init()` need never be called by users (and is not called by Sage) as it is called automatically when needed by one of two other functions which themseves use pari functions.  I would expect that whenever Sage calls an eclib function which results in eclib's `eclib_pari_init` to be called, it will find that libpari has already been initialised (so avma will be nonzero) and then do nothing.  But my expectation could be wrong in some situations?

See https://github.com/JohnCremona/eclib/blob/master/libsrc/parifact.cc


---

Comment by dimpase created at 2022-12-16 10:43:19

OK, `avma` is a global Pari variable. Declared in `pari/paristio.h` as 

`extern THREAD pari_sp avma;`

Potentially every Sage module depending on Pari might want to check it and run Pari initialisation if it decided to, or perhaps even unconditionally, without checking. I don't know how it plays up in the presence of multithreading, either.

Suspects: `lcalc`, `giac`...

By the way, also `import src.sage.libs.pari.convert_sage` triggers the slowdown.


---

Comment by dimpase created at 2022-12-16 14:14:03

`import sage.rings.integer` is another reproducer, with "only" 346 sub-imports

lcalc and giac are OK.


---

Comment by mmezzarobba created at 2022-12-16 14:34:26

Have you tried running both versions under `perf top` or `perf record` to see where the time goes?


---

Comment by dimpase created at 2022-12-16 15:22:58

Replying to [comment:32 Marc Mezzarobba]:
> Have you tried running both versions under `perf top` or `perf record` to see where the time goes?

I tried cProfile,  mentioned it on sage-devel.
Nothing to see, it's a slowdown of Pari itself that happens. You are welcome to take over.


---

Comment by dimpase created at 2022-12-16 16:44:45

Replying to [comment:31 Dima Pasechnik]:
> `import sage.rings.integer` is another reproducer, with "only" 346 sub-imports
it is reproducer on Fedora 34, but not on Gentoo. So that's getting even more bizarre.
> 
> lcalc and giac are OK.
on the other hand, they both unconditionally initialise Pari by calling `pari_init_options`.
In view of `avma` being a thread-local global variable, I'm not sure about what's going on there, but perhaps we still should patch them as follows:


```diff
--- a/src/lcalc/Lcommandline.cc
+++ b/src/lcalc/Lcommandline.cc
@@ -31,6 +31,9 @@ Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 #include "config.h"
 #include "Lcommandline.h"
 #include "cmdline.h"
+#if HAVE_LIBPARI
+#include <pari/pari.h>
+#endif
 
 void pari_err_recover_nop(long errnum) {return;}
 
@@ -482,10 +485,11 @@ int main (int argc, char *argv[])
 
 
             t2=.5; //t2=.5 because of the GAMMA(s+1/2)
-
+            if (!avma) {
             pari_init_opts(400000000,2,INIT_DFTm); // the last option is to prevent
             //pari from giving its interrupt signal when its elliptic curve a_p
             //algorithm is called and interrupted with ctrl-c.
+            }
 
             coeff = new Double[3];
             //compute the conductor which is copied to coeff[1]
```

and for giac:

```diff
diff --git a/src/pari.cc b/src/pari.cc
index 76ce8e1..bdf1073 100644
--- a/src/pari.cc
+++ b/src/pari.cc
@@ -97,8 +97,10 @@ namespace giac {
   static void call_pari_init()
   {
     // do not initialize INIT_JMP so that PARI error do not exit
-    pari_init_opts(pari_mem_size,pari_maxprime,INIT_SIGm | INIT_DFTm);
-    paristack_setsize(pari_mem_size, (1<<30));
+    if (!avma) {
+       pari_init_opts(pari_mem_size,pari_maxprime,INIT_SIGm | INIT_DFTm);
+       paristack_setsize(pari_mem_size, (1<<30));
+    }
     // initialize variable ordering x,y,z
     gp_read_str("[x,y,z,t]");
   }
```



---

Comment by mmezzarobba created at 2022-12-16 16:56:30

Replying to [comment:33 Dima Pasechnik]:
> Replying to [comment:32 Marc Mezzarobba]:
> > Have you tried running both versions under `perf top` or `perf record` to see where the time goes?
> 
> I tried cProfile,  mentioned it on sage-devel.
> Nothing to see, it's a slowdown of Pari itself that happens. You are welcome to take over.

I'm happy to help if I can, but at the moment I don't understand how to reproduce the slowdown. I was suggesting perf because I thought a systemwide profiler may help understand what is changing.


---

Comment by dimpase created at 2022-12-16 18:28:40

Replying to [comment:35 Marc Mezzarobba]:
> Replying to [comment:33 Dima Pasechnik]:
> > Replying to [comment:32 Marc Mezzarobba]:
> > > Have you tried running both versions under `perf top` or `perf record` to see where the time goes?
> > 
> > I tried cProfile,  mentioned it on sage-devel.
> > Nothing to see, it's a slowdown of Pari itself that happens. You are welcome to take over.
> 
> I'm happy to help if I can, but at the moment I don't understand how to reproduce the slowdown. I was suggesting perf because I thought a systemwide profiler may help understand what is changing.

I don't know (or can't recall :-)) how to use `perf`. I imagine one would need a special build of `libpari`, and some magical commands to run...


---

Comment by mmezzarobba created at 2022-12-17 08:51:30

Replying to [comment:36 Dima Pasechnik]:
> I don't know (or can't recall :-)) how to use `perf`. I imagine one would need a special build of `libpari`, and some magical commands to run...

Just run

```
perf record -- sage -python3 pari_timing.py
```

and then

```
perf report
```



---

Comment by mmezzarobba created at 2022-12-17 09:28:17

Also, after upgrading my system and going back to python 3.10, I do see the slowdown. I'll check what happens on the 3.11 branch with the same system packages -- but this looks like it will require a full rebuild, so it may take some time.


---

Comment by mmezzarobba created at 2022-12-17 13:44:04

The slowdown is still there after a full rebuild with python 3.11.


---

Comment by mmezzarobba created at 2022-12-17 13:47:27

Replying to [comment:14 Dima Pasechnik]:

> On Gentoo with system `python3.10.8`, system-wide `pari 2.15.1`, running this `p1.py`

You wrote “system-wide `pari 2.15.1`”, but if I understand the `spkg-configure` right, pari 2.15.* is not allowed by sage's configure. Could it be that your tests are not all using the same version of pari?


---

Comment by dimpase created at 2022-12-17 14:04:53

Replying to [comment:40 Marc Mezzarobba]:
> Replying to [comment:14 Dima Pasechnik]:
> 
> > On Gentoo with system `python3.10.8`, system-wide `pari 2.15.1`, running this `p1.py`
> 
> You wrote “system-wide `pari 2.15.1`”, but if I understand the `spkg-configure` right, pari 2.15.* is not allowed by sage's configure. Could it be that your tests are not all using the same version of pari?

I am on #34537 (update to Pari 2.15.1) in Gentoo.
So there is an evidence that in this case you need to import more than `sage.rings.integer` to see a slowdown.


---

Comment by mmezzarobba created at 2022-12-17 16:58:53

So I am now able to reproduce the issue. Comparing the profiles, the main difference seems to be a huge amount of time spent in the dynamic linker, more precisely in `_dl_check_map_versions()` (presumably called by `dlopen()`) and in `update_get_addr()`. Regarding the latter, the relevant code seems to be (`glibc/elf/dl-tls.c`):

```
void *
__tls_get_addr (GET_ADDR_ARGS)
{
  dtv_t *dtv = THREAD_DTV ();

  /* Update is needed if dtv[0].counter < the generation of the accessed
     module.  The global generation counter is used here as it is easier
     to check.  Synchronization for the relaxed MO access is guaranteed
     by user code, see CONCURRENCY NOTES in _dl_update_slotinfo.  */
  size_t gen = atomic_load_relaxed (&GL(dl_tls_generation));
  if (__glibc_unlikely (dtv[0].counter != gen))
    return update_get_addr (GET_ADDR_PARAM);   // <----------

  void *p = dtv[GET_ADDR_MODULE].pointer.val;

  if (__glibc_unlikely (p == TLS_DTV_UNALLOCATED))
    return tls_get_addr_tail (GET_ADDR_PARAM, dtv, NULL);

  return (char *) p + GET_ADDR_OFFSET;
}
```

Afaict the slow path is never taken when running cypari under plain Python, while it is taken all the time when parts of Sage are loaded.  But I don't know what to make of all that... (There is some information on TLS that looks like it might be relevant at https://chao-tic.github.io/blog/2018/12/25/tls)


---

Comment by dimpase created at 2022-12-17 18:20:21

I don't think your profiling makes much sense this way, as you are not excluding time taken by  `import`, right? 
You can mitigate this by increasing `number=` to 30 or 50 - then you won't see linker kicking in too much. 

Or I misunderstand, and you say that the linker is called by Pari?


---

Comment by nbruin created at 2022-12-17 21:51:23

When trying "python3 with cypari" and "python3 with sympy imported" with `number=50` the effect is much more pronounced than with `number=3` and it seems it's still stuff in the linker that explains the difference in the profile. `perf diff perf_python3.data perf_sympy.data` yields:

```
              +23.90%  ld-linux-x86-64.so.2                                        [.] _dl_update_slotinfo
    30.30%    -12.38%  libpari-gmp-tls.so.2.13.4                                   [.] 0x000000000009fc74
              +10.87%  ld-linux-x86-64.so.2                                        [.] update_get_addr
    10.31%     -4.60%  libpari-gmp-tls.so.2.13.4                                   [.] addii_sign
               +4.40%  ld-linux-x86-64.so.2                                        [.] __tls_get_addr_slow
     6.54%     -2.55%  libpari-gmp-tls.so.2.13.4                                   [.] xxgcduu
    11.61%     -2.51%  ld-linux-x86-64.so.2                                        [.] __tls_get_addr
     4.62%     -2.07%  libpari-gmp-tls.so.2.13.4                                   [.] shifti
     3.24%     -1.45%  libpari-gmp-tls.so.2.13.4                                   [.] muliispec
     3.11%     -1.16%  libpari-gmp-tls.so.2.13.4                                   [.] dvmdii
     2.43%     -1.08%  libpari-gmp-tls.so.2.13.4                                   [.] mulii
     2.09%     -1.01%  libgmp.so.10.4.1                                            [.] __gmpn_divrem_2
     1.91%     -0.88%  libpari-gmp-tls.so.2.13.4                                   [.] muluu
     1.75%     -0.84%  libgmp.so.10.4.1                                            [.] __gmpn_addmul_1_x86_64
     1.70%     -0.62%  libpari-gmp-tls.so.2.13.4                                   [.] abscmpii
     1.24%     -0.59%  libgmp.so.10.4.1                                            [.] __gmpn_invert_limb
     1.46%     -0.58%  libgmp.so.10.4.1                                            [.] __gmpn_tdiv_qr

```

where, as far as I can see, `__tls_get_addr_slow` doesn't show up at all for `python3`. So as far as I can see, some switch gets thrown that forces the linker to look up a symbol again and again, in a slower way. I would have expected dynamic linking to happen once in a run, and then getting out of the way. That doesn't seem to be happening here.


---

Comment by dimpase created at 2022-12-17 22:08:34

> I would have expected dynamic linking to happen once in a run, and then getting out of the way. That doesn't seem to be happening here. 

Indeed, this very, very weird. I've never heard about such cases. But it seems we are now debugging a rather eigenzinnig, pardon my Dutch, Pari's use of threads?


---

Comment by nbruin created at 2022-12-17 22:22:23

I'm also seeing the slow-down with just `import mpmath` (which is also something `sympy` does). So perhaps it's something mpmath does with its configuration of gmp? Note that mpfr, which is used by mpmath, also makes use of gmp, so that could be another source of conflicts. Plus: mpmath has some sage-specific backend stuff, so importing a sage-aware mpmath could have different effects than one that isn't.

One hypothesis would be that the gmp library gets linked twice: once by python and once by libpari, and perhaps under different threading conditions; leading to an error in initialization of the thread-local storage of libgmp; causing a slow symbol resolution in libgmp every time ...

I did not see a slowdown with `import gmpy2`, so it's something `mpmath` does. It may well be something in `sage.libs.mpmath.ext_main`, which a sage-aware mpmath imports.

In fact, [ext_libmp.pyx:11](https://github.com/sagemath/sage/blob/bb7ee856390710fc17caf02615de16f5adc66cf2/src/sage/libs/mpmath/ext_libmp.pyx#L11) indicates that sage-aware `mpmath` would call some initialization that has to do with gmp at import-time. So if libpari messes with threads afterwards and links gmp, there may be some room for things to go sideways.


---

Comment by dimpase created at 2022-12-17 23:36:24

Replying to [comment:46 Nils Bruin]:
> I'm also seeing the slow-down with just `import mpmath` (which is also something `sympy` does). So perhaps it's something mpmath does with its configuration of gmp? 

Sage-aware mpmath imports `sage.all` (see comment:24), so in this sense it's a moot point.
But yes, there is quite a bit of dodgy things going on in `mpmath`.


---

Comment by nbruin created at 2022-12-17 23:54:08

Replying to [comment:47 Dima Pasechnik]:
> Sage-aware mpmath imports `sage.all` (see comment:24), so in this sense it's a moot point.
> But yes, there is quite a bit of dodgy things going on in `mpmath`.

Apologies for replicating that already-established knowledge. Following up on comment:23: when using the correct syntax to set `MPMATH_NOSAGE=1`, the import of `mpmath` has no adverse effect. So it's something else in `sage.all` that causes the problem (edited in response to comment:49).


---

Comment by mmezzarobba created at 2022-12-18 09:04:57

Replying to [comment:48 Nils Bruin]:
> Following up on comment:23: as far as I can see, `MPMATH_NOSAGE, MPMATH_NOGMPY` have no influence on the result.

Are you sure?

```
(sage-sh) marc@tramontane:tmp$ python3 pari_timing.py
python environment with cypari2:           3.799615835014265
with mpmath loaded:   6.606633314979263
(sage-sh) marc@tramontane:tmp$ export MPMATH_NOSAGE=1
(sage-sh) marc@tramontane:tmp$ python3 pari_timing.py
python environment with cypari2:           3.996781985013513
with mpmath loaded:   4.207063634006772
```



---

Comment by dimpase created at 2022-12-18 11:21:35

to quote GCC people on the topic `__tls_get_addr`
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81501#c6

```
We recently upgraded our toolchain from GCC9 to GCC11, and we're seeing __tls_get_addr
take up to 10% of total runtime under some workloads, where it was 1-2% before.
```


indeed, this means that if `__tls_get_addr` gets slowed down, the same code might suddenly get visibly slower.


---

Comment by dimpase created at 2022-12-18 12:14:12

The story of `__tls_get_addr_slow` in glibc (https://github.com/lattera/glibc/blob/master/sysdeps/x86_64/tls_get_addr.S) starts from
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58066

but I can't make much sense of it. Something to do with misaligned stack.


---

Comment by vdelecroix created at 2022-12-18 13:17:11

Bill suggested to try PARI without thread support (`Configure --mt=single`).


---

Comment by mmezzarobba created at 2022-12-18 13:40:27

Possibly relevant:
https://sourceware.org/bugzilla/show_bug.cgi?id=19924


---

Comment by dimpase created at 2022-12-18 14:15:04

Replying to [comment:53 Marc Mezzarobba]:
> Possibly relevant:
> https://sourceware.org/bugzilla/show_bug.cgi?id=19924


```
we have noticed a performance degradation of TLS access in shared libraries. If another shared library that uses TLS is loaded via dlopen, __tls_get_addr takes significant more time. Once that shared library accesses it's TLS, the performance normalizes.
```


So it seems that if in the test setup we do touch TLS vars of all the dlopen'd libraries:

```
setup="""
import sage.all
_ = libgap(1)
_ = giac(1)
... etc
"""
```

then the test itself should run faster. (But I can't work on this now)


---

Comment by dimpase created at 2022-12-18 14:17:17

Replying to [comment:52 Vincent Delecroix]:
> Bill suggested to try PARI without thread support (`Configure --mt=single`).

this is incompatible with giac, IIRC


---

Comment by mmezzarobba created at 2022-12-18 14:28:31

Replying to [comment:54 Dima Pasechnik]:
> So it seems that if in the test setup we do touch TLS vars of all the dlopen'd libraries:

Doesn't that mean every cython module in Sage?... Or maybe not, because they don't use TLS?


---

Comment by dimpase created at 2022-12-18 14:34:28

Replying to [comment:56 Marc Mezzarobba]:
> Replying to [comment:54 Dima Pasechnik]:
> > So it seems that if in the test setup we do touch TLS vars of all the dlopen'd libraries:
> 
> Doesn't that mean every cython module in Sage?... Or maybe not, because they don't use TLS?

certainly not all of them matter (e.g. we know that importing sage.libs.eclib.mwrank is OK, but
sage.libs.eclib.mat is not)


---

Comment by mmezzarobba created at 2022-12-18 14:37:00

Some more details on where the slowdown appears to be happening, in case this gives someone an idea. If we look at the context where `__tls_get_addr` is running in the version with sage loaded, we get (children are callers; percentages are cycle counts for callchains with a given callee):

```
-   21,83%    21,83%  python3  ld-linux-x86-64.so.2
   - 78,46% _dl_update_slotinfo (inlined)
      - 65,84% update_get_addr (inlined)
         - __tls_get_addr (inlined)
            + 66,50% new_chunk (inlined)
            + 27,93% set_avma (inlined)
            + 2,41% dvmdii (inlined)
            + 1,57% addmulii_lg3 (inlined)
        3,91% 0x3
        3,36% 0x1
        2,25% 0x4
        2,23% 0x2
        0,73% 0xfffffffffffffffe
        0,60% 0x1f
        0,52% 0x17
   - 21,52% update_get_addr (inlined)
      - 37,56% __tls_get_addr (inlined)
         + 59,76% new_chunk (inlined)
         + 34,12% set_avma (inlined)
         + 2,43% dvmdii (inlined)
         + 1,26% addmulii_lg3 (inlined)
         + 0,66% qficomp0 (inlined)
        8,31% 0x3
        6,80% 0x2
        3,53% 0x4
        1,51% 0xffffffffffffffff
        1,14% 0x1
        1,10% 0x1f
        1,02% 0xfffffffffffffffe
        0,97% 0x17
```

So basically the callers are low-level pari functions that access things like `avma` and `pari_stack`.


---

Comment by mmezzarobba created at 2022-12-18 15:04:07

Replying to [comment:57 Dima Pasechnik]:
> we know that importing sage.libs.eclib.mwrank is OK, but
> sage.libs.eclib.mat is not

Above you said that they both reproduce the slowdown, and that's also what I observe. Looks like importing `sage.libs.eclib.mwrank` causes 69 `dlopen`s... (I'm counting lines containig `direct_opencount` in the output of `LD_DEBUG=files`; not surt if that's right.)


---

Comment by dimpase created at 2022-12-18 15:58:07

a missing 'not' in comment:27 - edited now.

however, this is all so volatile, no surprise it it is different for you.

I wonder if a fix is possible within Python importing machinery


---

Comment by tornaria created at 2022-12-18 20:19:21

I can reproduce the slowdown with `sage.all` loaded, but NOT with `mpmath`, `sympy`, `sage.libs.eclib.mwrank` nor `sage.rings.integer`:

```
python environment with cypari2:           4.8728421429987065
with sage custom GMP allocation functions: 4.884550739021506
with mpmath loaded:   4.292751049972139
with sympy loaded:   4.475166625983547
with sage.libs.eclib.mwrank loaded:   4.776536963006947
with sage.rings.integer loaded:   4.415787325997371
with the whole sagemath library loaded:   7.537986656010617
```

This is all with system pari 2.15.1 and system sagemath 9.7.


---

Comment by dimpase created at 2022-12-18 20:31:02

Replying to [comment:61 Gonzalo Tornaría]:
> I can reproduce the slowdown with `sage.all` loaded, but NOT with `mpmath`, `sympy`, `sage.libs.eclib.mwrank` nor `sage.rings.integer`:
> {{{
> python environment with cypari2:           4.8728421429987065
> with sage custom GMP allocation functions: 4.884550739021506
> with mpmath loaded:   4.292751049972139
> with sympy loaded:   4.475166625983547
> with sage.libs.eclib.mwrank loaded:   4.776536963006947
> with sage.rings.integer loaded:   4.415787325997371
> with the whole sagemath library loaded:   7.537986656010617
> }}}
> This is all with system pari 2.15.1 and system sagemath 9.7.

how about `sage.libs.eclib.mat` ? (I never succeeded with `sage.libs.eclib.mwrank`)


---

Comment by tornaria created at 2022-12-18 20:50:50

Replying to [comment:62 Dima Pasechnik]:
> how about `sage.libs.eclib.mat` ? (I never succeeded with `sage.libs.eclib.mwrank`)

Yes, and it tracks down to one of these:
 - `sage.rings.finite_rings.finite_field_constructor`
 - `sage.rings.finite_rings.finite_field_givaro`
 - `sage.rings.finite_rings.element_givaro`


---

Comment by tornaria created at 2022-12-18 23:33:38

This seems to depend on the order the shared libraries are loaded. In this case, loading `-larb` earlier seems to avoid the slowdown. Here's a hack to do it:

- Using the original file from the ticket description:

```
$ python pari_timing.py 
python environment with cypari2:           4.7771418140036985
with sage custom GMP allocation functions: 4.94545228901552
with the whole sagemath library lodaded:   7.068615118972957
```


- With "the hack":

```
$ python pari_timing-stub.py 
python environment with cypari2:           5.000043574022129
with sage custom GMP allocation functions: 4.970149756001774
with the whole sagemath library lodaded:   4.352886867010966
```

- It all amounts to load a trivial stub module before loading sage.all:

```
$ diff -u pari_timing.py pari_timing-stub.py 
--- pari_timing.py	2022-12-18 19:00:03.984366341 -0300
+++ pari_timing-stub.py	2022-12-18 19:00:18.996184656 -0300
@@ -19,6 +19,7 @@
       timeit.timeit(prog, setup=setup, number=3))
 
 setup="""
+import stub
 import sage.all
 """
 print("with the whole sagemath library lodaded:  ",
```

- The stub does nothing, but it is linked with `-larb`:

```
$ cat stub.c 
#include <Python.h>

struct PyModuleDef stub = {
	PyModuleDef_HEAD_INIT,
	"stub", NULL, -1, NULL
};

PyMODINIT_FUNC
PyInit_stub(void)
{
	return PyModule_Create(&stub);
}
$ cc -I/usr/include/python3.11 stub.c -fPIC -larb -shared -o stub.so
```



---

Comment by tornaria created at 2022-12-18 23:38:44

Voilà:

```
$ PYTHONPATH=pkgs/sagemath-standard/build/lib.linux-x86_64-cpython-311 python pari_timing.py 
python environment with cypari2:           4.7483764729695395
with sage custom GMP allocation functions: 4.830055630998686
with the whole sagemath library lodaded:   4.3143665379611775
$ git diff
diff --git a/src/sage/rings/all.py b/src/sage/rings/all.py
index a6090039db2..a2e379d4a42 100644
--- a/src/sage/rings/all.py
+++ b/src/sage/rings/all.py
@@ -72,6 +72,10 @@ from sage.rings.finite_rings.integer_mod_ring import IntegerModRing, Zmod
 from sage.rings.finite_rings.integer_mod import IntegerMod, Mod, mod
 Integers = IntegerModRing
 
+# this must come early (see #34850)
+# at least before finite_rings, number_field, polynomial, ...
+from sage.rings.real_arb import RealBallField, RBF
+
 # Finite fields
 from .finite_rings.all import *
 
@@ -103,8 +107,6 @@ from .real_double import RealDoubleField, RDF, RealDoubleElement
 
 from .real_lazy import RealLazyField, RLF, ComplexLazyField, CLF
 
-from sage.rings.real_arb import RealBallField, RBF
-
 # Polynomial Rings and Polynomial Quotient Rings
 from .polynomial.all import *
 
```



---

Comment by nbruin created at 2022-12-19 01:49:55

Cool! It definitely seems to solve the problem. However, it doesn't really explain what was going wrong here. And that means this solution may be a bit fragile, since you're just triggering something that happens to have the right (unknown) side-effect.

In particular, it's pretty clear that the problem here is coming from libpari, and libpari does not depend on arb, nor does arb depend on libpari. So I'd think it's a common dependence. If we look at those:

```
$ ldd /lib64/libpari.so
	linux-vdso.so.1 (0x00007ffc513f0000)
	libc.so.6 => /lib64/libc.so.6 (0x00007fe3cd400000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fe3ce256000)
	libm.so.6 => /lib64/libm.so.6 (0x00007fe3ce15d000)
	libgmp.so.10 => /lib64/libgmp.so.10 (0x00007fe3ce0b8000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fe3ce098000)
$ ldd /lib64/libarb.so
	linux-vdso.so.1 (0x00007ffce20ab000)
	libflint.so.17 => /lib64/libflint.so.17 (0x00007f1b6c600000)
	libmpfr.so.6 => /lib64/libmpfr.so.6 (0x00007f1b6d18c000)
	libgmp.so.10 => /lib64/libgmp.so.10 (0x00007f1b6c55b000)
	libm.so.6 => /lib64/libm.so.6 (0x00007f1b6c47d000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f1b6c200000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f1b6d25a000)
	libflexiblas.so.3 => /lib64/libflexiblas.so.3 (0x00007f1b6be00000)
	libntl.so.44 => /lib64/libntl.so.44 (0x00007f1b6ba00000)
	libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007f1b6b600000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f1b6d16a000)
	libgfortran.so.5 => /lib64/libgfortran.so.5 (0x00007f1b6b200000)
	libquadmath.so.0 => /lib64/libquadmath.so.0 (0x00007f1b6d122000)
	libgf2x.so.3 => /lib64/libgf2x.so.3 (0x00007f1b6d111000)
```

so perhaps you can already get the desired effect by linking to libgmp?


---

Comment by tornaria created at 2022-12-19 02:05:07

Here's a way to reproduce independent of sage and without compiled modules (other than using `ctypes` to dlopen libraries):

```
$ python pari_timing3.py 
python environment with cypari2:  4.842002137971576
with -lmpfi:                      8.262313047016505
with -larb:                       4.4230794320465066
```


Here's `pari_timing3.py`:

```python
import timeit

prog="""
from cypari2 import Pari
pari = Pari(size={size})
pari("{cmd}")
"""

prog = prog.format(size=2**24, cmd="quadclassunit(1 - 2^100)")

print("python environment with cypari2: ",
      timeit.timeit(prog, number=3))

setup="""
from ctypes import cdll
cdll.LoadLibrary('libmpfi.so')
"""
print("with -lmpfi:                     ",
      timeit.timeit(prog, setup=setup, number=3))

setup="""
from ctypes import cdll
cdll.LoadLibrary('libarb.so')
"""
print("with -larb:                      ",
      timeit.timeit(prog, setup=setup, number=3))
```


First guess: `-lmpfi` is doing something weird at load time. Then `-larb` is fixing it. For some reason, whatever `-larb` is doing doesn't work if it's done too late (as in `import sage.all` without my patch). Loading `-larb` first prevents `-lmpfr` from doing any wrong as you can see reverting the order in this script.


---

Comment by tornaria created at 2022-12-19 02:17:21

Changing `-lmpfi` for `-lmpfr` gives the same result.

Since `-larb` loads `-lmpfr` this explains why loading `-larb` early fixes the problem: since `-lmpfr` is already loaded, it won't be loaded later so it can no longer do any harm.

This still doesn't explain why loading arb too late in `sage.rings.all` won't fix the issue.


---

Comment by vdelecroix created at 2022-12-19 07:53:39

It seems to me that `timeit` is not run in an independent process. Namely if you **start** with `setup="import sage.all"` then everything get slow. Do anyone knows of a solution to run these in independent processes?


---

Attachment


---

Comment by vdelecroix created at 2022-12-19 09:49:20

In [pari_timing.py](https://trac.sagemath.org/raw-attachment/ticket/34850/pari_timing.py) the timeit is now run in independent process. I only see a slowdown for `import sage.all`

```
setup 0
import sage.all
9.015203488990664
--------------------------------------------------
setup 1
5.769485066004563
--------------------------------------------------
setup 2
import sympy
5.582505510014016
--------------------------------------------------
setup 3
from ctypes import cdll
cdll.LoadLibrary('libgmp.so')
5.503667832992505
--------------------------------------------------
setup 4
from ctypes import cdll
cdll.LoadLibrary('libarb.so')
5.5995761190133635
--------------------------------------------------
setup 5
from ctypes import cdll
cdll.LoadLibrary('libmpfr.so')
5.045454590988811
--------------------------------------------------
setup 6
from ctypes import cdll
cdll.LoadLibrary('libgiac.so')
5.5255847749940585
--------------------------------------------------
fast_runs: [1, 2, 3, 4, 5, 6]
slow_runs: [0]
```



---

Comment by dimpase created at 2022-12-19 11:12:14

Are you telling mpmath (imported from sympy) to not import sage.all? Cause that's what it does by default, and I get `slow_runs: [0,2]` (cf. comment:24)


---

Comment by dimpase created at 2022-12-19 11:29:34

Still, I can confirm Gonzalo's discovery that `arb` may be used to cure the slowness: modifying `setup 0` so that it's loaded first makes setup 0 fast:

```
setup 0
from ctypes import cdll
cdll.LoadLibrary('libarb.so')
import sage.all
4.591498055000557
--------------------------------------------------
setup 1
4.60094984099851
...
```


That is, I changed the test file part as follows:

```
...
# build different setups
setups = []
lib="arb"
setup = "from ctypes import cdll\n"
setup += "cdll.LoadLibrary('lib{}.so')\n".format(lib)
setup += "import sage.all"
setups.append(setup)
setups.append("")
setups.append("import sympy")
...
```



---

Comment by dimpase created at 2022-12-19 11:45:54

Another interesting observation - if I build giac with the patch in comment:34 then 

```
setup 6
from ctypes import cdll
cdll.LoadLibrary('libgiac.so')
```

gets as slow as `setup 0` (with `sage.all`).

I think it's Pari messing up things for everything else, no? Also, I don't know why Pari has to be called in `cysignals`, and nothing else - why such a royal treatment?


---

Comment by cremona created at 2022-12-19 11:46:09

So if arb knows what to do, it would be worth asking Fredrik J perhaps


---

Comment by dimpase created at 2022-12-19 11:55:17

Oh yeah, I was just going to invite Fredrik to look at this mess...


---

Comment by dimpase created at 2022-12-19 11:59:45

Replying to [comment:65 Gonzalo Tornaría]:
> Voilà:
> {{{
> $ PYTHONPATH=pkgs/sagemath-standard/build/lib.linux-x86_64-cpython-311 python pari_timing.py 
> python environment with cypari2:           4.7483764729695395
> with sage custom GMP allocation functions: 4.830055630998686
> with the whole sagemath library lodaded:   4.3143665379611775
> $ git diff
> diff --git a/src/sage/rings/all.py b/src/sage/rings/all.py
> ...
> }}}

does not work for me - while 

```
from ctypes import cdll
cdll.LoadLibrary('libarb.so')
import sage.all
```


does work. So it's even crazier than we think it is.


---

Comment by dimpase created at 2022-12-19 12:04:33

Fredrik, we'd really appreciate your input on the magical action of `arb` on `sage.all` we are observing here...


---

Comment by fredrik.johansson created at 2022-12-19 12:43:00

I have absolutely no idea what is going on.

Arb does use TLS but does nothing weird with it, AFAIK. Nor does it do anything weird with GMP.


---

Comment by mmezzarobba created at 2022-12-19 12:52:06

Replying to [comment:68 Gonzalo Tornaría]:
> Changing `-lmpfi` for `-lmpfr` gives the same result.
> 
> Since `-larb` loads `-lmpfr` this explains why loading `-larb` early fixes the problem: since `-lmpfr` is already loaded, it won't be loaded later so it can no longer do any harm.

So far (if I understand you correctly) it's not inconsistent with a link to https://sourceware.org/bugzilla/show_bug.cgi?id=19924. If loading an object that uses TLS slows down TLS accesses for previously loaded ones until the newly loaded object accesses TLS(!), the magic arb is doing may just be accessing, or causing mpfr to access, a thread-local variable... That's very speculative though.

> This still doesn't explain why loading arb too late in `sage.rings.all` won't fix the issue.

It may be interesting to compare the actual loading order of pari, mpfr, and arb (using LD_DEBUG=files) depending at what point arb is imported.


---

Comment by mmezzarobba created at 2022-12-19 13:08:51

Replying to [comment:65 Gonzalo Tornaría]:
> Voilà:
> {{{
> [...]
> $ git diff
> diff --git a/src/sage/rings/all.py b/src/sage/rings/all.py
> index a6090039db2..a2e379d4a42 100644
> --- a/src/sage/rings/all.py
> +++ b/src/sage/rings/all.py
> `@``@` -72,6 +72,10 `@``@` from sage.rings.finite_rings.integer_mod_ring import IntegerModRing, Zmod
>  from sage.rings.finite_rings.integer_mod import IntegerMod, Mod, mod
>  Integers = IntegerModRing
>  
> +# this must come early (see #34850)
> +# at least before finite_rings, number_field, polynomial, ...
> +from sage.rings.real_arb import RealBallField, RBF
> +
>  # Finite fields
>  from .finite_rings.all import *
> [...]
> }}}

Doesn't seem to work here...


---

Comment by tornaria created at 2022-12-19 13:23:02

Of course, the problem is loading pari + mpfr //in that order//:

```
setup 0

from ctypes import cdll
cdll.LoadLibrary('libmpfr.so')

4.866691738017835
--------------------------------------------------
setup 1

from ctypes import cdll
cdll.LoadLibrary('libpari.so')
cdll.LoadLibrary('libmpfr.so')

7.862922856991645
--------------------------------------------------
setup 2

from ctypes import cdll
cdll.LoadLibrary('libmpfr.so')
cdll.LoadLibrary('libpari.so')

4.771596382022835
--------------------------------------------------
fast_runs: [0, 2]
slow_runs: [1]
```



---

Comment by vdelecroix created at 2022-12-19 13:39:18

Indeed. And adding `arb` afterwards makes it fast again

```
setup 0
5.139136525016511
--------------------------------------------------
setup 1
import sage.all
8.28667692601448
--------------------------------------------------
setup 2
from ctypes import cdll
cdll.LoadLibrary('libpari.so')
cdll.LoadLibrary('libmpfr.so')
8.265609099005815
--------------------------------------------------
setup 3
from ctypes import cdll
cdll.LoadLibrary('libpari.so')
cdll.LoadLibrary('libmpfr.so')
cdll.LoadLibrary('libarb.so')
4.878138452011626
--------------------------------------------------
fast_runs: [0, 3]
slow_runs: [1, 2]
```



---

Comment by tornaria created at 2022-12-19 13:39:26

BTW, it is `libflint.so` that "fixes" it (libarb loads libflint):

```
setup 0

from ctypes import cdll
cdll.LoadLibrary('libpari.so')
cdll.LoadLibrary('libmpfr.so')
cdll.LoadLibrary('libflint.so')

4.84125757496804
--------------------------------------------------
setup 1

from ctypes import cdll
cdll.LoadLibrary('libpari.so')
cdll.LoadLibrary('libmpfr.so')

7.785827331012115
```



---

Comment by tornaria created at 2022-12-19 13:55:59

Something else: running the same test using musl libc (instead of glibc), shows no slowdown at all.


---

Comment by dimpase created at 2022-12-19 14:12:10

Replying to [comment:84 Gonzalo Tornaría]:
> Something else: running the same test using musl libc (instead of glibc), shows no slowdown at all.

This is not a big surprise, as the whole `__tls_get_addr_slow` business appears to be `glibc`-specific.

Does Sage as a whole work on `musl` `libc`?


---

Comment by dimpase created at 2022-12-19 14:17:52

Replying to [comment:78 Fredrik Johansson]:
> I have absolutely no idea what is going on.
> 
> Arb does use TLS but does nothing weird with it, AFAIK. Nor does it do anything weird with GMP.

there is a Dutch proverb "doe maar gewoon, dan doet je al gek genoeg" ("doing things a normal way is already crazy enough")

Could you describe briefly what you do with TLS? How does it play together with initialising (or not) GMP?


---

Comment by fredrik.johansson created at 2022-12-19 15:23:32

TLS is used for thread-local caches in MPFR and Arb for things like the value of pi. There are some caches in FLINT too, including the fmpz cache.

I don't think there is any TLS in GMP. The only mutable state in GMP AFAIK is the set of memory allocation functions.


---

Comment by tornaria created at 2022-12-19 21:21:38

Replying to [comment:85 Dima Pasechnik]:
> Replying to [comment:84 Gonzalo Tornaría]:
> > Something else: running the same test using musl libc (instead of glibc), shows no slowdown at all.
> 
> This is not a big surprise, as the whole `__tls_get_addr_slow` business appears to be `glibc`-specific.
> 
> Does Sage as a whole work on `musl` `libc`?

Works fine, all doctest pass, etc. This is using sage-the-library and all system packages, it may be that we use some patches for system packages that are not in the packages in sage-the-distro.
