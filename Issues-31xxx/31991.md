# Issue 31991: Meta-ticket: Efficient numerical computations with tensor trains

archive/issues_031754.json:
```json
{
    "body": "Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.\n\n- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)\n- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n\nThis functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n\nWe may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. \n\nOne can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n\nIdeally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n\nFor alternative tensor decomposition formats, please see the following survey papers:\n\n- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n- Tensor ring: (https://arxiv.org/abs/1807.02513)\n- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n\nOther implementations:\n- http://tensorly.org/dev/user_guide/tensor_decomposition.html\n\n**Tickets:**\n\n- #31998 Package `ttpy` (tensor trains)\n- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n- #31997 Topological tensor spaces (modules)\n- #29619 tensors should have a sparse iterator\n- #30373 Parent methods `tensor` vs. `tensor_product`\n- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n- #32034 Graphical representations of tensors\n\nCC:  @mkoeppe @egourgoulhon @dimpase\n\nStatus: new\n\nIssue created by migration from https://trac.sagemath.org/ticket/31991\n\n",
    "created_at": "2021-06-16T17:27:03Z",
    "labels": [
        "component: linear algebra"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.8",
    "title": "Meta-ticket: Efficient numerical computations with tensor trains",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/31991",
    "user": "https://github.com/honglizhaobob"
}
```
Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.

- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)
- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).

This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.

We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. 

One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).

Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.

For alternative tensor decomposition formats, please see the following survey papers:

- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
- Tensor ring: (https://arxiv.org/abs/1807.02513)
- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)

Other implementations:
- http://tensorly.org/dev/user_guide/tensor_decomposition.html

**Tickets:**

- #31998 Package `ttpy` (tensor trains)
- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
- #31997 Topological tensor spaces (modules)
- #29619 tensors should have a sparse iterator
- #30373 Parent methods `tensor` vs. `tensor_product`
- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
- #32034 Graphical representations of tensors

CC:  @mkoeppe @egourgoulhon @dimpase

Status: new

Issue created by migration from https://trac.sagemath.org/ticket/31991





---

archive/issue_comments_636415.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,4 @@\n-Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea in fact originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n \n``````\n",
    "created_at": "2021-06-16T17:29:21Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636415",
    "user": "https://github.com/honglizhaobob"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,4 @@
-Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea in fact originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
 This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
 
``````




---

archive/issue_comments_636416.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,20 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation (https://github.com/oseledets/ttpy).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-16T17:35:24Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636416",
    "user": "https://github.com/honglizhaobob"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,20 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation (https://github.com/oseledets/ttpy).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
 
 Comment: 1
 
``````




---

archive/issue_comments_636417.json:
```json
{
    "body": "<a id='comment:5'></a>Is `ttpy` still being maintained? I tried to install it (using `sage -pip install ttpy`) and ran into a Fortran issue:\n\n```\n    Error: Type mismatch between actual argument at (1) and actual argument at (2) (REAL(8)/COMPLEX(8)).\n    tt/tt-fort/ort.f90:271:22:\n    \n      271 |    call dgemv('t',n,r,1.d0,yy,n,x,1, 0.d0,gv,1)\n```",
    "created_at": "2021-06-16T17:41:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636417",
    "user": "https://github.com/mkoeppe"
}
```

<a id='comment:5'></a>Is `ttpy` still being maintained? I tried to install it (using `sage -pip install ttpy`) and ran into a Fortran issue:

```
    Error: Type mismatch between actual argument at (1) and actual argument at (2) (REAL(8)/COMPLEX(8)).
    tt/tt-fort/ort.f90:271:22:
    
      271 |    call dgemv('t',n,r,1.d0,yy,n,x,1, 0.d0,gv,1)
```



---

archive/issue_comments_636418.json:
```json
{
    "body": "<a id='comment:6'></a>(This is with gfortran 11.1 on macOS; this may be similar to the many issues that we had when gfortran 10 came along - #29456)",
    "created_at": "2021-06-16T17:59:32Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636418",
    "user": "https://github.com/mkoeppe"
}
```

<a id='comment:6'></a>(This is with gfortran 11.1 on macOS; this may be similar to the many issues that we had when gfortran 10 came along - #29456)



---

archive/issue_comments_636419.json:
```json
{
    "body": "<a id='comment:7'></a>Replying to [comment:6 mkoeppe]:\n> (This is with gfortran 11.1 on macOS; this may be similar to the many issues that we had when gfortran 10 came along - #29456)\n\n\nI didn't have an issue with downloading since I wasn't using pip but directly cloned the code on github. Based on their github's commit history it doesn't seem to be frequently maintained.. The MATLAB version is what I now use for my projects. I will update the MATLAB link in the ticket description.",
    "created_at": "2021-06-16T18:07:56Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636419",
    "user": "https://github.com/honglizhaobob"
}
```

<a id='comment:7'></a>Replying to [comment:6 mkoeppe]:
> (This is with gfortran 11.1 on macOS; this may be similar to the many issues that we had when gfortran 10 came along - #29456)


I didn't have an issue with downloading since I wasn't using pip but directly cloned the code on github. Based on their github's commit history it doesn't seem to be frequently maintained.. The MATLAB version is what I now use for my projects. I will update the MATLAB link in the ticket description.



---

archive/issue_comments_636420.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,20 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-16T18:08:43Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636420",
    "user": "https://github.com/honglizhaobob"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,20 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
 
 Comment: 1
 
``````




---

archive/issue_comments_636421.json:
```json
{
    "body": "<a id='comment:9'></a>Waouh! This looks promising!",
    "created_at": "2021-06-16T18:57:01Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636421",
    "user": "https://github.com/egourgoulhon"
}
```

<a id='comment:9'></a>Waouh! This looks promising!



---

archive/issue_comments_636422.json:
```json
{
    "body": "<a id='comment:10'></a>I have opened an upstream issue for the compilation errors: https://github.com/oseledets/ttpy/issues/82",
    "created_at": "2021-06-16T19:01:14Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636422",
    "user": "https://github.com/mkoeppe"
}
```

<a id='comment:10'></a>I have opened an upstream issue for the compilation errors: https://github.com/oseledets/ttpy/issues/82



---

archive/issue_comments_636423.json:
```json
{
    "body": "<a id='comment:11'></a>Replying to [comment:9 egourgoulhon]:\n> Waouh! This looks promising!\n\nTo make it clear: the above comment was about the ticket main goal and was not some ironical statement about the ttpy issue with gfortran ;-)",
    "created_at": "2021-06-16T19:12:25Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636423",
    "user": "https://github.com/egourgoulhon"
}
```

<a id='comment:11'></a>Replying to [comment:9 egourgoulhon]:
> Waouh! This looks promising!

To make it clear: the above comment was about the ticket main goal and was not some ironical statement about the ttpy issue with gfortran ;-)



---

archive/issue_comments_636424.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,20 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-16T19:26:32Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636424",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,20 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
 
 Comment: 1
 
``````




---

archive/issue_comments_636425.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,20 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-17T17:34:07Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636425",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,20 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
 
 Comment: 1
 
``````




---

archive/issue_comments_636426.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,3 +1,29 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n+\n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n \n \n Comment: 1\n``````\n",
    "created_at": "2021-06-17T17:46:47Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636426",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,3 +1,29 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
+
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
 
 
 Comment: 1
``````




---

archive/issue_comments_636427.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,30 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-18T01:45:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636427",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,30 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
 
 Comment: 1
 
``````




---

archive/issue_comments_636428.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,31 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n+- Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-18T20:45:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636428",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,31 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
+- Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
 
 Comment: 1
 
``````




---

archive/issue_comments_636429.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,31 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named \"Density Matrix Renormalization Group (DMRG)\" and later popularized by Prof. Ivan Oseledets. \n \n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-18T20:45:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636429",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,31 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named "Density Matrix Renormalization Group (DMRG)" and later popularized by Prof. Ivan Oseledets. 
 
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization), currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be TensorFlow, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+For the theory behind why one can reduce a normally exponential (in the number of dimensions) task to a linear task, see Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
 
 Comment: 1
 
``````




---

archive/issue_comments_636430.json:
```json
{
    "body": "<a id='comment:18'></a>slightly modified description",
    "created_at": "2021-06-19T21:27:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636430",
    "user": "https://github.com/honglizhaobob"
}
```

<a id='comment:18'></a>slightly modified description



---

archive/issue_comments_636431.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,32 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.\n \n+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)\n+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-19T21:27:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636431",
    "user": "https://github.com/honglizhaobob"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,32 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.
 
+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)
+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
 
 Comment: 1
 
``````




---

archive/issue_comments_636432.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,33 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.\n \n+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)\n+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n+- #32034 Graphical representations of tensors\n \n Comment: 1\n \n``````\n",
    "created_at": "2021-06-22T18:37:30Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636432",
    "user": "https://github.com/honglizhaobob"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,33 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.
 
+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)
+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
+- #32034 Graphical representations of tensors
 
 Comment: 1
 
``````




---

archive/issue_events_084603.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2021-07-19T01:16:42Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.5",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84603"
}
```



---

archive/issue_events_084604.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2021-12-14T02:04:49Z",
    "event": "demilestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.5",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84604"
}
```



---

archive/issue_events_084605.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2021-12-14T02:04:49Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.6",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84605"
}
```



---

archive/issue_events_084606.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-04-01T21:16:35Z",
    "event": "demilestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.6",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84606"
}
```



---

archive/issue_events_084607.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-04-01T21:16:35Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.7",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84607"
}
```



---

archive/issue_events_084608.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-08-31T02:51:13Z",
    "event": "demilestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.7",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84608"
}
```



---

archive/issue_events_084609.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-08-31T02:51:13Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "milestone": "sage-9.8",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/31991#event-84609"
}
```



---

archive/issue_comments_636433.json:
```json
{
    "body": "Description changed:\n``````diff\n--- \n+++ \n@@ -1,4 +1,35 @@\n+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.\n \n+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)\n+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295\u20132317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).\n+\n+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.\n+\n+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. \n+\n+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).\n+\n+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.\n+\n+For alternative tensor decomposition formats, please see the following survey papers:\n+\n+- Tucker format: (Falc\u00f3, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)\n+- Tensor ring: (https://arxiv.org/abs/1807.02513)\n+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)\n+\n+Other implementations:\n+- http://tensorly.org/dev/user_guide/tensor_decomposition.html\n+\n+**Tickets:**\n+\n+- #31998 Package `ttpy` (tensor trains)\n+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)\n+- #31997 Topological tensor spaces (modules)\n+- #29619 tensors should have a sparse iterator\n+- #30373 Parent methods `tensor` vs. `tensor_product`\n+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`\n+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.\n+- #32034 Graphical representations of tensors\n \n Comment: 1\n \n``````\n",
    "created_at": "2022-09-05T06:56:08Z",
    "issue": "https://github.com/sagemath/sagetest/issues/31991",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/31991#issuecomment-636433",
    "user": "https://github.com/mkoeppe"
}
```

Description changed:
``````diff
--- 
+++ 
@@ -1,4 +1,35 @@
+Following the progress with Ticket #30307, we propose to add computational functionalities with tensors via the tensor train decomposition, which is an idea originally named Matrix Product States in the quantum physics community and later popularized by Prof. Ivan Oseledets. As a computational tool, it represents tensors in compressed formats that would enable tensor operations with cost scaling only linearly in the number of dimensions.
 
+- Affleck et al., Valence bond ground states in isotropic quantum antiferromagnets (https://link.springer.com/article/10.1007/BF01218021)
+- Oseledets, Tensor-Train Decomposition, SIAM J. Sci. Comput., 33(5), 2295–2317 (https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3).
+
+This functionality is especially useful for high dimensional tasks to prevent the curse of dimensionality (such as in Riemannian optimization). Numerical packages are currently supported in MATLAB, Python, and C++. The MATLAB version is currently most versatile and robust.
+
+We may consider internalizing this computational library for Sage, as an effort to formalize the currently available implementations. This addition can either be an additional child class under `tensor`, and overwrite the arithmetics; or a separate module. The backend is currently proposed to be `tensorflow`, however this may change / be flexible based on how the refactoring of `Components` progresses. 
+
+One can look at this github page for an example implementation in MATLAB (https://github.com/oseledets/TT-Toolbox).
+
+Ideally after the implementation of this ticket, users can specify a fixed rank tensor for a general manifold using `tensor.modules`, specify a ring and a frame, and perform useful and normally computationally intractable tasks using tensor train as a backend. Based on the progress of this ticket, one may also consider implementing other backends for storing a numerical tensor such as tensor rings and quantized tensor train.
+
+For alternative tensor decomposition formats, please see the following survey papers:
+
+- Tucker format: (Falcó, Hackbusch, Nouy, Tree-based tensor formats, https://arxiv.org/abs/1810.01262)
+- Tensor ring: (https://arxiv.org/abs/1807.02513)
+- Quantized tensor train (see section 3.5.1): (https://refubium.fu-berlin.de/handle/fub188/3366)
+
+Other implementations:
+- http://tensorly.org/dev/user_guide/tensor_decomposition.html
+
+**Tickets:**
+
+- #31998 Package `ttpy` (tensor trains)
+- #31992 `FiniteRankFreeModuleMorphism`: Add method SVD (singular value decomposition)
+- #31997 Topological tensor spaces (modules)
+- #29619 tensors should have a sparse iterator
+- #30373 Parent methods `tensor` vs. `tensor_product`
+- #30235 Add construction methods to `FiniteRankFreeModule` and `CombinatorialFreeModule`
+- #30309 Meta-ticket: Unify free module elements API: methods `dict`, `monomial_coefficients`, etc.
+- #32034 Graphical representations of tensors
 
 Comment: 1
 
``````

