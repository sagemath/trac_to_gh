# Issue 29241: Add a memory bound in discrete_log function

Issue created by migration from https://trac.sagemath.org/ticket/29478

Original creator: @sylvainpelissier

Original creation time: 2020-04-08 10:18:23

Keywords: discrete_log groups

In the `discrete_log` function of **groups/generic.py** file, after the subgroup separation, the Baby step Giant step (`bsgs` function) algorithm will be used in each subgroup. However, bsgs requires O(sqrt(order)) space complexity. If the system memory is not big enough the system will become unstable and crash. Nevertheless with the `discrete_log_lambda` for the same subgroup size the computation would take longer but finishes normally since it use O(log(order)) space complexity.

For example with the following:

```
F = GF(140737488355333)
E = EllipticCurve(F,[1,139])
k = int(F.random_element()) % order
G = E.gen(0)
Q = k * G
x = bsgs(G, Q, [0,order], operation="+")
```


On my computer, the function bsg will consume too much memory and won't finish but the **discrete_log_lambda** function will finish properly after few hours.

I suggest to check the available memory in the system with the **virtual_memory_limit** function and if the ratio of the group element size times sqrt(order) over the memory available is to high to fall back using the `discrete_log_lambda` maybe with a warning. I do not know how to have a correct estimation of a group element size since `getsizeof` function does not give a correct result.


---

Comment by mkoeppe created at 2020-04-14 19:41:51

Batch modifying tickets that will likely not be ready for 9.1, based on a review of the ticket title, branch/review status, and last modification date.


---

Comment by mkoeppe created at 2021-02-13 20:51:01

Setting new milestone based on a cursory review of ticket status, priority, and last modification date.
