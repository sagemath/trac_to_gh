# Issue 34487: echelonize() on RR matrices should not use "classical" algorithm

Issue created by migration from https://trac.sagemath.org/ticket/34724

Original creator: dimpase

Original creation time: 2022-11-05 11:14:43

CC:  mkoeppe

From [sage-devel](https://groups.google.com/d/msgid/sage-devel/bec2d18e-8034-440c-8076-9a5e9ec93d2cn%40googlegroups.com):

Inverting RR-matrix

```
sage: m
[-6.12323399573677e-17     -1.72508242466029]
[    0.579682446302195  6.12323399573677e-17]
sage: m.parent()
Full MatrixSpace of 2 by 2 dense matrices over Real Field with 53 bits of precision
```


because it uses classical echelonize:

```
sage: A=m.augment(identity_matrix(RR,2))
sage: A
[-6.12323399573677e-17     -1.72508242466029      1.00000000000000     0.000000000000000]
[    0.579682446302195  6.12323399573677e-17     0.000000000000000      1.00000000000000]
sage: A.echelonize()
sage: A
[     1.00000000000000     0.000000000000000      4.00000000000000      1.72508242466029]
[    0.000000000000000      1.00000000000000    -0.579682446302195 -6.12323399573676e-17]
```

instead of 

```
sage: A.echelonize(algorithm='scaled_partial_pivoting');A
[     1.00000000000000     0.000000000000000  6.12323399573677e-17      1.72508242466029]
[    0.000000000000000      1.00000000000000    -0.579682446302195 -6.12323399573677e-17]
```



---

Comment by nbruin created at 2022-11-06 00:34:34

This issue has been around and has spawned ad-hoc workarounds: [numerical_inverse](https://github.com/sagemath/sage/blob/c3028e74d80d7f8473b3e421ba6d54ab95f288f9/src/sage/schemes/riemann_surfaces/riemann_surface.py#L251). There may be more appropriate backends to tap into than the generic algorithms we have implemented, for particular implementations of some numerical types.

(Of course even partial pivoting doesn't absolve you from thinking about numerical stability)


---

Comment by dimpase created at 2022-11-06 10:42:21

Replying to [comment:3 Nils Bruin]:
> This issue has been around and has spawned ad-hoc workarounds: [numerical_inverse](https://github.com/sagemath/sage/blob/c3028e74d80d7f8473b3e421ba6d54ab95f288f9/src/sage/schemes/riemann_surfaces/riemann_surface.py#L251). 

> There may be more appropriate backends to tap into than the generic algorithms we have implemented, for particular implementations of some numerical types.
> 
> (Of course even partial pivoting doesn't absolve you from thinking about numerical stability)

of course, but why such  sloppy coding in `echelonize()` ? It literally reads as "we care about p-adics, the rest can go and die from numerical instability hell" :-)

`​numerical_inverse` must be dog-slow. (convert data to ball arithmetics, call mpmath to do LU, convert back...)
In general, one should do SVD for inverting, this seems to be the best for badly conditioned data.


---

Comment by nbruin created at 2022-11-06 19:33:57

Replying to [comment:4 Dima Pasechnik]:
> `​numerical_inverse` must be dog-slow. (convert data to ball arithmetics, call mpmath to do LU, convert back...)
> In general, one should do SVD for inverting, this seems to be the best for badly conditioned data.
Well ... the conversions are `O((n*p)^2) bitoperations for an `n x n` matrix of precision `p`, so it's not dominant. But constants matter of course :-). The code for conversion between mpmath and CC is relatively efficient because the conversion routine does know there's mpfr underneath for both.

I think I ended up with and LUP decomposition because there was easy code accessible for that, whereas SVD was harder to find and/or required more complex interfacing to be written to arrive at an inverse. It's really more a tie-over until we have truly well-performing and fairly stable numerical inversion.

I mainly pointed out the ad-hoc routine because I suspect these occur in other parts of sagemath as well. The current routine seems to have been stable enough (these matrices tend to not be super-badly conditioned; it's just that classical pivoting tends to be unstable even for well-conditioned matrices) and not a choke point for performance. But if we end up with something more reliable and more performant it will be worth adapting it!


---

Comment by dimpase created at 2022-11-07 12:13:16

anyhow, what's wrong with doing

```diff
--- a/src/sage/matrix/matrix2.pyx
+++ b/src/sage/matrix/matrix2.pyx
@@ -7617,7 +7617,11 @@ cdef class Matrix(Matrix1):
                 except (AttributeError, TypeError):
                     algorithm = 'scaled_partial_pivoting_valuation'
             else:
-                algorithm = 'classical'
+                try:
+                    self.base_ring(1/2).abs()
+                    algorithm = 'scaled_partial_pivoting'
+                except (AttributeError, ArithmeticError, TypeError):
+                    algorithm = 'classical'
         try:
             if self.base_ring() in _Fields:
                 if algorithm in ['classical', 'partial_pivoting', 'scaled_partial_pivoting', 'scaled_partial_pivoting_valuation']:
```


Then for the case at hand we have sanity:

```python
sage: m=matrix(RR,[[-6.12323399573677e-17, -1.72508242466029], [ 0.579682446302195,  6.12323399573677e-17]])
sage: ~m*m
[     1.00000000000000  1.23259516440783e-32]
[-6.16297582203915e-33      1.00000000000000]
sage: (~m*m).norm()
1.0
```

as opposed to, in the unpatched case,

```python
sage: ~m
[     2.00000000000000      1.72508242466029]
[   -0.579682446302196 -6.12323399573678e-17]
sage: (~m*m).norm()
3.719050672118565
```



---

Comment by nbruin created at 2022-11-07 16:51:17

Replying to [comment:6 Dima Pasechnik]:
> anyhow, what's wrong with doing
> {{{#!diff
> +                try:
> +                    self.base_ring(1/2).abs()
> +                    algorithm = 'scaled_partial_pivoting'
> +                except (AttributeError, ArithmeticError, TypeError):
> +                    algorithm = 'classical'
The idea of using a heuristic to decide on the default pivoting strategy seems good to me. I'm not sure that the proposed heuristic is the right one. For instance, the proposed one would also use `scaled_partial_pivoting` on `QQ` and I'm not sure that's desirable (it's certainly a change that's outside the scope of this ticket).

Alternatives I can think of:
 - use category framework on the base ring to find out what to do (I wasn't able to find an apprpriate property there
 - have a property or a method on the parent that provides a default -- if that fails, fall back on the old strategy.
 - have a subclassing mechanism to provide appropriate echelonization approaches (the category framework with its "axiom"s does that, but I'm not sure if that's an appropriate mechanism for matrices).

I would lean to the second approach as striking a reasonable balance between explicit intent in the code and avoiding overengineering. In this case, `RealField` and `ComplexField` would grow a method `_matrix_pivoting_strategy` that just returns `scaled_partial_pivoting`. Introduction of such default preference expressions could grow slowly, as needed and it limits behaviour changes to those cases where it's explicitly desired.

It would be appealing to communicate an actual callable instead of a string indicating a choice elsewhere, but that might complicate the design and introduce circular imports.


---

Comment by dimpase created at 2022-11-07 18:04:20

We can improve the heuristic by dispatching on `self.base_ring().is_exact()` - if true, we do `classical`, as before, otherwise `scaled_partial_pivoting`. In particular this will preserve 
the current setting for `QQ`, etc.


---

Comment by nbruin created at 2022-11-07 20:52:59

Replying to [comment:8 Dima Pasechnik]:
> We can improve the heuristic by dispatching on `self.base_ring().is_exact()` - if true, we do `classical`, as before, otherwise `scaled_partial_pivoting`. In particular this will preserve 
> the current setting for `QQ`, etc.

Ah yes, that should be a clear improvement over the status quo.

Of course, even over QQ, a classical row reduction shouldn't be the default. Some kind of multimodular approach should have much better performance, but would need care to be properly implemented. So at some point we probably need more extensive subclassing of matrix classes, with their own echelon_form implementations.

*EDIT:* That subclassing has already happened. `echelonize` on a matrix over the rationals comes from `sage.matrix.matrix_rational_dense`, whereas the one over RR comes from `sage.matrix.matrix2`; same for `RealDoubleField()`. So to some extent, we are running into these problems because we didn't properly subclass matrix spaces for `RealField`, `ComplexField`, etc. As a result, the change-of-default would not affect matrices over QQ. Still, it's good to be conservative in changing the default, I think.

According to mpfr's page, https://www.alglib.net/ for real and https://eigen.tuxfamily.org/ for complex would be pre-existing libraries to tap into. One would think having some decent multiprecision linear algebra would be worthwhile? Perhaps a GSOC project? Not a very exciting one ...


---

Comment by dimpase created at 2022-11-08 09:45:13

New commits:


---

Comment by dimpase created at 2022-11-08 12:57:04

It's not clear how to deal with SR matrices (SR is not exact, but we can only do classic echelonize on it) like this, as SR is split out to its own package, and
so checking whether the base ring is SR directly doesn't seem to be possible, due to this structure - as importing SR is no good.

So further subclassing (?) seems hard to aviod - any ideas?


---

Comment by dimpase created at 2022-11-08 12:57:04

Changing status from new to needs_info.


---

Comment by dimpase created at 2022-11-08 13:54:46

while working on it I found a juicy test bug in `pseudoinverse` for an `RR` matrix `M`.
The pseudoinverse in the docstring, computed with `M.pseudoinverse(algorithm="exact")`,
is very bad quality, compared to `M.pseudoinverse(algorithm="numpy")`. I'm changing this test to a meaningful one.


---

Comment by git created at 2022-11-08 16:27:37

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by nbruin created at 2022-11-08 16:28:49

Replying to [comment:11 Dima Pasechnik]:
> So further subclassing (?) seems hard to aviod - any ideas?
For SR the subclassing has already happened:

```
sage: M=matrix(SR,2,2,[1,0,0,1])
sage: type(M)
<class 'sage.matrix.matrix_symbolic_dense.Matrix_symbolic_dense'>
```

so a stub `echelonize` implementation on `Matrix_symbolic_dense` with a different default and then just chains to the `echelonize` on super would do the trick.

Alternatively, we could have a class attribute on matrix types that gives the default echelonization algorithm and then just inherit or override it on the subclass.

For RR and CC the subclassing hasn't happened yet, so there we don't have a chance of overriding the default. But then the `is_exact()` heuristic is pretty good.

I think most performant matrix types would need an `echelonize` that is specific to the type of matrix, so I expect most of them to NOT chain. Indeed, for matrices over QQ there's a specific routine with its specific slew of algorithms and defaults.

I'd say a straightforward guide going forward is: if you need `echelonize` to behave differently (for instance, make it have a different default), then override it and possibly chain into super if you don't mind the penalty.

The generic default depending on `is_exact` is just a matter of being nice and reducing the cases where an override is required (in this case it delays the need to subclass for `RR, CC, RealBall, ComplexBall, RealDouble` etc.)

The alternative of having a class attribute encoding the default is not very robust, especially in the face of possible method overrides on subclasses. I dislike promoting solutions that increase call depth, but in this case I think an extra layer of indirection using exactly the tools that python offers for it gives a clean and at the moment fairly easily implemented solution.


---

Comment by git created at 2022-11-08 17:42:30

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by dimpase created at 2022-11-08 17:43:49

> so a stub echelonize implementation on Matrix_symbolic_dense with a different default and then just chains to the echelonize on super would do the trick. 

Does `cce62c2` do this? It seems, yes.


---

Comment by dimpase created at 2022-11-08 17:57:20

There is numerical noise in tests, mostly pointing at better quality of linear algebra.
E.g. in `riemann_surface` there is `numerical_inverse`, and the doctest saying

```
sage: max(abs(c) for c in (C^(-1)*C-C^0).list()) < 1e-10
False
```


But now it's almost as good and `numerical_inverse`:


```
sage: max(abs(c) for c in (numerical_inverse(C)*C-C^0).list())
2.37143742013377e-16
sage: max(abs(c) for c in (C^(-1)*C-C^0).list())
4.71027737605132e-16
```



---

Comment by dimpase created at 2022-11-08 18:00:27

I still need to add fixes for numerical noise - are you happy with the fix otherwise?


---

Comment by dimpase created at 2022-11-08 18:00:27

Changing status from needs_info to needs_review.


---

Comment by nbruin created at 2022-11-08 18:03:26

Replying to [comment:16 Dima Pasechnik]:
> > so a stub echelonize implementation on Matrix_symbolic_dense with a different default and then just chains to the echelonize on super would do the trick. 
> 
> Does `cce62c2` do this? It seems, yes.

Not quite. The chain call should probably respect the `algorithm` parameter by passing `algorithm=algorithm`. If you think it shouldn't be possible to select algorithm on symbolic variables, then the parameter `algorithm` should be removed from the signature. Then passing the fixed choice to super is good of course.


---

Comment by nbruin created at 2022-11-08 20:50:26

Replying to [comment:18 Dima Pasechnik]:
> I still need to add fixes for numerical noise - are you happy with the fix otherwise?

Yes! For comparing to numerical_inverse: based on some randomly generated matrices with complex numbers: it's significantly faster for many matrices; I haven't seen it slower; and the accuracy, while not exactly the same, is similar (sometimes a bit better, sometimes a bit worse, but it looks like the normal numerical noise one would expect).

This is a nice improvement. Had I read the inversion routine and that it just uses `echelonize`, and that there is a pivoting strategy for it, I would have used that rather than interface with arb.


---

Comment by git created at 2022-11-09 13:37:41

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by dimpase created at 2022-11-09 13:42:49

How would one do subclassing for `ComplexField`, `RealField`, etc? Would it need something like [PEP 646 – Variadic Generics](https://peps.python.org/pep-0646/) ?


---

Comment by nbruin created at 2022-11-09 23:16:28

Replying to [comment:22 Dima Pasechnik]:
> How would one do subclassing for `ComplexField`, `RealField`, etc? Would it need something like [PEP 646 – Variadic Generics](https://peps.python.org/pep-0646/) ?

I would expect that a high-performance version would not be generic at all, but hardcoded instead: while some current generic matrix operations are "in-place", they cause a lot of memory churn due to our mpfr wrappers having an extra level of indirection. The lower-level arithmetic routines of mpfr can work truly in-place. Especially for shorter length precision (remember we use mpfr for 53 bit precision as well!).

So I would expect that a dedicated matrix type for `RealField` matrices would have the bitstrings of the entries stored contiguously, and something similar for `ComplexField`. How that would work for ball fields I don't know, but I imagine something similar is possible (I don't know if that would be worth it, though). I did some of that on an ad-hoc basis in [RiemannTheta](https://github.com/nbruin/RiemannTheta), although it's really more arrays, vectors and inner products there -- I didn't really need linear algebra.

You could probably template that code a bit, but it would probably be more on the level of cython (where you can use C++ templates if I'm not mistaken) in order to interface efficiently with mpfr.

In terms of what would be minimally required: just subclass a class "matrix over inexact archimedian normed field" or something like that and put appropriate functionality there. It would basically be coded in the same generic way that the current generic_matrix is implemented, just with appropriate algorithm choices (and SVD!!!). That would be more for functionality support than for efficiency gains.


---

Comment by git created at 2022-11-10 17:08:15

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by dimpase created at 2022-11-10 17:12:07

Replying to [comment:23 Nils Bruin]:
[...]
> How that would work for ball fields I don't know,
AFAIK, `arb` has quite a bit of linear algebra nowadays
(I guess, these routines are on boxed data, to make them universal, but still...)

Anyhow, this ticket is now ready for review.


---

Comment by nbruin created at 2022-11-10 22:20:12

Replying to [comment:25 Dima Pasechnik]:
> Replying to [comment:23 Nils Bruin]:
> [...]
> > How that would work for ball fields I don't know,
> AFAIK, `arb` has quite a bit of linear algebra nowadays
> (I guess, these routines are on boxed data, to make them universal, but still...)

Indeed! Some of it may have been wrapped already in `matrix_complex_ball_dense.pyx`, which I just found exists...

> 
> Anyhow, this ticket is now ready for review.


---

Comment by nbruin created at 2022-11-10 22:33:03

LGTM


---

Comment by nbruin created at 2022-11-10 22:33:03

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2022-11-15 23:43:19

Resolution: fixed
