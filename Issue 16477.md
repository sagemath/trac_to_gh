# Issue 16477: Add a matrix of constraints in a LP

archive/issues_016477.json:
```json
{
    "body": "CC:  vbraun dimpase ingolfured novoselt\n\nSome users reported two problems with Sage's interface with LP solvers.\n\n1) It is slow to add constraints one by one to the solver's data structure\n\n2) It should be possible to add a matrix of constraints all at once\n\nWith a new syntax like that (thanks Dima), both problems can be circumvented:\n\n\n```\np = MixedIntegerLinearProgram()\nx = p.new_variable()\np.add_constraint(A_matrix*x <= a_vector)\n```\n\n\nThis would consider `x` as the vector `x[0],x[1],...,x[size_of_the_matrix - 1]`.\n\nIn the `add_constraint` function, we can then call the backend's function to add 10 000 constraints at the same time (probably better for the data structure), and then fill the entries.\n\nNathann\n\nIssue created by migration from https://trac.sagemath.org/ticket/16714\n\n",
    "created_at": "2014-07-25T14:40:03Z",
    "labels": [
        "linear programming",
        "major",
        "enhancement"
    ],
    "title": "Add a matrix of constraints in a LP",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/16477",
    "user": "ncohen"
}
```
CC:  vbraun dimpase ingolfured novoselt

Some users reported two problems with Sage's interface with LP solvers.

1) It is slow to add constraints one by one to the solver's data structure

2) It should be possible to add a matrix of constraints all at once

With a new syntax like that (thanks Dima), both problems can be circumvented:


```
p = MixedIntegerLinearProgram()
x = p.new_variable()
p.add_constraint(A_matrix*x <= a_vector)
```


This would consider `x` as the vector `x[0],x[1],...,x[size_of_the_matrix - 1]`.

In the `add_constraint` function, we can then call the backend's function to add 10 000 constraints at the same time (probably better for the data structure), and then fill the entries.

Nathann

Issue created by migration from https://trac.sagemath.org/ticket/16714





---

archive/issue_comments_216633.json:
```json
{
    "body": "This would also be very natural syntax to specify polyhedra.",
    "created_at": "2014-08-02T19:20:15Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216633",
    "user": "dimpase"
}
```

This would also be very natural syntax to specify polyhedra.



---

archive/issue_comments_216634.json:
```json
{
    "body": "Looks like a duplicate of #7293... (which has no code and no comments)",
    "created_at": "2014-08-05T06:21:00Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216634",
    "user": "jdemeyer"
}
```

Looks like a duplicate of #7293... (which has no code and no comments)



---

archive/issue_comments_216635.json:
```json
{
    "body": "One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.",
    "created_at": "2014-08-05T14:57:55Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216635",
    "user": "vbraun"
}
```

One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.



---

archive/issue_comments_216636.json:
```json
{
    "body": "> One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.\n\nI don't understand `O_o`\n\nWe do not mind if two matrices are multiplied, we mind when two LP variables are multiplied together. We just have to scream when two variables are multiplied, don't we ? So if a guy multiplies two matrices containing variables it will try to multiply the variables at some point so we are safe, aren't we ?\n\nNathann",
    "created_at": "2014-08-05T15:01:31Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216636",
    "user": "ncohen"
}
```

> One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.

I don't understand `O_o`

We do not mind if two matrices are multiplied, we mind when two LP variables are multiplied together. We just have to scream when two variables are multiplied, don't we ? So if a guy multiplies two matrices containing variables it will try to multiply the variables at some point so we are safe, aren't we ?

Nathann



---

archive/issue_comments_216637.json:
```json
{
    "body": "Replying to [comment:3 vbraun]:\n> One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.\n\nbut why would you want this? All that is needed is to pull the matrix apart, row by row, and create a linear function from each of them...",
    "created_at": "2014-08-05T15:02:34Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216637",
    "user": "dimpase"
}
```

Replying to [comment:3 vbraun]:
> One technical problem is that matrices in Sage can always be multiplied, so you can't create matrices over linear functions.

but why would you want this? All that is needed is to pull the matrix apart, row by row, and create a linear function from each of them...



---

archive/issue_comments_216638.json:
```json
{
    "body": "what might help, conceptually, if one can do linear maps just as well as linear functions. Then, when, say, A and B are matrices, one could imagine to do\n\n```\nx = p.new_variable()\ny = p.new_variable()\np.add_constraint(A*x+B*y <= c)\n```\n\nso that `A*x+B*y` is a linear map from the space of dimension `A.ncol()+B.ncol()` to the space of dimension `len(c)`.",
    "created_at": "2014-08-05T15:08:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216638",
    "user": "dimpase"
}
```

what might help, conceptually, if one can do linear maps just as well as linear functions. Then, when, say, A and B are matrices, one could imagine to do

```
x = p.new_variable()
y = p.new_variable()
p.add_constraint(A*x+B*y <= c)
```

so that `A*x+B*y` is a linear map from the space of dimension `A.ncol()+B.ncol()` to the space of dimension `len(c)`.



---

archive/issue_comments_216639.json:
```json
{
    "body": "I've started with tensors (over the base ring) of linear functions and free modules. This implements:\n\n```\nsage: m = matrix([[1,2],[3,4]]);  m\n[1 2]\n[3 4]\nsage: p = MixedIntegerLinearProgram()\nsage: v = p.new_variable(nonnegative=True)\nsage: m.row(0) * v[0]              # vector * linear function\n(1.0, 2.0)*x_0\nsage: m * v[0]                     # matrix * linear function\n[x_0   2*x_0]\n[3*x_0 4*x_0]\nsage: v * m                        # MIPVariable * matrix\n(3.0, 4.0)*x_1 + (1.0, 2.0)*x_0\n```\n\nThe last multiplication including MIPvariables is incomplete: You can't write `m * v` since MIPvariables do not fit into the parent/element framework, so they don't really participate in coercion. This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram. Once that is done it'll be easy to add (in)equalities of the new tensor elements.\n----\nNew commits:",
    "created_at": "2014-08-07T01:27:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216639",
    "user": "vbraun"
}
```

I've started with tensors (over the base ring) of linear functions and free modules. This implements:

```
sage: m = matrix([[1,2],[3,4]]);  m
[1 2]
[3 4]
sage: p = MixedIntegerLinearProgram()
sage: v = p.new_variable(nonnegative=True)
sage: m.row(0) * v[0]              # vector * linear function
(1.0, 2.0)*x_0
sage: m * v[0]                     # matrix * linear function
[x_0   2*x_0]
[3*x_0 4*x_0]
sage: v * m                        # MIPVariable * matrix
(3.0, 4.0)*x_1 + (1.0, 2.0)*x_0
```

The last multiplication including MIPvariables is incomplete: You can't write `m * v` since MIPvariables do not fit into the parent/element framework, so they don't really participate in coercion. This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram. Once that is done it'll be easy to add (in)equalities of the new tensor elements.
----
New commits:



---

archive/issue_comments_216640.json:
```json
{
    "body": "Hellooooo !\n\n> This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.\n\nCan it result in any slowdown for the usual usage of those variables ?\n\nNathann",
    "created_at": "2014-08-07T07:50:56Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216640",
    "user": "ncohen"
}
```

Hellooooo !

> This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.

Can it result in any slowdown for the usual usage of those variables ?

Nathann



---

archive/issue_comments_216641.json:
```json
{
    "body": "Replying to [comment:9 ncohen]:\n> Hellooooo !\n> \n> > This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.\n> \n> Can it result in any slowdown for the usual usage of those variables ?\nI'd be surprised.\nThe real bottleneck is in interacting with the solver and solving itself, anyway.\n\nBesides, the current prevalent use of MIP frontend is very inefficient, as you collect the linear system into a matrix, internally, and the backend is called each time a new constraint is added, right? It's certainly quicker to pass the matrix directly and call the backend once.",
    "created_at": "2014-08-07T09:25:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216641",
    "user": "dimpase"
}
```

Replying to [comment:9 ncohen]:
> Hellooooo !
> 
> > This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.
> 
> Can it result in any slowdown for the usual usage of those variables ?
I'd be surprised.
The real bottleneck is in interacting with the solver and solving itself, anyway.

Besides, the current prevalent use of MIP frontend is very inefficient, as you collect the linear system into a matrix, internally, and the backend is called each time a new constraint is added, right? It's certainly quicker to pass the matrix directly and call the backend once.



---

archive/issue_comments_216642.json:
```json
{
    "body": "Replying to [comment:10 dimpase]:\n> Replying to [comment:9 ncohen]:\n> > Hellooooo !\n> > \n> > > This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.\n> > \n> > Can it result in any slowdown for the usual usage of those variables ?\n> I'd be surprised.\n> The real bottleneck is in interacting with the solver and solving itself, anyway.\n\nWell, I need to compute a max matching on a large bipartite graph for some designs I build these days, and ...\n\n\n```\nsage: g=graphs.CompleteBipartiteGraph(100,200)\nsage: %time _=g.matching(algorithm=\"LP\", verbose=2)\n...\n\nRoot node processing (before b&c):\n  Real time             =    0.17 sec. (104.83 ticks)\nParallel b&c, 4 threads:\n  Real time             =    0.00 sec. (0.00 ticks)\n  Sync time (average)   =    0.00 sec.\n  Wait time (average)   =    0.00 sec.\n                          ------------\nTotal (root+branch&cut) =    0.17 sec. (104.83 ticks)\nCPU times: user 1.25 s, sys: 24 ms, total: 1.28 s\nWall time: 1.06 s\n```\n\n\nSage reports more than 1second and the solver report 0.17. I don't really know where the computations go to be honest at %prun does not say much. And I suspect that it does not say much because the code that takes time is written in Cython, i.e. the LP variables. But I haven't found a proof yet.\n\n> Besides, the current prevalent use of MIP frontend is very inefficient, as you collect the linear system into a matrix, internally, and the backend is called each time a new constraint is added, right? It's certainly quicker to pass the matrix directly and call the backend once.\n\nIt may be faster, but the two must be possible. You don't want to have to build a matrix for any of these problems, the code would be ugly as hell. Being able to index variables with \"anything\" certainly adds something, and not only for readability.\n\nhttp://steinertriples.fr/ncohen/tut/LP_examples/\n\nNathann",
    "created_at": "2014-08-07T09:32:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216642",
    "user": "ncohen"
}
```

Replying to [comment:10 dimpase]:
> Replying to [comment:9 ncohen]:
> > Hellooooo !
> > 
> > > This needs to be fixed, probably best by making MIPvariables the elements of the parent MixedIntegerLinearProgram.
> > 
> > Can it result in any slowdown for the usual usage of those variables ?
> I'd be surprised.
> The real bottleneck is in interacting with the solver and solving itself, anyway.

Well, I need to compute a max matching on a large bipartite graph for some designs I build these days, and ...


```
sage: g=graphs.CompleteBipartiteGraph(100,200)
sage: %time _=g.matching(algorithm="LP", verbose=2)
...

Root node processing (before b&c):
  Real time             =    0.17 sec. (104.83 ticks)
Parallel b&c, 4 threads:
  Real time             =    0.00 sec. (0.00 ticks)
  Sync time (average)   =    0.00 sec.
  Wait time (average)   =    0.00 sec.
                          ------------
Total (root+branch&cut) =    0.17 sec. (104.83 ticks)
CPU times: user 1.25 s, sys: 24 ms, total: 1.28 s
Wall time: 1.06 s
```


Sage reports more than 1second and the solver report 0.17. I don't really know where the computations go to be honest at %prun does not say much. And I suspect that it does not say much because the code that takes time is written in Cython, i.e. the LP variables. But I haven't found a proof yet.

> Besides, the current prevalent use of MIP frontend is very inefficient, as you collect the linear system into a matrix, internally, and the backend is called each time a new constraint is added, right? It's certainly quicker to pass the matrix directly and call the backend once.

It may be faster, but the two must be possible. You don't want to have to build a matrix for any of these problems, the code would be ugly as hell. Being able to index variables with "anything" certainly adds something, and not only for readability.

http://steinertriples.fr/ncohen/tut/LP_examples/

Nathann



---

archive/issue_comments_216643.json:
```json
{
    "body": "Replying to [comment:8 vbraun]:\n> I've started with tensors (over the base ring) of linear functions and free modules. This implements:\n> {{{\n> ...\n> sage: v * m                        # MIPVariable * matrix\n> (3.0, 4.0)*x_1 + (1.0, 2.0)*x_0\n> }}}\n\nthis is very nice; I hope this can be a basis for implementing an interface to \"conic programming\". That is, whenever you have a convex cone K in `R^n` (e.g. the positive ortant, or `{(x_0,...,x_{n-1}) | x_0^2 >= sum_{j=1}^{n-1} x_j^2}`, known as \"icecream cone\") you have a partial order `<_K` on `R^n` defined by `x<_K y` iff y-k is in K.\n\nthen one can do linear optimisation on the intersection of K with an affine subspace. For K being the positive ortant this is just the usual LP; for K the icecream cone this is a kind of norm minimisation, etc. CVXOPT has implementations for several different cones like this (by the way, semidefinite programming is yet another example, K being the cone of psd matrices in `R^nxn`).",
    "created_at": "2014-08-07T09:39:10Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216643",
    "user": "dimpase"
}
```

Replying to [comment:8 vbraun]:
> I've started with tensors (over the base ring) of linear functions and free modules. This implements:
> {{{
> ...
> sage: v * m                        # MIPVariable * matrix
> (3.0, 4.0)*x_1 + (1.0, 2.0)*x_0
> }}}

this is very nice; I hope this can be a basis for implementing an interface to "conic programming". That is, whenever you have a convex cone K in `R^n` (e.g. the positive ortant, or `{(x_0,...,x_{n-1}) | x_0^2 >= sum_{j=1}^{n-1} x_j^2}`, known as "icecream cone") you have a partial order `<_K` on `R^n` defined by `x<_K y` iff y-k is in K.

then one can do linear optimisation on the intersection of K with an affine subspace. For K being the positive ortant this is just the usual LP; for K the icecream cone this is a kind of norm minimisation, etc. CVXOPT has implementations for several different cones like this (by the way, semidefinite programming is yet another example, K being the cone of psd matrices in `R^nxn`).



---

archive/issue_comments_216644.json:
```json
{
    "body": "Replying to [comment:11 ncohen]:\n> Well, I need to compute a max matching on a large bipartite graph for some designs I build these days, and ...\n> \n\n\nBTW, using LP for this is a very inefficient thing. The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...",
    "created_at": "2014-08-07T09:45:12Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216644",
    "user": "dimpase"
}
```

Replying to [comment:11 ncohen]:
> Well, I need to compute a max matching on a large bipartite graph for some designs I build these days, and ...
> 


BTW, using LP for this is a very inefficient thing. The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...



---

archive/issue_comments_216645.json:
```json
{
    "body": "> BTW, using LP for this is a very inefficient thing. \n\nHow do you think you can prove this assertion ? Especially when the polytope of perfect matchings in a bipartite graph is integer ?..\n\n> The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...\n\nClaim without proof. Also, this isn't implemented, lest of all efficiently implemented.\n\nNathann",
    "created_at": "2014-08-07T09:49:06Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216645",
    "user": "ncohen"
}
```

> BTW, using LP for this is a very inefficient thing. 

How do you think you can prove this assertion ? Especially when the polytope of perfect matchings in a bipartite graph is integer ?..

> The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...

Claim without proof. Also, this isn't implemented, lest of all efficiently implemented.

Nathann



---

archive/issue_comments_216646.json:
```json
{
    "body": "Replying to [comment:14 ncohen]:\n> > BTW, using LP for this is a very inefficient thing. \n> \n> How do you think you can prove this assertion ? Especially when the polytope of perfect matchings in a bipartite graph is integer ?..\n> \n> > The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...\n> \n> Claim without proof. Also, this isn't implemented, lest of all efficiently implemented.\n\nCome on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)\n\nConvert your graph into networkx one and call it...\nAlthough indeed this might not be the fastest implementation around, sure.",
    "created_at": "2014-08-07T10:12:45Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216646",
    "user": "dimpase"
}
```

Replying to [comment:14 ncohen]:
> > BTW, using LP for this is a very inefficient thing. 
> 
> How do you think you can prove this assertion ? Especially when the polytope of perfect matchings in a bipartite graph is integer ?..
> 
> > The classical combinatorial algorithms (see e.g. [Hungarian method](http://equatorialmaths.wordpress.com/2009/09/23/hungarian-algorithm-take-1/)) will be much faster...
> 
> Claim without proof. Also, this isn't implemented, lest of all efficiently implemented.

Come on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)

Convert your graph into networkx one and call it...
Although indeed this might not be the fastest implementation around, sure.



---

archive/issue_comments_216647.json:
```json
{
    "body": "> Come on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)\n\nNo, that is Edmond's algorith for general graphs (it is not bipartite-specific) and it is also the default algorithm called by `Graph.matching` in case. Plus it's pure Python.\n\nNathann",
    "created_at": "2014-08-07T10:17:21Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216647",
    "user": "ncohen"
}
```

> Come on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)

No, that is Edmond's algorith for general graphs (it is not bipartite-specific) and it is also the default algorithm called by `Graph.matching` in case. Plus it's pure Python.

Nathann



---

archive/issue_comments_216648.json:
```json
{
    "body": "Replying to [comment:16 ncohen]:\n> > Come on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)\n> \n> No, that is Edmond's algorith for general graphs (it is not bipartite-specific) and it is also the default algorithm called by `Graph.matching` in case. Plus it's pure Python.\n>\nEdmonds' on bipartite graphs will do more or less the same amount of work as the Hungarian on bipartite graphs. Trust me, I taught these things few times, I used to know details, and even implemented Hungarian method in C and in Python :-)\n\nAlthough Hungarian isn't the fastest guy in town. \n\nPS. It is common (to the combinatorial optimisation community) knowledge that LP-based methods hopelessly lose here. I don't make baseless claims here, I know well what I am talking about...",
    "created_at": "2014-08-07T10:28:02Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216648",
    "user": "dimpase"
}
```

Replying to [comment:16 ncohen]:
> > Come on, it is implemented e.g. here: [networkx](http://networkx.github.io/documentation/latest/reference/generated/networkx.algorithms.matching.maximal_matching.html#networkx.algorithms.matching.maximal_matching)
> 
> No, that is Edmond's algorith for general graphs (it is not bipartite-specific) and it is also the default algorithm called by `Graph.matching` in case. Plus it's pure Python.
>
Edmonds' on bipartite graphs will do more or less the same amount of work as the Hungarian on bipartite graphs. Trust me, I taught these things few times, I used to know details, and even implemented Hungarian method in C and in Python :-)

Although Hungarian isn't the fastest guy in town. 

PS. It is common (to the combinatorial optimisation community) knowledge that LP-based methods hopelessly lose here. I don't make baseless claims here, I know well what I am talking about...



---

archive/issue_comments_216649.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-09T00:23:54Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216649",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216650.json:
```json
{
    "body": "I've added a Parent for MIPVariables, this allows for matrix multiplication to work from both sides:\n\n```\n            sage: p = MixedIntegerLinearProgram()\n            sage: v = p.new_variable()\n            sage: m = matrix([[1,2], [3,4]])\n            sage: v * m\n            (3.0, 4.0)*x_1 + (1.0, 2.0)*x_0\n            sage: m * v\n            (2.0, 4.0)*x_1 + (1.0, 3.0)*x_0\n```\n\nThe parent must refer back to the mip so there is a reference cycle (#12616). Deal with it...",
    "created_at": "2014-08-09T00:28:03Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216650",
    "user": "vbraun"
}
```

I've added a Parent for MIPVariables, this allows for matrix multiplication to work from both sides:

```
            sage: p = MixedIntegerLinearProgram()
            sage: v = p.new_variable()
            sage: m = matrix([[1,2], [3,4]])
            sage: v * m
            (3.0, 4.0)*x_1 + (1.0, 2.0)*x_0
            sage: m * v
            (2.0, 4.0)*x_1 + (1.0, 3.0)*x_0
```

The parent must refer back to the mip so there is a reference cycle (#12616). Deal with it...



---

archive/issue_comments_216651.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-09T13:44:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216651",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216652.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-10T14:05:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216652",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216653.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-11T21:41:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216653",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216654.json:
```json
{
    "body": "Inequalities with vector/matrix-valued linear functions:\n\n```\nsage: mip.<x> = MixedIntegerLinearProgram()\nsage: x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]]) + x[1] * identity_matrix(3) >= 0\n[0 0 0]    [x_1 0         x_0]\n[0 0 0] <= [0   x_0 + x_1 0  ]\n[0 0 0]    [x_0 0         x_1]\n```\n",
    "created_at": "2014-08-11T21:57:14Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216654",
    "user": "vbraun"
}
```

Inequalities with vector/matrix-valued linear functions:

```
sage: mip.<x> = MixedIntegerLinearProgram()
sage: x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]]) + x[1] * identity_matrix(3) >= 0
[0 0 0]    [x_1 0         x_0]
[0 0 0] <= [0   x_0 + x_1 0  ]
[0 0 0]    [x_0 0         x_1]
```




---

archive/issue_comments_216655.json:
```json
{
    "body": "Whats the point of the `add_linear_constraints` method in backends? Add multiple trivial constraints without a way to hand it any coefficient data? Really? ;-)",
    "created_at": "2014-08-12T16:14:04Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216655",
    "user": "vbraun"
}
```

Whats the point of the `add_linear_constraints` method in backends? Add multiple trivial constraints without a way to hand it any coefficient data? Really? ;-)



---

archive/issue_comments_216656.json:
```json
{
    "body": "> Whats the point of the `add_linear_constraints` method in backends? Add multiple trivial constraints without a way to hand it any coefficient data? Really? ;-)\n\nThey also expose the solver's corresponding functions, which do the very same. And that's probably where we will get a speedup, for it seems that calling add_constraint n times instead of calling add_constraints(n) does make a difference.\n\nNathann",
    "created_at": "2014-08-12T16:18:10Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216656",
    "user": "ncohen"
}
```

> Whats the point of the `add_linear_constraints` method in backends? Add multiple trivial constraints without a way to hand it any coefficient data? Really? ;-)

They also expose the solver's corresponding functions, which do the very same. And that's probably where we will get a speedup, for it seems that calling add_constraint n times instead of calling add_constraints(n) does make a difference.

Nathann



---

archive/issue_comments_216657.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-12T16:18:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216657",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216658.json:
```json
{
    "body": "Replying to [comment:26 ncohen]:\n> They also expose the solver's corresponding functions, which do the very same. And that's probably where we will get a speedup, for it seems that calling add_constraint n times instead of calling add_constraints(n) does make a difference.\n\nObviously it is very fast to solve trivial constraints. Read my question again.",
    "created_at": "2014-08-12T16:22:10Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216658",
    "user": "vbraun"
}
```

Replying to [comment:26 ncohen]:
> They also expose the solver's corresponding functions, which do the very same. And that's probably where we will get a speedup, for it seems that calling add_constraint n times instead of calling add_constraints(n) does make a difference.

Obviously it is very fast to solve trivial constraints. Read my question again.



---

archive/issue_comments_216659.json:
```json
{
    "body": "This:\n\n```\n    cpdef add_linear_constraint(self, constraints, lower_bound, upper_bound, name=*)\n    cpdef add_linear_constraints(self, int number, lower_bound, upper_bound, names=*)\n```\n",
    "created_at": "2014-08-12T16:22:55Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216659",
    "user": "vbraun"
}
```

This:

```
    cpdef add_linear_constraint(self, constraints, lower_bound, upper_bound, name=*)
    cpdef add_linear_constraints(self, int number, lower_bound, upper_bound, names=*)
```




---

archive/issue_comments_216660.json:
```json
{
    "body": "> Obviously it is very fast to solve trivial constraints. Read my question again.\n\nVolker you make mistakes too, and if there is something I don't stand for long it is people giving me orders. If there is a misunderstanding, we will clear it.\n\nWhat I said is that people (some I met, or myself in the past) noticed that it took time to add constraints to a LP. I am not talking of solving the LP itself. Just imagine that the matrix is stored as a dense contiguous matrix, and that adding a row means having to copy everything.\n\nI don't know if this is how it is implemented, but on some occasions adding constraints one by one (each with the same number of nonzero coefficients and looking the same) takes a time which is not constant per call, i.e. calling `add_constraint` on a matrix that is already big is longer than on an empty matrix.\n\nWhich means that calling add_linear_constraints may make a difference. Which would also explain why the solvers contain this in their API, even though it seems useless at first.\n\nI just tried it again by adding the same constraint \"b[3]<=8\" many many times but in this case the time seems to be constant. Perhaps there is something else missing.\n\nNathann",
    "created_at": "2014-08-12T16:40:26Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216660",
    "user": "ncohen"
}
```

> Obviously it is very fast to solve trivial constraints. Read my question again.

Volker you make mistakes too, and if there is something I don't stand for long it is people giving me orders. If there is a misunderstanding, we will clear it.

What I said is that people (some I met, or myself in the past) noticed that it took time to add constraints to a LP. I am not talking of solving the LP itself. Just imagine that the matrix is stored as a dense contiguous matrix, and that adding a row means having to copy everything.

I don't know if this is how it is implemented, but on some occasions adding constraints one by one (each with the same number of nonzero coefficients and looking the same) takes a time which is not constant per call, i.e. calling `add_constraint` on a matrix that is already big is longer than on an empty matrix.

Which means that calling add_linear_constraints may make a difference. Which would also explain why the solvers contain this in their API, even though it seems useless at first.

I just tried it again by adding the same constraint "b[3]<=8" many many times but in this case the time seems to be constant. Perhaps there is something else missing.

Nathann



---

archive/issue_comments_216661.json:
```json
{
    "body": "I'm not trying to give you orders, I just want to know whether `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?",
    "created_at": "2014-08-12T17:24:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216661",
    "user": "vbraun"
}
```

I'm not trying to give you orders, I just want to know whether `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?



---

archive/issue_comments_216662.json:
```json
{
    "body": "> I'm not trying to give you orders, I just want to know whether `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?\n\nI guess that it is meant to allocate space for new rows, before changing the coefficient. As I said above I implemented this function only to expose those that are available in the solvers, and they do the very same thing (if not less).\n\nGLPK: takes only a number of rows as an argument. Nothing else\n\n```\nglp_add_rows(self.lp, number)\n```\n\n\nCplex: takes a number of rows and the corresponding upper/lower bound. Plus names\n\n\n```\nCPXnewrows(self.env, self.lp, number, bound, sense, rng, c_names if names else NULL) #\n```\n\n\nLooks like the others don't have this function implemented.\n\nNathann",
    "created_at": "2014-08-12T20:30:22Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216662",
    "user": "ncohen"
}
```

> I'm not trying to give you orders, I just want to know whether `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?

I guess that it is meant to allocate space for new rows, before changing the coefficient. As I said above I implemented this function only to expose those that are available in the solvers, and they do the very same thing (if not less).

GLPK: takes only a number of rows as an argument. Nothing else

```
glp_add_rows(self.lp, number)
```


Cplex: takes a number of rows and the corresponding upper/lower bound. Plus names


```
CPXnewrows(self.env, self.lp, number, bound, sense, rng, c_names if names else NULL) #
```


Looks like the others don't have this function implemented.

Nathann



---

archive/issue_comments_216663.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-12T20:58:01Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216663",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216664.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-12T21:07:07Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216664",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216665.json:
```json
{
    "body": "I'm proposing a new interface for vector-valued linear functions. This should be the most natural way to hand over multiple constraints in one call. By default, this just falls back to adding scalar constraints individually. In another ticket we can add special backend support for that.",
    "created_at": "2014-08-12T21:53:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216665",
    "user": "vbraun"
}
```

I'm proposing a new interface for vector-valued linear functions. This should be the most natural way to hand over multiple constraints in one call. By default, this just falls back to adding scalar constraints individually. In another ticket we can add special backend support for that.



---

archive/issue_comments_216666.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-12T21:57:19Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216666",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216667.json:
```json
{
    "body": "This is all that should go into this ticket, please review.",
    "created_at": "2014-08-12T22:21:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216667",
    "user": "vbraun"
}
```

This is all that should go into this ticket, please review.



---

archive/issue_comments_216668.json:
```json
{
    "body": "Changing status from new to needs_review.",
    "created_at": "2014-08-12T22:21:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216668",
    "user": "vbraun"
}
```

Changing status from new to needs_review.



---

archive/issue_comments_216669.json:
```json
{
    "body": "typo:\n\nimplinicly -> implicitly (?) in `linear_tensor.py`\n\nand there is also at least one instanciate* instead of instantiate* in the diff...",
    "created_at": "2014-08-13T08:52:35Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216669",
    "user": "dimpase"
}
```

typo:

implinicly -> implicitly (?) in `linear_tensor.py`

and there is also at least one instanciate* instead of instantiate* in the diff...



---

archive/issue_comments_216670.json:
```json
{
    "body": "Replying to [comment:32 ncohen]:\n> `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?\n> \n> I guess that it is meant to allocate space for new rows, before changing the coefficient. As I said above I implemented this function only to expose those that are available in the solvers, and they do the very same thing (if not `less).\n\nGUROBI does have [`GRBaddconstrs`](http://www.gurobi.com/documentation/5.6/reference-manual/c_grbaddconstrs) which a full thing, in the sense you can specify a bunch of constraints, specified by a sparse matrix, at once.\n\n> \n> GLPK: takes only a number of rows as an argument. Nothing else\n> {{{\n> glp_add_rows(self.lp, number)\n> }}}\n> \n> Cplex: takes a number of rows and the corresponding upper/lower bound. Plus names\n> \n> {{{\n> CPXnewrows(self.env, self.lp, number, bound, sense, rng, c_names if names else NULL) #\n> }}}\n\nthere is [`CPXaddrows`](http://web.njit.edu/all_topics/Prog_Lang_Docs/cplex80/doc/refman/html/addrows.html) in CPLEX, similar to the one in GUROBI.",
    "created_at": "2014-08-13T09:15:06Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216670",
    "user": "dimpase"
}
```

Replying to [comment:32 ncohen]:
> `add_linear_constraints` is supposed to work, and if not how you'd envision it to work. Create empty rows and then mutate later? Or is the constraint data just missing from the argspec? or what?
> 
> I guess that it is meant to allocate space for new rows, before changing the coefficient. As I said above I implemented this function only to expose those that are available in the solvers, and they do the very same thing (if not `less).

GUROBI does have [`GRBaddconstrs`](http://www.gurobi.com/documentation/5.6/reference-manual/c_grbaddconstrs) which a full thing, in the sense you can specify a bunch of constraints, specified by a sparse matrix, at once.

> 
> GLPK: takes only a number of rows as an argument. Nothing else
> {{{
> glp_add_rows(self.lp, number)
> }}}
> 
> Cplex: takes a number of rows and the corresponding upper/lower bound. Plus names
> 
> {{{
> CPXnewrows(self.env, self.lp, number, bound, sense, rng, c_names if names else NULL) #
> }}}

there is [`CPXaddrows`](http://web.njit.edu/all_topics/Prog_Lang_Docs/cplex80/doc/refman/html/addrows.html) in CPLEX, similar to the one in GUROBI.



---

archive/issue_comments_216671.json:
```json
{
    "body": "Replying to [comment:24 vbraun]:\n> Inequalities with vector/matrix-valued linear functions:\n> {{{\n> sage: mip.<x> = MixedIntegerLinearProgram()\n> sage: x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]]) + x[1] * identity_matrix(3) >= 0\n> [0 0 0]    [x_1 0         x_0]\n> [0 0 0] <= [0   x_0 + x_1 0  ]\n> [0 0 0]    [x_0 0         x_1]\n> }}}\n\nis it expected that `mip.show()` shows some kind of nonsense after such inequalities are entered?",
    "created_at": "2014-08-13T12:09:02Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216671",
    "user": "dimpase"
}
```

Replying to [comment:24 vbraun]:
> Inequalities with vector/matrix-valued linear functions:
> {{{
> sage: mip.<x> = MixedIntegerLinearProgram()
> sage: x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]]) + x[1] * identity_matrix(3) >= 0
> [0 0 0]    [x_1 0         x_0]
> [0 0 0] <= [0   x_0 + x_1 0  ]
> [0 0 0]    [x_0 0         x_1]
> }}}

is it expected that `mip.show()` shows some kind of nonsense after such inequalities are entered?



---

archive/issue_comments_216672.json:
```json
{
    "body": "Right now you can only add scalar and vector-valued linear constraints. I intended the matrix notation for semidefinite programming. But you can't do anything with them yet.",
    "created_at": "2014-08-13T12:42:53Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216672",
    "user": "vbraun"
}
```

Right now you can only add scalar and vector-valued linear constraints. I intended the matrix notation for semidefinite programming. But you can't do anything with them yet.



---

archive/issue_comments_216673.json:
```json
{
    "body": "Replying to [comment:41 vbraun]:\n> Right now you can only add scalar and vector-valued linear constraints. I intended the matrix notation for semidefinite programming. \n\nGreat! \n\nHow does one recover the original matrices x[i]'s are multiplied with? I see that now each constraint gets .rhs which is a matrix of linear forms. For SDP backends one would typically want the original matrices. (Of course they can still be found...)\n\nWell, this is probably for another ticket.",
    "created_at": "2014-08-13T12:55:45Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216673",
    "user": "dimpase"
}
```

Replying to [comment:41 vbraun]:
> Right now you can only add scalar and vector-valued linear constraints. I intended the matrix notation for semidefinite programming. 

Great! 

How does one recover the original matrices x[i]'s are multiplied with? I see that now each constraint gets .rhs which is a matrix of linear forms. For SDP backends one would typically want the original matrices. (Of course they can still be found...)

Well, this is probably for another ticket.



---

archive/issue_comments_216674.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-13T13:01:45Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216674",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216675.json:
```json
{
    "body": "IMHO if you want to pass a dense matrix you should just implement a separate method for that. Since the point of the interface is to simplify the variable ordering/indexing I'm cutting the matrix into columns and only store the columns. The solver backend should then put them back into a matrix (or whatever storage the backend uses) in the backend's order.",
    "created_at": "2014-08-13T13:06:20Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216675",
    "user": "vbraun"
}
```

IMHO if you want to pass a dense matrix you should just implement a separate method for that. Since the point of the interface is to simplify the variable ordering/indexing I'm cutting the matrix into columns and only store the columns. The solver backend should then put them back into a matrix (or whatever storage the backend uses) in the backend's order.



---

archive/issue_comments_216676.json:
```json
{
    "body": "In the previous comment I was talking about vector-valued linear functions.\n\nFor matrix-valued linear functions you get the coefficient matrices in the usual way:\n\n```\nsage: m = x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]])\nsage: m.dict()[0]\n[0.0 0.0 1.0]\n[0.0 1.0 0.0]\n[1.0 0.0 0.0]\n```\n",
    "created_at": "2014-08-13T13:09:10Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216676",
    "user": "vbraun"
}
```

In the previous comment I was talking about vector-valued linear functions.

For matrix-valued linear functions you get the coefficient matrices in the usual way:

```
sage: m = x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]])
sage: m.dict()[0]
[0.0 0.0 1.0]
[0.0 1.0 0.0]
[1.0 0.0 0.0]
```




---

archive/issue_comments_216677.json:
```json
{
    "body": "Replying to [comment:45 vbraun]:\n> In the previous comment I was talking about vector-valued linear functions.\n> \n> For matrix-valued linear functions you get the coefficient matrices in the usual way:\n> {{{\n> sage: m = x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]])\n> sage: m.dict()[0]\n> [0.0 0.0 1.0]\n> [0.0 1.0 0.0]\n> [1.0 0.0 0.0]\n> }}}\n\ncan this be made explicit in a docstring? Otherwise looks good to me.",
    "created_at": "2014-08-13T16:38:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216677",
    "user": "dimpase"
}
```

Replying to [comment:45 vbraun]:
> In the previous comment I was talking about vector-valued linear functions.
> 
> For matrix-valued linear functions you get the coefficient matrices in the usual way:
> {{{
> sage: m = x[0] * matrix([[0,0,1],[0,1,0],[1,0,0]])
> sage: m.dict()[0]
> [0.0 0.0 1.0]
> [0.0 1.0 0.0]
> [1.0 0.0 0.0]
> }}}

can this be made explicit in a docstring? Otherwise looks good to me.



---

archive/issue_comments_216678.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-13T22:19:53Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216678",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216679.json:
```json
{
    "body": "Replying to [comment:46 dimpase]:\n> can this be made explicit in a docstring?\n\ndone",
    "created_at": "2014-08-13T22:20:29Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216679",
    "user": "vbraun"
}
```

Replying to [comment:46 dimpase]:
> can this be made explicit in a docstring?

done



---

archive/issue_comments_216680.json:
```json
{
    "body": "is the syntax\n\n```\nblah.<foo>=MixedIntegerLinearProgram()\n```\n\nnew? I can't recall seeing this before. It should be documented in mip.pyx, as an alternative to\n\n```\nblah=MixedIntegerLinearProgram()\nfoo=blah.new_variable()\n```\n",
    "created_at": "2014-08-14T05:32:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216680",
    "user": "dimpase"
}
```

is the syntax

```
blah.<foo>=MixedIntegerLinearProgram()
```

new? I can't recall seeing this before. It should be documented in mip.pyx, as an alternative to

```
blah=MixedIntegerLinearProgram()
foo=blah.new_variable()
```




---

archive/issue_comments_216681.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2014-08-14T10:40:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216681",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_216682.json:
```json
{
    "body": "IMHO accepting generator syntax not really something that needs to be spelled out, but is something that should be expected to work. I've used it in most new doctests so its not like it is hidden away. In any case, added another note to `new_variable`.",
    "created_at": "2014-08-14T10:42:47Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216682",
    "user": "vbraun"
}
```

IMHO accepting generator syntax not really something that needs to be spelled out, but is something that should be expected to work. I've used it in most new doctests so its not like it is hidden away. In any case, added another note to `new_variable`.



---

archive/issue_comments_216683.json:
```json
{
    "body": "OK!",
    "created_at": "2014-08-14T20:07:41Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216683",
    "user": "dimpase"
}
```

OK!



---

archive/issue_comments_216684.json:
```json
{
    "body": "Changing status from needs_review to positive_review.",
    "created_at": "2014-08-14T20:07:41Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216684",
    "user": "dimpase"
}
```

Changing status from needs_review to positive_review.



---

archive/issue_comments_216685.json:
```json
{
    "body": "Resolution: fixed",
    "created_at": "2014-08-16T07:52:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216685",
    "user": "vbraun"
}
```

Resolution: fixed



---

archive/issue_comments_216686.json:
```json
{
    "body": "Hello !\n\nI was just trying to see how this new code works (Thanks Volker !!!) and see what we can earn by implementing solver-specific \"add_constraints\". And while playing, I found that :\n\n\n```\nsage: p.<x>  = MixedIntegerLinearProgram()\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-14-300a6e09d28b> in <module>()\n----> 1 p = MixedIntegerLinearProgram(names=('x',)); (x,) = p._first_ngens(1)\n\n/home/ncohen/.Sage/local/lib/python2.7/site-packages/sage/numerical/mip.so in sage.numerical.mip.MixedIntegerLinearProgram.__init__ (build/cythonized/sage/numerical/mip.c:1740)()\n\nTypeError: __init__() got an unexpected keyword argument 'names'\n```\n\n\nNathann",
    "created_at": "2014-08-17T09:00:15Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216686",
    "user": "ncohen"
}
```

Hello !

I was just trying to see how this new code works (Thanks Volker !!!) and see what we can earn by implementing solver-specific "add_constraints". And while playing, I found that :


```
sage: p.<x>  = MixedIntegerLinearProgram()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-14-300a6e09d28b> in <module>()
----> 1 p = MixedIntegerLinearProgram(names=('x',)); (x,) = p._first_ngens(1)

/home/ncohen/.Sage/local/lib/python2.7/site-packages/sage/numerical/mip.so in sage.numerical.mip.MixedIntegerLinearProgram.__init__ (build/cythonized/sage/numerical/mip.c:1740)()

TypeError: __init__() got an unexpected keyword argument 'names'
```


Nathann



---

archive/issue_comments_216687.json:
```json
{
    "body": "Replying to [comment:55 ncohen]:\n> Hello !\n> \n> I was just trying to see how this new code works (Thanks Volker !!!) and see what we can earn by implementing solver-specific \"add_constraints\". And while playing, I found that :\n> \n> {{{\n> sage: p.<x>  = MixedIntegerLinearProgram()\n> ---------------------------------------------------------------------------\n> TypeError                                 Traceback (most recent call last)\n> <ipython-input-14-300a6e09d28b> in <module>()\n> ----> 1 p = MixedIntegerLinearProgram(names=('x',)); (x,) = p._first_ngens(1)\n> \n> /home/ncohen/.Sage/local/lib/python2.7/site-packages/sage/numerical/mip.so in sage.numerical.mip.MixedIntegerLinearProgram.__init__ (build/cythonized/sage/numerical/mip.c:1740)()\n> \n> TypeError: __init__() got an unexpected keyword argument 'names'\n> }}}\n\nhmm, it seems you must have something weird in the codebase in your local Sage install...",
    "created_at": "2014-08-17T10:51:55Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216687",
    "user": "dimpase"
}
```

Replying to [comment:55 ncohen]:
> Hello !
> 
> I was just trying to see how this new code works (Thanks Volker !!!) and see what we can earn by implementing solver-specific "add_constraints". And while playing, I found that :
> 
> {{{
> sage: p.<x>  = MixedIntegerLinearProgram()
> ---------------------------------------------------------------------------
> TypeError                                 Traceback (most recent call last)
> <ipython-input-14-300a6e09d28b> in <module>()
> ----> 1 p = MixedIntegerLinearProgram(names=('x',)); (x,) = p._first_ngens(1)
> 
> /home/ncohen/.Sage/local/lib/python2.7/site-packages/sage/numerical/mip.so in sage.numerical.mip.MixedIntegerLinearProgram.__init__ (build/cythonized/sage/numerical/mip.c:1740)()
> 
> TypeError: __init__() got an unexpected keyword argument 'names'
> }}}

hmm, it seems you must have something weird in the codebase in your local Sage install...



---

archive/issue_comments_216688.json:
```json
{
    "body": "Oh right. a `touch numerical/*` and `sage -b` solved it ! Thanks !\n\nNathann",
    "created_at": "2014-08-17T15:04:24Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216688",
    "user": "ncohen"
}
```

Oh right. a `touch numerical/*` and `sage -b` solved it ! Thanks !

Nathann



---

archive/issue_comments_216689.json:
```json
{
    "body": "Here is the work of my MSc student on an interface for SDP solver, that builds upon this ticket: https://github.com/ingolfured/sageproject/\n\nit has a backend to the SDP solver in CVXOPT.",
    "created_at": "2014-08-17T20:37:18Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216689",
    "user": "dimpase"
}
```

Here is the work of my MSc student on an interface for SDP solver, that builds upon this ticket: https://github.com/ingolfured/sageproject/

it has a backend to the SDP solver in CVXOPT.



---

archive/issue_comments_216690.json:
```json
{
    "body": "Yo Dima ! If you happen to write a LP-related ticket, could you do that ?\n\n\n```diff\n+++ b/src/sage/numerical/mip.pyx\n@@ -1183,7 +1183,7 @@ cdef class MixedIntegerLinearProgram(SageObject):\n             if b.is_variable_integer(i):\n                 var_type = 'an integer'\n             elif b.is_variable_binary(i):\n-                var_type = 'a boolean variable'\n+                var_type = 'a boolean'\n             else:\n                 var_type = 'a continuous'\n             if varid_name[i] == str(self.gen(i)):\n```\n\n\nOtherwise we get stuff like that :\n\n\n```\nsage: p.show()\nMaximization:\n \nConstraints:\n  x_0 <= 1.0\nVariables:\n  x_0 is a boolean variable variable (min=0.0, max=1.0)\n```\n\n\nAnd it does not seem worth a ticket of its own...\n\nNathann",
    "created_at": "2014-08-18T09:40:14Z",
    "issue": "https://github.com/sagemath/sagetest/issues/16477",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/16477#issuecomment-216690",
    "user": "ncohen"
}
```

Yo Dima ! If you happen to write a LP-related ticket, could you do that ?


```diff
+++ b/src/sage/numerical/mip.pyx
@@ -1183,7 +1183,7 @@ cdef class MixedIntegerLinearProgram(SageObject):
             if b.is_variable_integer(i):
                 var_type = 'an integer'
             elif b.is_variable_binary(i):
-                var_type = 'a boolean variable'
+                var_type = 'a boolean'
             else:
                 var_type = 'a continuous'
             if varid_name[i] == str(self.gen(i)):
```


Otherwise we get stuff like that :


```
sage: p.show()
Maximization:
 
Constraints:
  x_0 <= 1.0
Variables:
  x_0 is a boolean variable variable (min=0.0, max=1.0)
```


And it does not seem worth a ticket of its own...

Nathann
