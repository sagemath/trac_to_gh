# Issue 15062: Incorrect results for analytic Sha due to low precision

Issue created by migration from https://trac.sagemath.org/ticket/15299

Original creator: jdemeyer

Original creation time: 2013-10-16 21:05:51

CC:  cremona was

See [https://groups.google.com/forum/#!topic/sage-support/rYQ4rWyncG4](https://groups.google.com/forum/#!topic/sage-support/rYQ4rWyncG4)


---

Comment by was created at 2013-10-16 21:11:27

It gets worse!


```
sage: E = EllipticCurve('11a')
sage: E.lseries().deriv_at1()
0
sage: E.lseries().dokchitser().derivative(1)
0.308708533963172
```


I.e., it is wrong for all curves of rank 0 too...  (This isn't what we wrote the code for.  Ugh.)


---

Comment by jdemeyer created at 2013-10-16 22:20:17

So this is plain wrong then (I don't know enough mathematics to judge this):

```
        if self.__E.root_number() == 1:
            return 0
```



---

Comment by jdemeyer created at 2013-10-16 22:32:54

I don't know about `11a1`, but this at least fixes the original problem.


---

Comment by cremona created at 2013-10-17 08:52:52

Replying to [comment:3 jdemeyer]:
> So this is plain wrong then (I don't know enough mathematics to judge this):
> {{{
>         if self.__E.root_number() == 1:
>             return 0
> }}}
The root number is the sign of the functional equation so is +1 for even analytic rank and -1 for odd.  This function computes the first derivative.  *In practice* this is something one would only want to do if the 0'th derivative was already known to be 0, in which case the code you quote would be OK since if the value is 0 and the order is even then the order is at least 2 so the first derivative is exactly 0.  But of course this function then lies in wait for the user who decides they want the first derivative's value even when the value is nonzero (as for 11a1).  The trouble is that (1) Formulas for the r'th derivative which are implemented are *only* valid under the assumption that all previous derivatives are 0;  and of course (2) proving the earlier derivatives are exactly 0 is usually impossible with current theory.

Where does that leave this deriv_at1 function?  At the very least it should come with a huge warning about all this.  And it really should return 0 when the root number is +1 unless the user has made an explicit assumption (assume_order_of_vanishing_is_positive=True, say) and otherwise raise a NotImplemented error (or attempt to prove that L(1)=0).


---

Comment by jdemeyer created at 2013-10-17 19:16:34

Changing status from new to needs_review.


---

Comment by cremona created at 2013-10-18 14:06:37

...review in progress...and all works fine, including docbuilding.  the changes look good to the human eye (this one at least).  Oops, forgot the --long when testing..... and it's still all good.  Thanks, Jeroen!


---

Comment by jdemeyer created at 2013-10-20 21:48:02

Corrected the error computation for `at1()`. I believe this is rigorous now:

```
        for n in xrange(1,k+1):
            term = (zpow * an[n])/n
            zpow *= z
            L += term
            # 8n+1 is the relative error in half-ulps to compute term.
            # For addition, multiplication, division, sqrt, this is
            # bounded by the number of operations. exp(x) multiplies the
            # relative error by abs(x) and adds 1 half-ulp. The relative
            # error for -2*pi/sqrtN is 3 half-ulps. Assuming that
            # 2*pi/sqrtN <= 2, the relative error for z is 7 half-ulps.
            # This implies a relative error of 8n-1 half-ulps for zpow.
            # Adding 2 for the computation of term gives:
            error += term.ulp()*(8*n+1) + L.ulp()
```



---

Comment by cremona created at 2013-10-21 08:43:22

Replying to [comment:10 jdemeyer]:
> Corrected the error computation for `at1()`. I believe this is rigorous now:
> {{{
>         for n in xrange(1,k+1):
>             term = (zpow * an[n])/n
>             zpow *= z
>             L += term
>             # 8n+1 is the relative error in half-ulps to compute term.
>             # For addition, multiplication, division, sqrt, this is
>             # bounded by the number of operations. exp(x) multiplies the
>             # relative error by abs(x) and adds 1 half-ulp. The relative
>             # error for -2*pi/sqrtN is 3 half-ulps. Assuming that
>             # 2*pi/sqrtN <= 2, the relative error for z is 7 half-ulps.
>             # This implies a relative error of 8n-1 half-ulps for zpow.
>             # Adding 2 for the computation of term gives:
>             error += term.ulp()*(8*n+1) + L.ulp()
> }}}

I can see where this is in the code -- can you say how it affects any doctest outputs?  I am not a numerical analyst...


---

Comment by jdemeyer created at 2013-10-21 09:52:53

Replying to [comment:11 cremona]:
> can you say how it affects any doctest outputs?
I would only make the `error` results slightly larger than before. Perhaps more importantly, I believe the error computation is now rigorous, in the sense that one could _prove_ that the actual error is bound by the `error` result.


---

Comment by jdemeyer created at 2013-10-28 12:06:08

If you care less about speed, I could also write a version using interval arithmetic, which will be simpler but slower.


---

Comment by jdemeyer created at 2013-10-31 08:14:57

Added tolerance to `E.lseries().twist_values(1, -12, -4)` doctest to account for doctest failure on ia64.


---

Comment by jdemeyer created at 2013-11-04 07:24:31

Passes tests on the buildbots now.


---

Comment by pbruin created at 2013-11-04 17:30:32

Replying to [comment:5 cremona]:
> Where does that leave this deriv_at1 function?  At the very least it should come with a huge warning about all this.  And it really should return 0 when the root number is +1 unless the user has made an explicit assumption (assume_order_of_vanishing_is_positive=True, say) and otherwise raise a NotImplemented error (or attempt to prove that L(1)=0).
The `Lseries_ell` class has a method `L1_vanishes()`, written by William Stein.  According to the documentation, it is provably correct if the Manin constant is <= 2 for the optimal quotient in the isogeny class.  This method is used in some places, but not currently in `deriv_at1()`, where we might use it to check the assumption _L_(_E_, 1) = 0.  If that is too slow, a `check=False` flag could be added.


---

Comment by pbruin created at 2013-11-04 17:32:58

Jeroen, do you have a particular reason for writing `QQ()` and `R()` instead of `QQ(0)` and `R(0)`?  It saves one character, but is less readable.


---

Comment by jdemeyer created at 2013-11-04 19:34:23

Replying to [comment:17 pbruin]:
> Jeroen, do you have a particular reason for writing `QQ()` and `R()` instead of `QQ(0)` and `R(0)`?  It saves one character, but is less readable.
No reason, it's just a habit to think in term of default constructors (I guess this is my C++ background). I just benchmarked `QQ()`, `QQ(0)` and `QQ.zero()` and the latter is the fastest, so perhaps we should use that.


---

Comment by jdemeyer created at 2013-11-04 20:23:16

Replying to [comment:16 pbruin]:
> The `Lseries_ell` class has a method `L1_vanishes()`, written by William Stein.  According to the documentation, it is provably correct if the Manin constant is <= 2 for the optimal quotient in the isogeny class.  This method is used in some places, but not currently in `deriv_at1()`, where we might use it to check the assumption _L_(_E_, 1) = 0.  If that is too slow, a `check=False` flag could be added.

Do you propose that this change should be made, or is it just an observation? Given that the function `deriv_at1()` is in practice only called when we know that `L(E,1) = 0`, I personally think the warning suffices.


---

Comment by pbruin created at 2013-11-04 21:12:54

Replying to [comment:19 jdemeyer]:
> Do you propose that this change should be made, or is it just an observation? Given that the function `deriv_at1()` is in practice only called when we know that `L(E,1) = 0`, I personally think the warning suffices.
I agree, it was more an observation that we could in principle use `L1_vanishes()` here than a proposal to actually do it.

There is a formula for the _r_-th derivative which is valid when all lower derivatives vanish.  As far as I know, only for the 0-th derivative is there a known way to prove that it vanishes by a numerical computation.  For parity reasons (the root number), this means that if the order of vanishing is 0, 1 or 2, then we can prove this.  If the order of vanishing is 3, then in general we don't know how to prove that it is not 1.

This means that if and when the formula mentioned above is implemented, we won't be able to verify the condition "all lower derivatives are 0" when _r_ is at least 3.  Hence we should probably not insist on verifying it when _r_ = 1.


---

Comment by pbruin created at 2013-11-04 22:34:01

Another question: is it necessary to compute the error bound to the same precision as the result, i.e. in `RealField(prec)`?  It seems sufficient, and more efficient, to compute it in a lower-precision `RealField` or even just using Python floats.


---

Comment by cremona created at 2013-11-05 09:31:38

Replying to [comment:20 pbruin]:
> Replying to [comment:19 jdemeyer]:
> > Do you propose that this change should be made, or is it just an observation? Given that the function `deriv_at1()` is in practice only called when we know that `L(E,1) = 0`, I personally think the warning suffices.
> I agree, it was more an observation that we could in principle use `L1_vanishes()` here than a proposal to actually do it.
> 
> There is a formula for the _r_-th derivative which is valid when all lower derivatives vanish.  As far as I know, only for the 0-th derivative is there a known way to prove that it vanishes by a numerical computation.  For parity reasons (the root number), this means that if the order of vanishing is 0, 1 or 2, then we can prove this.  If the order of vanishing is 3, then in general we don't know how to prove that it is not 1.

You can go one step further thanks to Gross-Zagier:  if the parity is odd and L'(1) looks zero then you can prove it, since if in fact L'(1)!=0 then the curve would have rank 1, but you can disprove that by finding three (or oeven only 2) independent points. See by talk http://homepages.warwick.ac.uk/staff/J.E.Cremona/papers/bsd50.pdf if you want to read more!

> 
> This means that if and when the formula mentioned above is implemented, we won't be able to verify the condition "all lower derivatives are 0" when _r_ is at least 3.  Hence we should probably not insist on verifying it when _r_ = 1.


---

Comment by pbruin created at 2013-11-06 12:18:39

Replying to [comment:22 cremona]:
> You can go one step further thanks to Gross-Zagier:  if the parity is odd and L'(1) looks zero then you can prove it, since if in fact L'(1)!=0 then the curve would have rank 1, but you can disprove that by finding three (or oeven only 2) independent points.
That is true (in fact I seem to remember learning this from the talk you linked to).  However, it does require you to search for points; there seems to be no "analytic" way of proving that L'(1) = 0 by computing it to finite precision, like the `L1_vanishes()` function.


---

Comment by jdemeyer created at 2013-11-08 16:06:04

Replying to [comment:21 pbruin]:
> Another question: is it necessary to compute the error bound to the same precision as the result, i.e. in `RealField(prec)`?  It seems sufficient, and more efficient, to compute it in a lower-precision `RealField`
Done. Needs #15337.

> or even just using Python floats.
Not a good idea, as these have limited range and it's a lot harder (maybe even impossible) to control the rounding. One doctest has an error of 2.74997188336901e-449, which would be rounded to 0.0 as Python `float`.


---

Comment by pbruin created at 2013-11-08 23:46:41

Changing status from needs_review to positive_review.


---

Comment by pbruin created at 2013-11-08 23:46:41

This looks very good now.  The error analysis appears to be completely rigorous for `at1()` and almost completely rigorous for `deriv_at1()`, the only source of non-rigorousness being due to the unknown error in the exponential integral function `eint1()` from PARI.  This ticket is not the place to try to fix this, though.

Are the PARI developers aware of this precision issue?  Should it be regarded it as a bug, or does PARI not strive for proven error bounds for functions such as `eint1()`?


---

Comment by jdemeyer created at 2013-11-09 14:26:56

Replying to [comment:25 pbruin]:
> Are the PARI developers aware of this precision issue?
No idea. I might report it.

> does PARI not strive for proven error bounds for functions such as `eint1()`?
I don't think PARI does. However, the errors are quite large (over 30 bits can be wrong), so perhaps that's a bug indeed.


---

Comment by jdemeyer created at 2013-11-09 23:09:47

Reported the precision issue, they fixed it. There is supposed to be an absolute error bound (not relative), but I don't know what the bound is...


---

Comment by jdemeyer created at 2013-11-12 23:47:36

Moved part of the patch to #15402.


---

Attachment

Changed error bounds because of #15402, needs review.


---

Comment by jdemeyer created at 2013-11-13 16:44:43

Changing status from positive_review to needs_review.


---

Comment by pbruin created at 2013-11-19 18:20:04

Changing status from needs_review to positive_review.


---

Comment by pbruin created at 2013-11-19 18:20:04

Looks even better than before, the precision is now much better under control thanks to #15402, and the remaining "arbitrary" precision increase is clearly motivated.

As in #15402, just a trivial review patch to refer to a section instead of a page number in Cohen's book.


---

Attachment

replace page number by section number in Cohen reference


---

Comment by jdemeyer created at 2013-11-24 17:26:13

Resolution: fixed
