# Issue 11568: reading integers from a file takes quadratic time

Issue created by migration from https://trac.sagemath.org/ticket/11740

Original creator: zimmerma

Original creation time: 2011-08-24 17:32:58

Assignee: AlexGhitza

CC:  leif eviatarbach

[This problem was mentioned to me by Thorsten Kleinjung.]

Consider a file `b.sage` containing a single line `sigma = 111...111` with 200000 ones, and another file `c.sage` with
400000 ones. Then with Sage 4.7:

```
sage: t=cputime()
sage: load b.sage
sage: cputime(t)
1.7999999999999998
sage: t=cputime()
sage: load c.sage
sage: cputime(t)
6.7800000000000011
```


Magma V2.17-7 reads the 400000-digit integer in about 80ms, i.e.,
about 85 times faster than Sage. I consider this as a defect.


---

Comment by AlexGhitza created at 2011-08-24 21:33:01

Note that it seems to really just be something about *reading from a file*, and not about *parsing a long integer*:


```
sage: for j in range(10):
....:     s = '1'*(100000*2^j)
....:     t = cputime()
....:     si = ZZ(s)
....:     cputime(t)
....:     
0.056663000000000352
0.13999099999999842
0.33331100000000191
0.78328300000000084
1.8498789999999996
4.4430429999999994
10.539313
23.651790999999999
52.459912999999986
114.52253299999998
```



---

Comment by nbruin created at 2011-09-03 21:00:42

I think it's python's string-to-int routine, which we hit because preparsing transforms
"100" :-> "Integer(100)"
If instead we do
"100" :-> "Integer('100')" we don't observe the slowdown:

```
sage: for j in range(10):
....:        s = '1'*(100000*2^j)
....:        t = cputime()
....:        si = eval("".join(["Integer('",s,"')"]))
....:        cputime(t)
0.0069990000000004216
0.014998000000000289
0.038993999999998863
0.078988000000000724
0.18397200000000069
0.44893199999999922
1.0758369999999999
2.2736540000000023
5.2492010000000029
13.029020000000003
```

If you do ` si = int(s) ` you get the quadratic behaviour as well. I see the following solutions:
 - fix python's string-to-int to run in O(input size)
 - change the preparser to generate Integer('100') instead of Integer(1000)
Option number one has the biggest benefit because that has a chance of improving python for everyone.


---

Comment by nbruin created at 2011-09-03 21:28:25

It looks like Python's conversion routine has already received some attention, so you might just be seeing GMP vs. non-GMP here. See
[Tim Peter's 2006 patch](http://svn.python.org/view/python/trunk/Objects/longobject.c?revision=46203&view=markup) (particularly line 1441 and further. This is line 1510 in python-2.6.4.p11)

So perhaps the best action is <shudder> to change the preparser.


---

Comment by leif created at 2011-09-03 21:29:47

Replying to [comment:3 nbruin]:
> I think it's python's string-to-int routine ... 

> 

> I see the following solutions: 

>  - fix python's string-to-int to run in O(input size)

That would be best, but we may get in trouble with Python upgrades. (Should get reported upstream if appropriate though.)

Have you already identified Python's bad code?




>  - change the preparser to generate Integer('100') instead of Integer(1000)

That would give quite surprising results... (Do you intend to scale _all_ integers by 0.1, or just replace 1000 by 100? The former would indeed improve the performance.) ;-)

Most probably the easiest (or quickest) way to fix this, though we have different source files for the preparser code (IIRC) which always have to be kept in sync, which is odd as well. (And the preparser is subject to other changes, too.)




> Option number one has the biggest benefit because that has a chance of improving python for everyone.

_In principle_<sup>TM</sup> yes. Best when a suitable fix is submitted upstream (and accepted of course).


---

Comment by nbruin created at 2011-09-03 22:05:12

gmp/mpir indeed has "subquadratic" code; see [mpn/generic/set_str.c](http://boxen.math.washington.edu/svn/mpir/mpir/trunk/mpn/generic/set_str.c) (the version in sage has better comments in the file). It only kicks in for strings longer than 4000 characters, so it doesn't seem to be the kind of optimization Python should care about.

[I was originally envisioning to just drop every third 0 encountered. That gives an even better performance boost in special cases, but that micro-optimization would not affect the example from the report]


---

Comment by leif created at 2011-09-03 22:20:13

I did not know Python doesn't support base-1 representation...

Do we have direct conversion routines Python long <-> `mpz_t` / `mpn_t`?

If we only changed `Integer(N)` to `Integer("N")`, "raw" integers wouldn't benefit, which have other advantages.

(I wouldn't make the Python spkg depend on GMP/MPIR btw.; instead we might clone / adapt GMP's code in a patch to the Python spkg. No idea how easy / reasonable that is.)


---

Comment by leif created at 2011-09-03 22:24:00

Replying to [comment:6 nbruin]:
> [I was originally envisioning to just drop every third 0 encountered. That gives an even better performance boost in special cases, but that micro-optimization would not affect the example from the report]

Can you provide a patch also fixing the affected doctests? :P


---

Comment by nbruin created at 2011-09-04 04:54:42

Replying to [comment:7 leif]:
> Do we have direct conversion routines Python long <-> `mpz_t` / `mpn_t`?
It looks like we do:

```
sage: for j in range(10):
....:        s = '1'*(100000*2^j)
....:        t = cputime()
....:        si = Integer(int(eval("".join(["Integer('",s,"')"]))))
....:        cputime(t)
0.012997999999999621
0.014997000000001037
0.036994999999997447
0.074988000000001165
0.17497399999999885
0.42593499999999906
1.0088469999999994
2.1586709999999982
5.0012399999999992
12.395115000000001
```

> (I wouldn't make the Python spkg depend on GMP/MPIR btw.; instead we might clone / adapt GMP's code in a patch to the Python spkg. No idea how easy / reasonable that is.)

Yes, that is what I thought as well. However even in GMP/MPIR they only think it's worth switching algorithm for 4000+ digits. I would not expect Python maintainers to be very interested in optimizing for such a rare situation.

There is a much easier workaround, though: Write your integers base 2,4,8,16 (or even 32). The 16 is probably most straightforward, because literals are automatically recognised when prefixed by "0x". It's a much more efficient way of storing integers and a 4000 digit number is going to be meaningless to the human eye anyway. This is really *much* faster:

```
sage: for j in range(10):
....:        s = "0x"+'1'*(100000*2^j)
....:        t = cputime()
....:        si = eval(s)
....:        cputime(t)
0.0019989999999978636
0.0020000000000095497
0.0019999999999953388
0.0049989999999979773
0.012997999999996068
0.025995999999992137
0.05199300000001017
0.099985000000003765
0.19696999999999321
0.38994000000000995
```

So my proposal: Document that if people want to store long numbers, they should do so in hex. Mention that "Integer(<string>)" is asymptotically faster that "int(<string>)" and that the preparser turns "<literal>" into something equivalent to "Integer(int(<string>))".


---

Comment by zimmerma created at 2011-09-04 18:06:27

Replying to [comment:9 nbruin]:
> So my proposal: Document that if people want to store long numbers, they should do so in hex. Mention that "Integer(<string>)" is asymptotically faster that "int(<string>)" and that the preparser turns "<literal>" into something equivalent to "Integer(int(<string>))".

I guess you are joking? No serious computer algebra package would *require* users to write inputs
in hexadecimal.

Paul Zimmermann


---

Comment by nbruin created at 2011-09-04 20:24:27

Replying to [comment:10 zimmerma]:
> I guess you are joking? No serious computer algebra package would *require* users to write inputs
> in hexadecimal.

I didn't mean the documentation addition as a joke and I didn't mean it as a requirement. Just a tip to get better performance, and that tip will apply even if we can get reading decimal integer representations faster.

It's the Python routine that is less-than-optimal here. If we can fix that and get it accepted upstream: Great! However, CPython is meant to be general purpose, so doesn't want good MPint performance at all cost (otherwise they would have moved to GMP already), so our patch would have to be clean and concise as well.

Patching Python with something that won't be accepted upstream is probably worse than the disease. Projects like sage-on-gentoo etc. are definitely not going to be happy with it.

I realized we cannot change the preparser to do "100":->"Integer('100')" in general. Currently, the "100" in "Integer(100)" gets converted to an integer constant *at compile time*. Quoting it would trigger a string-to-int conversion *at run time*. `preparse_file` alleviates this somewhat by pushing constant definitions outside of loops, but the principle remains. 

The workaround for the example in the report at the moment is:

```
sigma=Integer(file('b.sage').read()[7:])
```

In the presence of Python's suboptimal behaviour, "load" simply optimizes for a different use-case (by preferring to translate via Python's "int"). Actually, that is a wrong optimization! "load <file>.sage" never leads to a ".pyc" file, so "compile" and "run" time are never separate.

So perhaps
`sage.misc.preparser.preparse_file` needs a third state for "numeric_literals" which makes integer constants convert via strings? This option would be safe if preparse/compile/run are guaranteed to only occur together. Some experiments would quickly show if this should be done to all integer literals or just those of certain minimum length. I wouldn't dare to touch the preparser but someone else might.
