# Issue 22844: Matchings in multigraphs

Issue created by migration from https://trac.sagemath.org/ticket/23081

Original creator: zgershkoff

Original creation time: 2017-05-25 23:35:46

CC:  dcoudert tscrim




---

Comment by zgershkoff created at 2017-05-26 23:52:54

Changing priority from major to minor.


---

Comment by zgershkoff created at 2017-05-26 23:52:54

Changing keywords from "" to "matching".


---

Comment by zgershkoff created at 2017-05-26 23:52:54

Changing component from PLEASE CHANGE to graph theory.


---

Comment by zgershkoff created at 2017-05-26 23:52:54

Changing type from PLEASE CHANGE to enhancement.


---

Comment by zgershkoff created at 2017-05-26 23:52:54

Changing status from new to needs_info.


---

Comment by zgershkoff created at 2017-05-26 23:52:54

My initial attempt was to build a new list of edges to spare the expense of creating a new graph, but when I saw how complicated that would be because of all the methods and data structures associated with the graph (an edge list, an edge iterator, a dictionary with vertex pairs as keys and edge labels as values), I decided to keep it simple, and I reset my earlier commits.

It works now, but it's a little inconsistent, and I'm not sure how to best resolve it. The problem is when the set of edge labels is a mix of other types. For simple graphs G, if an edge label is not in RR, the weight of that edge is set to `1`. Otherwise the label is used as the edge weight. (Incidentally, I need to go back to bipartite matchings and make it act like this too.) If G allows multiple edges, however, `G.allow_multiple_edges(False, keep_label='max')` will directly compare the labels of parallel edges and keep the highest one _without_ first setting the weight of strings or `None` to `1`.

The solution I'm considering is to modify the code of `allow_multiple_edges` so it assigns edges a weight before comparing them, the way `matching` does now. However, this seems like it could be too broad of a change. Another solution is to trust the user to make their edge labels consistent before trying to take a weighted matching. I don't know if that is in line with the [SageMath](SageMath) design philosphy.
----
New commits:


---

Comment by dcoudert created at 2017-05-27 09:23:18

Working with loops, multiple edges and weights is a mess. The behavior and the terminology (`use_edge_labels` or `by_weights` or etc.) should be unified. For instance, you can pass a `weight_function` to `shortest_path`. This is convenient and forces the user to be careful with the kind of labels and how to use it. Such improvement is however a hard task.

A problem with loops:
I don't no why the matching method uses `self._scream_if_not_simple(allow_loops=True)`. It is not documented and it causes the following issue:

```
sage: G = Graph([(0,0),(0,1),(1,1)], loops=True)
sage: G.matching(algorithm='LP')
[(0, 0, None), (1, 1, None)]
sage: G.matching(algorithm='Edmonds')
[(0, 1, None)]
```



Now, if you want to make the matching method work with multiple edges, you can avoid a copy of the graph as follows. This is certainly not the most elegant method, but it's working.

```
        #self._scream_if_not_simple(allow_loops=True)
        from sage.rings.real_mpfr import RR
        def weight(x):
            if x in RR:
                return x
            else:
                return 1

        def reorder(u, v):
            if u < v:
                return u, v
            else:
                return v, u

        W = dict()
        for u,v,l in self.edge_iterator():
            u, v = reorder(u, v)
            if u is v:
                continue
            if use_edge_labels:
                l = weight(l)
            if not (u, v) in W or ( (u, v) in W and use_edge_labels and W[u, v] < l ):
                W[u, v] = l

        if algorithm == "Edmonds":
            import networkx
            g = networkx.Graph()
            if use_edge_labels:
                for u, v in W:
                    g.add_edge(u, v, attr_dict={"weight": W[u, v]})
            else:
                for u, v in W:
                    g.add_edge(u, v)
            d = networkx.max_weight_matching(g)
            if value_only:
                if use_edge_labels:
                    return sum(W[u, v] for u, v in six.iteritems(d) if u < v)
                else:
                    return Integer(len(d) // 2)
            else:
                return [(u, v, W[u, v]) for u, v in six.iteritems(d) if u < v]

        elif algorithm == "LP":
            from sage.numerical.mip import MixedIntegerLinearProgram
            # returns the weight of an edge considering it may not be
            # weighted ...
            p = MixedIntegerLinearProgram(maximization=True, solver=solver)
            b = p.new_variable(binary=True)
            if use_edge_labels:
                p.set_objective( p.sum( W[u, v] * b[u, v] for u, v in W ) )
            else:
                p.set_objective( p.sum( b[u, v] for u, v in W ) )
            # for any vertex v, there is at most one edge incident to v in
            # the maximum matching
            for v in g.vertex_iterator():
                p.add_constraint(
                    p.sum(b[reorder(u, v)]
                          for u in self.neighbors(v) if u != v), max=1)
            if value_only:
                if use_edge_labels:
                    return p.solve(objective_only=True, log=verbose)
                else:
                    return Integer(round(p.solve(objective_only=True, log=verbose)))
            else:
                p.solve(log=verbose)
                b = p.get_values(b)
                return [(u, v, W[u, v]) for u, v in W if b[u, v] == 1]

        else:
            raise ValueError('algorithm must be set to either "Edmonds" or "LP"')
```



---

Comment by git created at 2017-05-30 01:46:03

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by zgershkoff created at 2017-05-30 01:54:21

Changing status from needs_info to needs_review.


---

Comment by zgershkoff created at 2017-05-30 01:54:21

I like that implementation-- I wish I had kept mine so I could compare it, but it does seem to be much faster than copying the graph. I took out the `reorder` method because it seems that the edges are stored in an ordered way. (That is, they were ordered in all the tests I did, and digging through the sagemath and networkx code didn't prove otherwise.) The `LP` method wasn't working correctly, but it seems to be working after I put in the business about `min(u,v), max(u,v)`.

I also added a test for computing the matching of a multigraph, and it passes the doctest fine.

I'm a little mystified as to why some of the changes I did seemed to make it slower. David's algorithm ran at a time comparable to the old one, but around when I added the `L` dictionary, the run time practically doubled. Perhaps there were background programs on my computer that started consuming more resources.


---

Comment by zgershkoff created at 2017-05-30 02:27:03

I should clarify that the doctests for graphs passed. When I tried everything, there were some problems, but the problem is likely with my installation and not with changes to `graph.py`.


---

Comment by git created at 2017-06-02 21:26:55

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by dcoudert created at 2017-06-03 11:14:59

In the LP, the only place where you need to use `min(u,v), max(u,v)` is `p.sum(b[min(u, v), max(u,v)] for u in self.neighbors(v) if u != v), max=1)`. Here you have no guarantee that `u < v`, but each time you use `for u, v in W`,  we know that `u < v`.

You can remove the line `#self._scream_if_not_simple(allow_loops=True)`.


---

Comment by git created at 2017-06-05 00:01:14

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2017-06-05 00:24:44

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by zgershkoff created at 2017-06-05 00:28:26

In the case, I've put the reorder method back.

I do think this ticket needs to be completed (or loops need to be disallowed, because they caused false results as you've pointed out) but I'm still confused as to how my changes have made it so much slower.


---

Comment by zgershkoff created at 2017-06-05 00:31:14

Actually, I think I see it now. I'll fix it shortly.


---

Comment by git created at 2017-06-05 00:53:02

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2017-06-05 00:55:48

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by tscrim created at 2017-06-05 01:53:57

While combining the dictionaries does somewhat reduce the memory footprint, the extra step of extracting from a list is probably adding up as you probably surmised. However, the extra Python calls to `reorder` might also have a detrimental effect compared to the essentially system call of `min`. It might be worthwhile to run it through `%lprun` (and `%prun`) and see what lines in particular are taking the time. Do you have a particular medium sized example that is non-trivial but not-to-big that you are testing with?


---

Comment by git created at 2017-06-05 02:16:39

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by zgershkoff created at 2017-06-05 02:29:38

When I changed it to only call `weight(l)` if `allow_edge_labels` is `True`, it sped up significantly.

The example I've been using for a baseline reading is `G = graphs.CompleteGraph(500)`, which takes 1.05 seconds with `timeit()` on the develop branch, and 1.15 seconds on this branch. When I do `G.matching(allow_edge_labels=True)`, both branches take about 2.25 seconds. Since you suggested a non-trivial one, I tried `H = graphs.GeneralizedPetersenGraph(500,5)`. This one takes about 400ms on either branch regardless of edge labels.


---

Comment by tscrim created at 2017-06-05 05:34:35

On

```
sage: H = graphs.GeneralizedPetersenGraph(600,8)
sage: %lprun -f H.matching H.matching(use_edge_labels=True)
```

over 99% of the time is spent on the call to networkx:

```
  4418         1      6946726 6946726.0     99.4              d = networkx.max_weight_matching(g)
```

Similarly when `use_edge_labels=False`. When I do `algorithm='LP'`, similar amount of time is spent doing the LP stuff.

Conclusion: There is almost no speed to be gained except on really small graphs. So there is nothing else to really optimize out, at least by these examples.


---

Comment by dcoudert created at 2017-06-05 08:26:52

The patch is now good to go (passes all tests, etc.). I agree with Travis that it would be a lot of work to save small computation time.


---

Comment by dcoudert created at 2017-06-05 08:26:52

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2017-06-13 06:51:19

Resolution: fixed
