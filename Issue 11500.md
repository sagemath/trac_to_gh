# Issue 11500: Some more doctests from the book "Calcul mathématique avec Sage"

Issue created by migration from https://trac.sagemath.org/ticket/11672

Original creator: mmezzarobba

Original creation time: 2011-08-09 19:59:51

Assignee: mvngu

CC:  malb bouillaguet

The attached patch adds to the Sage testsuite most examples appearing Chapters 6 and 8 (Polynomials, Polynomial Systems) of the French book [Calcul mathématique avec Sage](http://sagebook.gforge.inria.fr/). See #9395 for some background.

All tests pass with sage 4.7.


---

Comment by mmezzarobba created at 2011-08-10 06:03:42

Changing status from new to needs_review.


---

Comment by zimmerma created at 2011-08-24 19:36:50

Martin, please could you review this? This will help our book to be more stable with future
versions of Sage. Thank you in advance (as a coauthor of the book, I'm quite reluctant to
review this).

Paul


---

Comment by malb created at 2011-08-24 21:50:05

Sure, I can take a look:

 * Please rename the patch to have the ".patch" suffix.
 * There might be an issue with the UTF-8, but we should try and wait if anybody complaints.
 * Do you really want to include an autogenerated file without any manual post processing? ("This file was *autogenerated* from sagebook.tex with sagetex.sty")
 * Please add your full name to the author field here on Trac.
 * I get some numerical noise errors:

```python
File "/mnt/usb1/scratch/malb/sage-4.7.2/devel/sage/sage/tests/french_book/mpoly.py", line 373:
    sage: [CDF['x'](p(y=ys[0][0])).roots() for p in J.gens()]
Expected:
    [[(-0.6 - 1.30624677741e-16*I, 1), (0.6 + 1.30624677741e-16*I, 1)],
    [(0.6 - 3.13499226579e-16*I, 1), (2.6 + 3.13499226579e-16*I, 1)]]
Got:
    [[(-0.6 - 1.30628991909e-16*I, 1), (0.6 + 1.30628991909e-16*I, 1)], 
    [(0.6 - 3.13509580582e-16*I, 1), (2.6 + 3.13509580582e-16*I, 1)]]
```



---

Comment by malb created at 2011-08-24 21:50:05

Changing status from needs_review to needs_work.


---

Comment by zimmerma created at 2011-08-25 10:15:13

Martin, on which platform do you get numerical noise errors? With Sage 4.7 on a 64-bit Core 2,
I get no difference.

Paul


---

Comment by malb created at 2011-08-26 12:40:24

This was on sage.math (sorry, should have mentioned).


---

Comment by Bouillaguet created at 2013-01-12 09:02:35

I will try to make this go through


---

Comment by zimmerma created at 2013-01-12 11:32:51

thanks Charles!

Paul


---

Comment by mmezzarobba created at 2013-01-18 13:14:03

Changing priority from major to minor.


---

Comment by mmezzarobba created at 2013-01-18 13:14:03

Changing status from needs_work to needs_review.


---

Comment by mmezzarobba created at 2013-01-18 13:16:52

Charles: Thanks!

Here is a new version of the patch, updated to the last draft of the book (which contains what should hopefully be the final version of the chapters in question...) and to Sage 5.5.


---

Comment by Bouillaguet created at 2013-01-18 16:57:21

Looks good to me !


---

Comment by Bouillaguet created at 2013-01-18 16:57:21

Changing status from needs_review to positive_review.


---

Comment by jdemeyer created at 2013-01-19 13:59:32

Changing status from positive_review to needs_work.


---

Comment by jdemeyer created at 2013-01-19 13:59:32

On sage.math (Linux x86_64):

```
sage -t  -force_lib devel/sage/sage/tests/french_book/mpoly.py
**********************************************************************
File "/release/merger/sage-5.7.beta0/devel/sage-main/sage/tests/french_book/mpoly.py", line 366:
    sage: [CDF['x'](p(y=ys[0][0])).roots() for p in J.gens()]
Expected:
    [[(-0.6 - 1.30624...e-16*I, 1), (0.6 + 1.30624...e-16*I, 1)],
    [(0.6 - 3.13499...e-16*I, 1), (2.6 + 3.13499...e-16*I, 1)]]
Got:
    [[(-0.6 - 1.30628991909e-16*I, 1), (0.6 + 1.30628991909e-16*I, 1)], [(0.6 - 3.13509580582e-16*I, 1), (2.6 + 3.13509580582e-16*I, 1)]]
**********************************************************************
```



---

Comment by mmezzarobba created at 2013-01-19 14:32:30

I am unable to reproduce the failure you get on sage.math (though I am working on Linux x86_64 too),  so I just reduced the number of digits taken into account in this test again.


---

Comment by mmezzarobba created at 2013-01-19 14:32:30

Changing status from needs_work to needs_review.


---

Comment by Bouillaguet created at 2013-01-20 10:28:57

Changing status from needs_review to positive_review.


---

Comment by Bouillaguet created at 2013-01-20 10:28:57

I confirm that I ran the tests before giving the earlier positive review. It did work fine on my core i7-3615QM CPU running OS X 10.7.5...

The new patch also gets a positive review (the tests pass).


---

Comment by jdemeyer created at 2013-01-21 14:54:49

Changing status from positive_review to needs_work.


---

Comment by jdemeyer created at 2013-01-21 14:54:49

On bsd (OS X 10.6 x86_64):

```
sage -t  --long -force_lib devel/sage/sage/tests/french_book/mpoly.py
**********************************************************************
File "/Users/buildbot/build/sage/bsd-1/bsd_full/build/sage-5.7.alpha1/devel/sage-main/sage/tests/french_book/mpoly.py", line 373:
    sage: J.variety(RDF)
Expected:
    [{y: 396340.89..., x: 26.61226...}]
Got:
    [{y: 396340.890167, x: -54.9445477048}]
**********************************************************************
```


On arando (Linux Ubuntu 12.04 i686):

```
sage -t  --long -force_lib devel/sage/sage/tests/french_book/mpoly.py
**********************************************************************
File "/var/lib/buildbot/build/sage/arando-1/arando_full/build/sage-5.7.alpha1/devel/sage-main/sage/tests/french_book/mpoly.py", line 373:
    sage: J.variety(RDF)
Expected:
    [{y: 396340.89..., x: 26.61226...}]
Got:
    [{y: 396340.890167, x: -46.7888621592}]
**********************************************************************
```



---

Comment by zimmerma created at 2013-01-21 15:46:11

I wonder why we get different results for `x`. If all computations are based on IEEE 754, we should get the same (wrong) value. Is it possible to know which components of Sage are used for `J.variety(RDF)` on each platform? On mine I get:

```
sage: get_systems('J.variety(RDF)')                          
['MPFI', 'Singular', 'numpy', 'ginac']
```

Since MPFI is based on MPFR, MPFI should be architecture independent, but I wonder if the same applies to Singular, numpy and ginac...

Paul


---

Attachment

New version (last draft of the book, Sage 5.5)


---

Comment by mmezzarobba created at 2013-01-21 16:21:09

Paul: Could it be that whatever is used is compiled using extended precision on some platforms? Another strange thing, though, is that

```
sage: J.variety(Reals(p))
[{y: -1.00000001350726, x: -1929.60019649825}, {y: -0.999999986492719, x: 1929.62017010095}, {y: 396340.890166545, x: 10.3009522157620}]
```

finds 3 solutions for p as low as 39...

Anyway, this test doesn't make much sense, I shouldn't have included this test in the first place. Here is a new version of the patch without that test. I took the opportunity to ignore the last few significant digits in another numerical result that might change a bit in the future.


---

Comment by mmezzarobba created at 2013-01-21 16:21:20

Changing status from needs_work to needs_review.


---

Comment by zimmerma created at 2013-01-21 16:30:40

> Paul: Could it be that whatever is used is compiled using extended precision on some platforms?

yes, or with a different rounding precision. Anyway, it might be worth tracking down this issue. Jeroen, is there a way to trace the calls to the different Sage components (MPFI, Singular, numpy, ginac)?

Paul


---

Comment by Bouillaguet created at 2013-01-21 16:44:11

Replying to [comment:16 mmezzarobba]:
> Paul: Could it be that whatever is used is compiled using extended precision on some platforms? Another strange thing, though, is that
> {{{
> sage: J.variety(Reals(p))
> [{y: -1.00000001350726, x: -1929.60019649825}, {y: -0.999999986492719, x: 1929.62017010095}, {y: 396340.890166545, x: 10.3009522157620}]
> }}}
> finds 3 solutions for p as low as 39...
> 
> Anyway, this test doesn't make much sense, I shouldn't have included this test in the first place. Here is a new version of the patch without that test. I took the opportunity to ignore the last few significant digits in another numerical result that might change a bit in the future.

Well, the test obviously indicate that there is a problem somewhere, and I tend to believe that we should keep the test and track the problem.

Either your computation is ill-conditioned, and the test should be removed from Sage (and probably also from the book), or there is a component of sage that misbehaves. We should understand what the situation is, and act accordingly. These tests have been there for 1.5 years, so we are not in a hurry.  Maybe we could compare intermediate results on different machines?


---

Comment by jdemeyer created at 2013-01-21 19:34:00

Replying to [comment:18 zimmerma]:
> Jeroen, is there a way to trace the calls to the different Sage components (MPFI, Singular, numpy, ginac)?
I don't think so.  You could of course use `gdb` and set suitable breakpoints...


---

Comment by mmezzarobba created at 2013-01-21 19:43:22

Replying to [comment:19 Bouillaguet]:
> Either your computation is ill-conditioned, and the test should be removed from Sage (and probably also from the book), or there is a component of sage that misbehaves. We should understand what the situation is, and act accordingly.

Sorry for the lack of context. Yes, the computation is ill-conditioned, that's deliberate (see http://purple.lateralis.org/sagebook-r1014.pdf, p. 210-211), and all three results above are completely wrong (the correct result is x=6.305...).

As Paul noted, in an ideal world, this code should nontheless yield the same result everywhere, so it might have made sense to test that the result didn't change over time. But there are a number of reasonable explanations for what we observe, and (as far as I can tell) in Sage, floating-point results are usually interpreted as mere approximations that should agree with the correct answer "up to some numerical noise" rather than well-defined exact results. So I am in favor of dropping this testcase—unless the policy _is_ that machine-precision floating-point computations should yield exactly the same result on all platforms.

(Of course, this doesn't mean we shouldn't try to understand exactly what is going on!)


---

Comment by jdemeyer created at 2013-01-21 19:49:30

Or, add

```
J.variety(RDF)    # random
```



---

Comment by zimmerma created at 2013-01-21 20:41:05

I have created #13980 to keep track of the numerical noise issue, so that we can make progress on this
ticket independently.

Paul


---

Comment by mmezzarobba created at 2013-01-21 21:12:29

Replying to [comment:22 jdemeyer]:

> Or, add ` J.variety(RDF)    # random `

Is that better? (Given that many examples from the book are already excluded from the tests for various reasons.) Please tell me if you would like me to update the test file.


---

Comment by jdemeyer created at 2013-01-21 21:16:10

It was only a suggestion, do as you wish.


---

Comment by Bouillaguet created at 2013-01-26 11:11:43

Changing status from needs_review to positive_review.


---

Comment by jdemeyer created at 2013-01-30 07:34:57

Resolution: fixed
