# Issue 19630: Add Motzkin decomposition for convex cones

Issue created by migration from https://trac.sagemath.org/ticket/19867

Original creator: mjo

Original creation time: 2016-01-12 01:50:47

CC:  novoselt

Any convex cone can be decomposed into a linear subspace (its lineality space) and a pointed cone (the intersection of the original cone with the orthogonal complement of the lineality space). This can be thought of as a special case of the Motzkin decomposition for convex cones.

The name "Motzkin decomposition" for cones is nonstandard, but the name typically used to refer to the result is "cone decomposition" which is not enlightening.



---

Comment by mjo created at 2016-01-12 01:58:11

Changing status from new to needs_review.


---

Comment by mjo created at 2016-01-12 01:58:11

New commits:


---

Comment by novoselt created at 2016-01-14 06:37:37

I don't think "cone's" is correct English, at least it sounds weird.

The way you set it up, it is NOT the direct sum, but Minkowski sum.

If Motzkin decomposition is not standard, why is it called like this here? I mean it would be nice to elaborate that it is not just your invention ;-)

I would put extra check to handle corner cases so that the strict component of a strictly convex cone `is` this cone in Python sense, same with subspaces.


---

Comment by git created at 2016-01-14 16:33:14

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mjo created at 2016-01-14 17:02:57

Replying to [comment:2 novoselt]:
> I don't think "cone's" is correct English, at least it sounds weird.
> 
> The way you set it up, it is NOT the direct sum, but Minkowski sum.

I clarified it to be the Minkowski sum of orthogonal things.


> If Motzkin decomposition is not standard, why is it called like this here? I mean it would be nice to elaborate that it is not just your invention ;-)

There is a ".. NOTE" stating that the name is nonstandard. The "cone decomposition theorem" is variously attributed to Motzkin, and his decomposition of polyhedra is closely related, but I'm prevented by the language barrier from seeing just how appropriate the name might be. Supposedly the pointed/subspace decomposition was used in the original paper on the double description method but I can't find that online either.

I don't mind changing the name -- got any ideas? The name "lineal decomposition" doesn't seem to be taken.


> 
> I would put extra check to handle corner cases so that the strict component of a strictly convex cone `is` this cone in Python sense, same with subspaces.

Done!


---

Comment by vbraun created at 2016-01-15 22:52:18

maybe just .decomposition()?


---

Comment by mjo created at 2016-01-16 17:24:58

Replying to [comment:5 vbraun]:
> maybe just .decomposition()?

That makes it sound more important than it is... I can think of a few other decompositions with equal (or more) claim to the same name. What if we add one of those later?

Some other candidates would be `linear_subspace_decomposition`, `subspace_decomposition`, or `strictly_convex_decomposition`. If we're willing to use the word "pointed" to mean "strictly convex," then there's also `pointed_decomposition` and `pointed_subspace_decomposition`. That last one is probably the most descriptive of them all, but having a method named `strictly_convex_subspace_decomposition` would be getting a bit ridiculous lengthwise.


---

Comment by novoselt created at 2016-01-17 19:19:27

Cones have been there for a while and we didn't have problems with decomposition ambiguity, so Volker's suggestion makes sense to me.

Also I've missed it before, but the components are NOT orthogonal - there is no such concept for rays in the same lattice. If you insist on casting the dual cone to the primal lattice etc, then please add a warning block that this orthogonality is to be understood under the dot product for the primal lattice. Except that there is no dot product between rays of the same lattice... What do you actually need this orthogonality for anyway? Why not just pick generators which are not in L?


---

Comment by git created at 2016-01-18 15:41:47

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mjo created at 2016-01-18 16:03:11

Replying to [comment:7 novoselt]:
> Cones have been there for a while and we didn't have problems with decomposition ambiguity, so Volker's suggestion makes sense to me.

Ok, done.

> Also I've missed it before, but the components are NOT orthogonal - there is no such concept for rays in the same lattice.

Ah, sorry, got carried away when I replaced "direct sum."

> What do you actually need this orthogonality for anyway? Why not just pick generators which are not in L?

Unfortunately they do need to be orthogonal -- that's what makes the decomposition useful, and is what people expect. A lot of results for strictly convex cones can be made to work in general by extending them to the decomposition. One place I use this is to show that a cone is closed. If its strictly convex component is closed, then the whole thing is, because you can take a convergent sequence and show that the components converge where they live. That doesn't work unless the components are orthogonal.

But how to fix it... I could remove all mention of orthogonality? Then the docstring would say that you get a strictly convex cone and a linear subspace, but it wouldn't say that they're orthogonal. As an implementation detail, they would be, of course. One problem there is that I really like that test with the orthogonal projectors, but I could move that into my own personal library.

I can fake orthogonality with `sublattice_complement`. For example,


```
L_perp_gens = [ dir*l for dir in [1,-1]
                      for l in L.sublattice_complement().gens() ]
L_perp = Cone(L_perp_gens, self.lattice(), check=False)
```


But the docs mention that the lifting is non-canonical so I don't think it's any better than coercing the rays of the dual.

Or, I could just leave this function out? I need the projectors test, and it's a nice "look what I can do" function, but the function itself isn't critical to me. I could skip it entirely, and leave the function along with the orthogonal projectors test in my own library.


---

Attachment


---

Comment by novoselt created at 2016-01-31 05:24:22

If orthogonality is essential but we don't have a concept of it, then perhaps we just should not have this method indeed.

Regarding direct sum vs. Minkowski: you should be explicit what exactly is lineality space L. The current implementation returns a cone which lives in the ambient lattice which spans L. I would argue that it should be a sublattice of the ambient lattice, not a cone at all. If you insist on it being a cone, then it should be cone equal to L that lives in the sublattice L. The "other part" C should then live in some sublattice which together with L give the whole abmient lattice and indeed their direct sum will give the original cone while Minkowski sum would not make sense.

Sorry for dragging a simple function - it just does not feel right to me...


---

Comment by mjo created at 2016-01-31 15:24:41

Replying to [comment:10 novoselt]:
> If orthogonality is essential but we don't have a concept of it, then perhaps we just should not have this method indeed.

No problem, I don't want to waste time on the things that aren't important. I've inlined this function into the test suite for the paper.


> The current implementation returns a cone which lives in the ambient lattice which spans L. I would argue that it should be a sublattice of the ambient lattice, not a cone at all. 

The choice to make it a cone was mostly for symmetry in the strictly convex / solid cases, and so that things like `L.contains()`, `L.is_trivial()`, and `K is L` would work. Nevertheless, I don't know WTF I was thinking with this test:


```
sage: (C,L) = K.decomposition()
sage: x = K.random_element(ring=QQ)
sage: C.contains(x) or L.contains(x)
True
```


That's false in stupid cases like the sum of two generators, one from `C` and one from `L`. Guess I had bad luck with the random number generator. And the test after that should be,


```
x.is_zero() or not (C.contains(x) and L.contains(x))
```



---

Comment by novoselt created at 2016-01-31 17:06:22

Changing status from needs_review to positive_review.


---

Comment by novoselt created at 2016-01-31 17:06:22

Hmmm, I think I actually thought it was strange but moved on. It is however worrisome - most random elements of a cone with nontrivial C and L will be out of both, why wasn't this test failing???


---

Comment by mjo created at 2016-01-31 17:18:14

It does fail eventually; I must have had extraordinarily bad luck. It's hard to imagine that I ran that test fewer than 100 times, but once I moved it into a smaller file and ran with `--file-iterations=500` it failed.

For it to fail you need `random_cone` to give you a nontrivial cone with a nontrivial lineality space that isn't equal to the entire cone. Then `random_element` has to assign nonzero coefficients in the right places. I guess I can believe it, even though 100 is a very conservative estimate.


---

Comment by vbraun created at 2016-02-23 22:58:22

Resolution: wontfix
