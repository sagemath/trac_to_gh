# Issue 17687: Failure in doctest framework when running in docker

Issue created by migration from Trac.

Original creator: Vincent.Neri

Original creation time: 2015-03-10 08:59:17

CC:  vbraun nthiery hivert jdemeyer

Keywords: docker

In all cases, all tests pass smoothly, except for three files:

    sage -t --warn-long 19.5 src/sage/interfaces/qsieve.py  # Bad exit: 1
    sage -t --warn-long 19.5 local/lib/python2.7/site-packages/sagenb-0.11.4-py2.7.egg/sagenb/notebook/worksheet.py
    # Bad exit: 1
    sage -t --warn-long 19.5 local/lib/python2.7/site-packages/sagenb-0.11.4-py2.7.egg/sagenb/notebook/cell.py
    # Bad exit: 1

In all three cases, it's actually the doctesting framework that fails,
at the end of forker.py, where it seems to be a race condition.

    Traceback (most recent call last):
      File "/home/sage/sage-6.4.1/local/lib/python/multiprocessing/process.py", line 258, in _bootstrap
        self.run()
      File "/home/sage/sage-6.4.1/local/lib/python2.7/site-packages/sage/doctest/forker.py", line 1839, in run
        task(self.options, self.outtmpfile, msgpipe, self.result_queue)
      File "/home/sage/sage-6.4.1/local/lib/python2.7/site-packages/sage/doctest/forker.py", line 2159, in __call__
        result_queue.put(result, False)
      File "/home/sage/sage-6.4.1/local/lib/python/multiprocessing/queues.py", line 102, in put
        raise Full
    Full
        Bad exit: 1


---

Comment by nthiery created at 2015-03-10 09:38:47

Changing status from new to needs_review.


---

Comment by jdemeyer created at 2015-03-10 09:45:44

New commits:


---

Comment by jdemeyer created at 2015-03-10 09:56:06

Changing status from needs_review to needs_work.


---

Comment by jdemeyer created at 2015-03-10 09:56:06

Let's first take some time to understand this problem before jumping to add workarounds.


---

Comment by jdemeyer created at 2015-03-10 10:17:57

What happens if you add this patch (just this one, not on top of the branch of this ticket)?

```
diff --git a/src/sage/doctest/forker.py b/src/sage/doctest/forker.py
index fde65c3..91eb232 100644
--- a/src/sage/doctest/forker.py
+++ b/src/sage/doctest/forker.py
`@``@` -1538,6 +1538,7 `@``@` class DocTestDispatcher(SageObject):
                             # would leave them open until we call
                             # report(), parallel testing can easily fail
                             # with a "Too many open files" error.
+                            time.sleep(1)
                             w.save_result_output()
                             finished.append(w)
                     workers = new_workers
```



---

Comment by jdemeyer created at 2015-03-10 10:27:21

Or even better, try this:

```
diff --git a/src/sage/doctest/forker.py b/src/sage/doctest/forker.py
index fde65c3..d214dc2 100644
--- a/src/sage/doctest/forker.py
+++ b/src/sage/doctest/forker.py
`@``@` -1538,6 +1538,7 `@``@` class DocTestDispatcher(SageObject):
                             # would leave them open until we call
                             # report(), parallel testing can easily fail
                             # with a "Too many open files" error.
+                            time.sleep(1)
                             w.save_result_output()
                             finished.append(w)
                     workers = new_workers
`@``@` -2159,5 +2160,6 `@``@` class DocTestTask(object):
             result = (0, DictAsObject(dict(err=exc_info[0], tb=tb)))
 
         if result_queue is not None:
+            print("Process %s writing to %r"%(os.getpid(), result_queue))
             result_queue.put(result, False)
         return result
```


and run

```
./sage -bt src/sage/rings/integer.pyx      # or whatever test which normally works
./sage -bt src/sage/interfaces/qsieve.py   # or whatever test which normally fails
```



---

Comment by nthiery created at 2015-03-10 10:53:17

Replying to [comment:9 jdemeyer]:
> Or even better, try this:

Vincent just tried, and indeed the error disappears. Funilly enough, for qsieve.py, but not for cell.py, it's actually enough to only insert a

```
      print("test")
```

(without the sleep beforehand) to make the error disappear! Speak of an heisenbug.

Ok, now we would need to understand why there is a race condition ...


---

Comment by jdemeyer created at 2015-03-10 10:54:36

Replying to [comment:10 nthiery]:
> Replying to [comment:9 jdemeyer]:
> > Or even better, try this:
> 
> Vincent just tried, and indeed the error disappears.

Please report full output, all details...


---

Comment by vbraun created at 2015-03-14 19:00:55

I get the same error on my machine...


---

Comment by Vincent.Neri created at 2015-03-14 23:26:34

Just tried to increase the /dev/shm size to 2G using mount -o remount,size=2G /dev/shm in the (privileged) docker container and the failure still happens. Changing storage driver in docker from devicemapper to aufs, btrfs or overlay yields the same error.


---

Comment by vbraun created at 2015-03-14 23:56:03

Its pretty clear that its a race between the fork() inside qsieve and the python multiprocessing stuff.  Presumably the fork child needs to do something but isn't fast enough in this case.


---

Comment by jdemeyer created at 2015-03-15 10:23:13

> Its pretty clear that its a race between the fork() inside qsieve and the python multiprocessing stuff
Why do you think that?

Given that people are ignoring my requests for more information, nothing is clear to me.


---

Comment by jdemeyer created at 2015-03-15 10:23:13

Changing status from needs_work to needs_info.


---

Attachment


---

Comment by nthiery created at 2015-03-15 11:52:19

Replying to [comment:15 jdemeyer]:
> Given that people are ignoring my requests for more information, nothing is clear to me.

See the attached files [attachment:report1] and [attachment:report2].
Hopefully you can find something interesting besides what we reported.

I very well know the importance of having all data when tracking down
bugs, and how frustrating and time consuming it can be when you don't
have it under hand. And I appreciate that you are spending time on
this issue. But reciprocally, it's also time consuming to build
informative reports that reach the right balance of conciseness (so
that we can keep an overview of the ticket) while not losing any
useful information (for the record, I just spent 30 minutes on it).

With the issue at hand, it is really easy to reproduce everything. In
case you need to experiment further, may I recommend that you run the
tests yourself?

Thanks,
                                       Nicolas

PS: If this can save you a couple minutes, please send me your public
ssh key, and I'll grant you access to the exact machine on which I ran
the tests which has docker and the sage image installed.


---

Comment by jdemeyer created at 2015-03-15 12:28:30

Replying to [comment:16 nthiery]:
> In
> case you need to experiment further, may I recommend that you run the
> tests yourself?
I have never used docker and I don't feel like installing docker just to help fix this issue.

> PS: If this can save you a couple minutes, please send me your public
> ssh key, and I'll grant you access to the exact machine on which I ran
> the tests which has docker and the sage image installed.
That's by far the best solution.


---

Comment by nthiery created at 2015-03-15 12:55:59

Replying to [comment:17 jdemeyer]:
> That's by far the best solution.

Thanks for your key. You should now be able to:

```
ssh root`@`onevm-92.lal.in2p3.fr
docker run -t -i sagemath/sage su - sage
...                   # as in the ticket description
```

Let me know if you encounter any issue.

Anyone else interested in investigating on this machine, just send me
your key. Otherwise installing docker boils down to:

```
     apt-get install docker.io    # on Ubuntu/Debian
```


I have updated the description to include this.
Cheers,
                            Nicolas


---

Comment by jdemeyer created at 2015-03-16 10:12:53


```
jdemeyer`@`tamiyo:/usr/local/src/sage-git$ ssh root`@`onevm-92.lal.in2p3.fr
ssh: connect to host onevm-92.lal.in2p3.fr port 22: Connection timed out
```



---

Comment by nthiery created at 2015-04-22 09:01:15

Hi!

During PyCon 2015, I discussed this issue with a couple people, and filled up the following ticket:

https://github.com/docker/docker/issues/12277

Ollie Walsh jumped in, investigated, and wrote an interesting report. It might actually be an issue on Sage's side. Could someone familiar with the peexpect interface and our doctest forker have a look at his report?

Is the notebook also using queues in a way which could explain the similar looking issue appearing in the notebook tests?


---

Comment by nthiery created at 2015-06-03 21:43:24

Hi,

There are a couple other sites (including the math lab in Orsay) that would be happy to setup a patchbot if a docker container solution was available. Given that we are rather stuck, and unless someone is willing to dig in soon, I would be tempted to go for the workaround of [comment:6] for now. What do you think?

Cheers,
                                Nicolas


---

Comment by jdemeyer created at 2015-06-14 20:54:14

Replying to [comment:23 nthiery]:
> There are a couple other sites (including the math lab in Orsay) that would be happy to setup a patchbot if a docker container solution was available. Given that we are rather stuck, and unless someone is willing to dig in soon, I would be tempted to go for the workaround of [comment:6] for now. What do you think?

I don't like fixing a problem with a work-around if one does not understand why the problem occurs or even why the fix works.

From my initial investigation, I couldn't figure out what the problem was. If you're willing to re-install the docker image on that machine, I can have a second fresh look.


---

Comment by nthiery created at 2015-06-14 21:38:49

Replying to [comment:24 jdemeyer]:
> I don't like fixing a problem with a work-around if one does not understand why the problem occurs or even why the fix works.

Yes, that's why I called it a work around, not a fix :-)

> From my initial investigation, I couldn't figure out what the
> problem was. If you're willing to re-install the docker image on
> that machine, I can have a second fresh look .

Great, thanks! I'll recreate the machine for you, presumably tomorrow.


---

Comment by nthiery created at 2015-06-16 14:51:41

Hi Jeroen!

The machine is up and running for you. Note that the docker image
slightly evolved since last time (there is a default entry point set
to start sage automatically by default). I just updated the
instructions accordingly in the ticket description. If you want to be
able to use strace & stuff in a privileged docker container this
becomes:


```
> ssh root`@`onevm-143.lal.in2p3.fr

root`@`onevm-143:~# docker run --privileged -t -i --entrypoint "sudo" sagemath/sage su -

-bash-4.3# yum install strace

-bash-4.3# su - sage

... as in the ticket description ...
```


Note that in this docker image `sage` is sudoer without password.

Good hunt!
                                Nicolas


---

Comment by vbraun created at 2015-06-16 21:36:02

Minimal testcase (run with `sage -python docker-semaphore.py`):

```python
import multiprocessing, os, sys, time, signal, pexpect

## Not importing Sage makes the bug go away                                                                                                              
import sage.all


class Subprocess(multiprocessing.Process):

    def run(self):
        result_queue = multiprocessing.Queue(1)

        try:
            print('run-pre: {0}'.format(result_queue._sem))
            p = pexpect.spawn('QuadraticSieve')
            p.sendline('100000000000000000050700000000000000004563\n\n\n')
            p.closed = True  # avoid surprises from p.__del__                                                                                            
            print('run-post 1: {0}'.format(result_queue._sem))
            os.kill(p.pid, signal.SIGHUP)
            print('run-post 2: {0}'.format(result_queue._sem))
            time.sleep(0.1)
            print('run-post 3: {0}'.format(result_queue._sem))
        except SystemExit as err:
            # Not catching SystemExit makes the bug go away                                                                                              
            print(err)

        # Checking empty makes the bug go away                                                                                                           
        # print(result_queue.empty())                                                                                                                    

        # This is the bug, value should equal to one                                                                                                     
        v = result_queue._sem.get_value()
        if v == 0:
            print('got the wrong value for the semaphore')

        # Raises Full                                                                                                                                    
        result_queue.put(123, False)


if __name__ == '__main__':
    w = Subprocess()
    w.start()
```

Output inside the docker container is 

```
run-pre: <BoundedSemaphore(value=1, count=0, maxvalue=1)>
run-post 1: <BoundedSemaphore(value=1, count=0, maxvalue=1)>
run-post 2: <BoundedSemaphore(value=1, count=0, maxvalue=1)>
run-post 3: <BoundedSemaphore(value=0, count=0, maxvalue=1)>
got the wrong value for the semaphore
```

The value of the semaphore switches to zero a short while after the process is killed, but I don't understand why.  Must be something that we drag in with importing sage...


---

Comment by jdemeyer created at 2015-06-17 07:17:34

Changing status from needs_info to needs_work.


---

Comment by jdemeyer created at 2015-06-17 07:17:34

Changing component from doctest framework to interfaces.


---

Comment by jdemeyer created at 2015-06-17 07:41:08

Attached branch fixes the problem for `qsieve.py`, but not for the notebook.
----
New commits:


---

Comment by vbraun created at 2015-06-17 07:46:42

Whats the race?


---

Comment by jdemeyer created at 2015-06-17 07:49:42

It's this from `sagespawn.pyx` (#17686):

```
try:
    spawn.__init__(self, *args, **kwds)
finally:
    # Protect against a race condition where the child process
    # is interrupted *before* calling execve(). Then the SIGINT
    # signal will raise a Python KeyboardInterrupt and we end
    # up here.
    if self.pid == 0:
        _exit(1)
```



---

Comment by jdemeyer created at 2015-06-17 08:03:04

Replying to [comment:28 vbraun]:
> Minimal testcase (run with `sage -python docker-semaphore.py`):

Thank you for this excellent analysis! It was in particular the `except SystemExit as err:` which made me recognize a potential bug I had fixed in #17686.


---

Attachment

Make pexpect race condition reproducible


---

Comment by jdemeyer created at 2015-06-17 08:11:57

Adding Volker Braun as author as credit for [comment:28]


---

Comment by nthiery created at 2015-06-17 08:14:42

Great job guys! Thanks so much.

Do you think the issue with the notebook is of the same nature and could be fixed along the same lines?


---

Comment by jdemeyer created at 2015-06-17 08:17:27

Replying to [comment:39 nthiery]:
> Do you think the issue with the notebook is of the same nature and could be fixed along the same lines?

Most likely yes, except that the notebook is _supposed_ to be independent of Sage (even though, in practice, it's not). There is also the non-trivial issue of actually getting a patch into `sagenb` and making a new `sagenb` release.


---

Comment by jdemeyer created at 2015-06-17 08:18:18

I wonder if we could somehow monkey-patch the `pexcept` module such that `pexpect.spawn` always refers to the Sage version?


---

Comment by vbraun created at 2015-06-17 15:12:02

Replying to [comment:35 jdemeyer]:
> It was in particular the `except SystemExit as err:` which made me recognize a potential bug I had fixed in #17686.

I agree that it looks related, but there is actually no SystemExit caught. So I still don't understand how that race could have been responsible. Actually I don't understand at all why the SystemExit matters, it must be either a) coincidence, its just timing for the underlying race or b) catching SystemExit somehow changes how signals are handled.


---

Comment by jdemeyer created at 2015-06-17 15:28:46

Replying to [comment:42 vbraun]:
> there is actually no SystemExit caught.
Wrong...

Here is what really happens:
1. `pexpect` calls `os.fork()`, creating 2 Python processes.
1. In the child process, `pexpect` creates a `pty` and redirects stdin/stdout/stderr to this `pty`. As a consequence, anything `print`ed by the child process is not seen.
1. The child process receives a `SIGHUP`, which (due to the interrupt code in Sage) raises `SystemExit`.
1. The child process catches `SystemExit` and runs the code which is really meant for the parent process (this is the bug that #17686 fixes).
1. The child process puts something in the queue, filling it.
1. The parent process also tries to put something in the queue, which fails.


---

Comment by vbraun created at 2015-06-17 17:36:00

Replying to [comment:43 jdemeyer]:
> 1. In the child process, `pexpect` creates a `pty` and redirects stdin/stdout/stderr to this `pty`.

Argh, true.


---

Comment by vbraun created at 2015-06-17 17:40:04

Assuming there is still a failure in sagenb: can you open a new ticket? Then I'll merge this one...

Logging (optional, to a file, with high-resolution time stamps) would be nice. Not on this ticket, obviously.


---

Comment by jdemeyer created at 2015-06-17 17:47:46

Changing status from needs_work to positive_review.


---

Comment by jdemeyer created at 2015-06-17 17:50:27

Follow-up: #18721


---

Comment by vbraun created at 2015-06-19 08:25:03

Resolution: fixed
