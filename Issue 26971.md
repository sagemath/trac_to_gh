# Issue 26971: Make comparison of Matrix_gfpn_dense more efficient

Issue created by migration from https://trac.sagemath.org/ticket/27208

Original creator: SimonKing

Original creation time: 2019-02-02 20:13:03

Keywords: meataxe comparison

Currently, when comparing two matrices of type `Matrix_gfpn_dense`, the data of both is translated into a python string and then the two python strings are compared. That's very inefficient (because of copying data).

I suggest to use the upstream matrix comparison function instead.


---

Comment by SimonKing created at 2019-02-02 20:36:10

Somehow I wonder if I should make this ticket depend on #27198. However, the intended changes are thematically independent of #27198 and they concern separate locations in the code (thus, there should be no merge conflicts. Hence, I will base it on vanilla sage.


---

Comment by SimonKing created at 2019-02-02 22:46:04

Now I am totally puzzled. I expected that the upstream function would be a lot faster than Sage's `_cmp_` method (which *copies* matrix data into a string and then compares the strings). However, Sage's comparison performs like this:

```
sage: A = random_matrix(GF(9,'x'), 1000,2000)
sage: B = random_matrix(GF(9,'x'), 1000,2000)
sage: C = copy(A)
sage: %timeit A == B
The slowest run took 17.06 times longer than the fastest. This could mean that an intermediate result is being cached.
1000000 loops, best of 3: 699 ns per loop
sage: %timeit A == C
1 loop, best of 3: 1.29 s per loop
```

whereas the upstream function performs like this:

```
sage: A = random_matrix(GF(9,'x'), 1000,2000)
sage: B = random_matrix(GF(9,'x'), 1000,2000)
sage: C = copy(A)
sage: %timeit A == B
The slowest run took 25.28 times longer than the fastest. This could mean that an intermediate result is being cached.
1000000 loops, best of 3: 755 ns per loop
sage: %timeit A == C
1 loop, best of 3: 1.4 s per loop
```



---

Comment by jdemeyer created at 2019-02-03 09:00:20

Replying to [comment:4 SimonKing]:
> which *copies* matrix data into a string and then compares the strings

Are you sure about that? My bet is that what you think is happening is not what is actually happening.


---

Comment by SimonKing created at 2019-02-03 09:06:09

Replying to [comment:5 jdemeyer]:
> Replying to [comment:4 SimonKing]:
> > which *copies* matrix data into a string and then compares the strings
> 
> Are you sure about that? My bet is that what you think is happening is not what is actually happening.

In `_cmp_`, we currently have

```python
        d1 = <char*>(self.Data.Data)
        d2 = <char*>(N.Data.Data)
        cdef bytes s1, s2
        s1 = PyBytes_FromStringAndSize(d1, self.Data.RowSize * self.Data.Nor)
        s2 = PyBytes_FromStringAndSize(d2, N.Data.RowSize * N.Data.Nor)
```

The docs for `PyBytes_FromStringAndSize` say

```
Return a new bytes object with a copy of the string v as value and length len
```

So, it's a copy.


---

Comment by SimonKing created at 2019-02-03 09:31:37

There is another point: meataxe stores a matrix row in `long*`, i.e., in a block of memory whose length is a multiple of `sizeof(long)`. Therefore, there are padding bytes. These bytes are supposed to be zero.

Upstream apparently did not trust that the padding bytes are zero, and in fact I recall that during development of the first version of `Matrix_gfpn_dense`, non-zero padding bytes originally created trouble, however in the last couple of years I have never seen a non-zero padding byte again.

Therefore, I do believe that non-zero padding bytes was a bug that upstream (i.e., myself) fixed some years ago, and we can now trust that they are zero.

By consequence, we have the following possibilities to compare matrices:

1. Copy the whole chunk of memory to bytes, using `PyBytes_FromStringAndSize`, and compare the bytes. That's what we currently do.
2. Do not copy the whole chunk of memory, but instead compare chunks of memory by `memcmp`: Size `FfCurrentRowSize*self.Data.Nor`.
3. Do not look at the padding bytes, but loop through the rows of the matrix and use `memcmp` for comparison: `self.Data.Nor` times `FfCurrentRowSizeIo`, using `FfStepRow` for cycling through the rows.
4. Call the upstream function `MatCompare`, which basically is the same as 3., but uses `MatGetPtr` to access the rows.
5. Create a new upstream version of SharedMeatAxe, which replaces `MatGetPtr` by `FfStepPtr` when cycling through the rows of a matrix.

About `FfStepRow` versus `MatGetPtr`: The former advances an existing pointer by `FfCurrentRowSize`, hence, only involves a single addition. The latter directly accesses the i-th row of a matrix M, which means to compute `M->Data + i*M->RowSize`.

Apparently when looping through the rows of M, `MatGetPtr` has overhead compared with `FfStepRow`.

EDIT: Or create a new upstream version that uses `FfStepRow` for cycling, but is less paranoid about non-zero padding bytes and uses `memcpm` in `MatCompare`.


---

Comment by SimonKing created at 2019-02-03 09:47:53

I searched the upstream sources for `MatGetPtr` and found that it is quite often used in cases for which `FfStepPtr` might be more suitable. Changing ALL these places would involve some work.

Suggestion: We should improve Sage's current comparison of `Matrix_gfpn_dense` now, and when I find the time (i.e., not immediately), I'll change upstream.


---

Comment by SimonKing created at 2019-02-03 09:55:21

Question related with #27198: Isn't `s1 = PyBytes_FromStringAndSize(d1, self.Data.RowSize * self.Data.Nor)` giving an overflow for very large matrices?


---

Comment by SimonKing created at 2019-02-03 09:58:59

I just tested. Oddly enough, using `memcmp` for the whole chunk of memory isn't faster than what upstream does. Strange. Is looking at the few extra padding bytes really creating an overhead that is worse than the overhead created by `MatGetPtr`?


---

Comment by SimonKing created at 2019-02-03 10:42:02

I did not succeed to get a consistent significant speedup, at least not for matrices of moderate size (~1000 rows/columns). However, I think the old code could give an overflow for huge matrices. So, to the very least, the overflow should be fixed, and I think the new commit fixes it.

However, I will re-test for larger matrices (~10000 rows/columns). Perhaps a speedup will be visible there?
----
New commits:


---

Comment by SimonKing created at 2019-02-03 13:02:58

Changing status from new to needs_review.


---

Comment by SimonKing created at 2019-02-03 13:02:58

The commit does fix a problem (I think), although I didn't succeed yet to make it all faster. So, needs review, for now.


---

Comment by tscrim created at 2019-02-07 00:30:31

Changing status from needs_review to positive_review.


---

Comment by tscrim created at 2019-02-07 00:30:31

Since you seem to be satisfied with the current state of this ticket, I am setting to a positive review.


---

Comment by vbraun created at 2019-02-07 18:19:01

Merge conflict


---

Comment by vbraun created at 2019-02-07 18:19:01

Changing status from positive_review to needs_work.


---

Comment by git created at 2019-02-11 13:01:25

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by SimonKing created at 2019-02-11 13:10:41

Replying to [comment:15 vbraun]:
> Merge conflict

Solved.

I tried once more to improve speed, but to no avail. So, I merely fix a potential overflow for large matrices. I guess Travis' positive review is still valid...


---

Comment by SimonKing created at 2019-02-11 13:10:41

Changing status from needs_work to positive_review.


---

Comment by vbraun created at 2019-02-13 20:56:31

Resolution: fixed
