# Issue 29559: Mixed Forms - Fast zero check

Issue created by migration from https://trac.sagemath.org/ticket/29796

Original creator: @mjungmath

Original creation time: 2020-06-04 21:08:26

CC:  egourgoulhon tscrim mkoeppe




---

Comment by @mjungmath created at 2020-06-04 21:16:22

Changing keywords from "" to "manifolds, mixed_forms".


---

Comment by @mjungmath created at 2020-06-04 21:16:22

Changing component from PLEASE CHANGE to geometry.


---

Comment by @mjungmath created at 2020-06-04 21:16:34

Changing type from PLEASE CHANGE to enhancement.


---

Comment by @mjungmath created at 2020-06-18 08:36:54

Changing keywords from "manifolds, mixed_forms" to "manifolds, differential_forms, parallel".


---

Comment by @mjungmath created at 2020-06-18 20:48:41

This is my very first approach simply copied from the previous ones. However, I noticed that in lower dimensions, the parallelization is even slower. Furthermore, one could improve this process a little bit further just my considering distinct indices from the beginning (see the check in the loop).

I appreciate any help since I am not familiar with effective parallelization.
----
New commits:


---

Comment by git created at 2020-06-20 00:08:36

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @mjungmath created at 2020-06-20 00:08:55

Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.

However, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.

I appreciate any suggestions. I feel a little bit lost here.

----
New commits:


---

Comment by egourgoulhon created at 2020-06-21 16:23:52

Replying to [comment:9 gh-mjungmath]:
> Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.
> 
> However, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.
> 
> I appreciate any suggestions. I feel a little bit lost here.
> 
I would say that the behaviour that you observe is due to the computation being not fully parallelized in the current code. Indeed, in the final lines

```
            for ii, val in paral_wedge(listParalInput):
                 for jj in val:
                     cmp_r[[jj[0]]] += jj[1]
```

the computation `cmp_r[This is the Trac macro *jj[0* that was inherited from the migration](https://trac.sagemath.org/wiki/WikiMacros#jj[0-macro)] += jj[1]` is performed sequentially.


---

Comment by @mjungmath created at 2020-06-21 19:30:05

Interestingly, I dropped the summation completely, and still, the computation takes longer than without parallelization. This is odd, isn't it?

Even this modification doesn't improve anything:


```python
        ind_list = [(ind_s, ind_o) for ind_s in cmp_s._comp
                                   for ind_o in cmp_o._comp
                    if len(ind_s+ind_o) == len(set(ind_s+ind_o))]
        nproc = Parallelism().get('tensor')
        if nproc != 1:
            # Parallel computation
            lol = lambda lst, sz: [lst[i:i + sz] for i in
                                   range(0, len(lst), sz)]
            ind_step = max(1, int(len(ind_list) / nproc))
            local_list = lol(ind_list, ind_step)
            # list of input parameters:
            listParalInput = [(cmp_s, cmp_o, ind_part) for ind_part in
                              local_list]

            @parallel(p_iter='multiprocessing', ncpus=nproc)
            def paral_wedge(s, o, local_list_ind):
                partial = []
                for ind_s, ind_o in local_list_ind:
                    ind_r = ind_s + ind_o
                    partial.append([ind_r, s._comp[ind_s] * o._comp[ind_o]])
                return partial
            for ii, val in paral_wedge(listParalInput):
                for jj in val:
                    cmp_r[[jj[0]]] = jj[1]
        else:
            # Sequential computation
            for ind_s, ind_o in ind_list:
                ind_r = ind_s + ind_o
                cmp_r[[ind_r]] += cmp_s._comp[ind_s] * cmp_o._comp[ind_o]
```


If `nproc` is set to 1, the original speed is preserved.

I am fully aware that this leads to wrong results and the summation should be covered within the parallelization, somehow. Nevertheless, this seems strange to me.


---

Comment by @mjungmath created at 2020-06-23 17:21:24

Besides this odd fact, do you have any ideas how the summation can be parallelized, too?


---

Comment by mkoeppe created at 2021-02-13 20:51:01

Setting new milestone based on a cursory review of ticket status, priority, and last modification date.


---

Comment by @mjungmath created at 2021-06-27 11:52:02

By the way, why don't we use `MapReduce` patterns (or similar) for parallelizations? The parallelization syntax used all over is hardly readable imho.

See for example: https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac
