# Issue 29559: Mixed Forms - Fast zero check

archive/issues_029559.json:
```json
{
    "body": "CC:  @egourgoulhon @tscrim @mkoeppe\n\n\n\nIssue created by migration from https://trac.sagemath.org/ticket/29796\n\n",
    "created_at": "2020-06-04T21:08:26Z",
    "labels": [
        "PLEASE CHANGE",
        "major"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.8",
    "title": "Mixed Forms - Fast zero check",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/29559",
    "user": "@mjungmath"
}
```
CC:  @egourgoulhon @tscrim @mkoeppe



Issue created by migration from https://trac.sagemath.org/ticket/29796





---

archive/issue_comments_419810.json:
```json
{
    "body": "Changing keywords from \"\" to \"manifolds, mixed_forms\".",
    "created_at": "2020-06-04T21:16:22Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419810",
    "user": "@mjungmath"
}
```

Changing keywords from "" to "manifolds, mixed_forms".



---

archive/issue_comments_419811.json:
```json
{
    "body": "Changing component from PLEASE CHANGE to geometry.",
    "created_at": "2020-06-04T21:16:22Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419811",
    "user": "@mjungmath"
}
```

Changing component from PLEASE CHANGE to geometry.



---

archive/issue_comments_419812.json:
```json
{
    "body": "Changing type from PLEASE CHANGE to enhancement.",
    "created_at": "2020-06-04T21:16:34Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419812",
    "user": "@mjungmath"
}
```

Changing type from PLEASE CHANGE to enhancement.



---

archive/issue_comments_419813.json:
```json
{
    "body": "Changing keywords from \"manifolds, mixed_forms\" to \"manifolds, differential_forms, parallel\".",
    "created_at": "2020-06-18T08:36:54Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419813",
    "user": "@mjungmath"
}
```

Changing keywords from "manifolds, mixed_forms" to "manifolds, differential_forms, parallel".



---

archive/issue_comments_419814.json:
```json
{
    "body": "This is my very first approach simply copied from the previous ones. However, I noticed that in lower dimensions, the parallelization is even slower. Furthermore, one could improve this process a little bit further just my considering distinct indices from the beginning (see the check in the loop).\n\nI appreciate any help since I am not familiar with effective parallelization.\n----\nNew commits:",
    "created_at": "2020-06-18T20:48:41Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419814",
    "user": "@mjungmath"
}
```

This is my very first approach simply copied from the previous ones. However, I noticed that in lower dimensions, the parallelization is even slower. Furthermore, one could improve this process a little bit further just my considering distinct indices from the beginning (see the check in the loop).

I appreciate any help since I am not familiar with effective parallelization.
----
New commits:



---

archive/issue_comments_419815.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2020-06-20T00:08:36Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419815",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_419816.json:
```json
{
    "body": "Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.\n\nHowever, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.\n\nI appreciate any suggestions. I feel a little bit lost here.\n\n----\nNew commits:",
    "created_at": "2020-06-20T00:08:55Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419816",
    "user": "@mjungmath"
}
```

Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.

However, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.

I appreciate any suggestions. I feel a little bit lost here.

----
New commits:



---

archive/issue_comments_419817.json:
```json
{
    "body": "Replying to [comment:9 gh-mjungmath]:\n> Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.\n> \n> However, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.\n> \n> I appreciate any suggestions. I feel a little bit lost here.\n> \nI would say that the behaviour that you observe is due to the computation being not fully parallelized in the current code. Indeed, in the final lines\n\n```\n            for ii, val in paral_wedge(listParalInput):\n                 for jj in val:\n                     cmp_r[[jj[0]]] += jj[1]\n```\n\nthe computation `cmp_r[This is the Trac macro *jj[0* that was inherited from the migration](https://trac.sagemath.org/wiki/WikiMacros#jj[0-macro)] += jj[1]` is performed sequentially.",
    "created_at": "2020-06-21T16:23:52Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419817",
    "user": "@egourgoulhon"
}
```

Replying to [comment:9 gh-mjungmath]:
> Some computations in 4 dimensions made it slightly worse: from around 8 sec to 15 sec. In contrast, more complicated computations in 6 dimensions yield a good improvement.
> 
> However, I noticed that the cpus are not fully engaged and run around 20-80% workload, varying all the time. Hence, there is much room for improvement.
> 
> I appreciate any suggestions. I feel a little bit lost here.
> 
I would say that the behaviour that you observe is due to the computation being not fully parallelized in the current code. Indeed, in the final lines

```
            for ii, val in paral_wedge(listParalInput):
                 for jj in val:
                     cmp_r[[jj[0]]] += jj[1]
```

the computation `cmp_r[This is the Trac macro *jj[0* that was inherited from the migration](https://trac.sagemath.org/wiki/WikiMacros#jj[0-macro)] += jj[1]` is performed sequentially.



---

archive/issue_comments_419818.json:
```json
{
    "body": "Interestingly, I dropped the summation completely, and still, the computation takes longer than without parallelization. This is odd, isn't it?\n\nEven this modification doesn't improve anything:\n\n\n```python\n        ind_list = [(ind_s, ind_o) for ind_s in cmp_s._comp\n                                   for ind_o in cmp_o._comp\n                    if len(ind_s+ind_o) == len(set(ind_s+ind_o))]\n        nproc = Parallelism().get('tensor')\n        if nproc != 1:\n            # Parallel computation\n            lol = lambda lst, sz: [lst[i:i + sz] for i in\n                                   range(0, len(lst), sz)]\n            ind_step = max(1, int(len(ind_list) / nproc))\n            local_list = lol(ind_list, ind_step)\n            # list of input parameters:\n            listParalInput = [(cmp_s, cmp_o, ind_part) for ind_part in\n                              local_list]\n\n            @parallel(p_iter='multiprocessing', ncpus=nproc)\n            def paral_wedge(s, o, local_list_ind):\n                partial = []\n                for ind_s, ind_o in local_list_ind:\n                    ind_r = ind_s + ind_o\n                    partial.append([ind_r, s._comp[ind_s] * o._comp[ind_o]])\n                return partial\n            for ii, val in paral_wedge(listParalInput):\n                for jj in val:\n                    cmp_r[[jj[0]]] = jj[1]\n        else:\n            # Sequential computation\n            for ind_s, ind_o in ind_list:\n                ind_r = ind_s + ind_o\n                cmp_r[[ind_r]] += cmp_s._comp[ind_s] * cmp_o._comp[ind_o]\n```\n\n\nIf `nproc` is set to 1, the original speed is preserved.\n\nI am fully aware that this leads to wrong results and the summation should be covered within the parallelization, somehow. Nevertheless, this seems strange to me.",
    "created_at": "2020-06-21T19:30:05Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419818",
    "user": "@mjungmath"
}
```

Interestingly, I dropped the summation completely, and still, the computation takes longer than without parallelization. This is odd, isn't it?

Even this modification doesn't improve anything:


```python
        ind_list = [(ind_s, ind_o) for ind_s in cmp_s._comp
                                   for ind_o in cmp_o._comp
                    if len(ind_s+ind_o) == len(set(ind_s+ind_o))]
        nproc = Parallelism().get('tensor')
        if nproc != 1:
            # Parallel computation
            lol = lambda lst, sz: [lst[i:i + sz] for i in
                                   range(0, len(lst), sz)]
            ind_step = max(1, int(len(ind_list) / nproc))
            local_list = lol(ind_list, ind_step)
            # list of input parameters:
            listParalInput = [(cmp_s, cmp_o, ind_part) for ind_part in
                              local_list]

            @parallel(p_iter='multiprocessing', ncpus=nproc)
            def paral_wedge(s, o, local_list_ind):
                partial = []
                for ind_s, ind_o in local_list_ind:
                    ind_r = ind_s + ind_o
                    partial.append([ind_r, s._comp[ind_s] * o._comp[ind_o]])
                return partial
            for ii, val in paral_wedge(listParalInput):
                for jj in val:
                    cmp_r[[jj[0]]] = jj[1]
        else:
            # Sequential computation
            for ind_s, ind_o in ind_list:
                ind_r = ind_s + ind_o
                cmp_r[[ind_r]] += cmp_s._comp[ind_s] * cmp_o._comp[ind_o]
```


If `nproc` is set to 1, the original speed is preserved.

I am fully aware that this leads to wrong results and the summation should be covered within the parallelization, somehow. Nevertheless, this seems strange to me.



---

archive/issue_comments_419819.json:
```json
{
    "body": "Besides this odd fact, do you have any ideas how the summation can be parallelized, too?",
    "created_at": "2020-06-23T17:21:24Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419819",
    "user": "@mjungmath"
}
```

Besides this odd fact, do you have any ideas how the summation can be parallelized, too?



---

archive/issue_comments_419820.json:
```json
{
    "body": "Setting new milestone based on a cursory review of ticket status, priority, and last modification date.",
    "created_at": "2021-02-13T20:51:01Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419820",
    "user": "@mkoeppe"
}
```

Setting new milestone based on a cursory review of ticket status, priority, and last modification date.



---

archive/issue_comments_419821.json:
```json
{
    "body": "By the way, why don't we use `MapReduce` patterns (or similar) for parallelizations? The parallelization syntax used all over is hardly readable imho.\n\nSee for example: https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac",
    "created_at": "2021-06-27T11:52:02Z",
    "issue": "https://github.com/sagemath/sagetest/issues/29559",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/29559#issuecomment-419821",
    "user": "@mjungmath"
}
```

By the way, why don't we use `MapReduce` patterns (or similar) for parallelizations? The parallelization syntax used all over is hardly readable imho.

See for example: https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac
