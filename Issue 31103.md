# Issue 31103: memory leak when constructing a row of an integer matrix

Issue created by migration from https://trac.sagemath.org/ticket/31340

Original creator: @DaveWitteMorris

Original creation time: 2021-02-04 19:54:07

CC:  embray dimpase

Keywords: matrix

It was discovered in #31313 that constructing a row of an integer matrix results in a memory leak on some platforms. The leak was seen on MacOS 10.15.7 and `CoCalc` (Ubuntu 20.04), but not CentOS Linux 7.9.2009 (see ticket:31313#comment:6 and ticket:31313#comment:5). On MacOS:

```
sage: A = Matrix(ZZ, 200)
sage: def memleak(A):
....:     for i in range(10000):
....:         row0 = A[0]
sage: memleak(A)
sage: start_mem = get_memory_usage()
sage: memleak(A)
sage: print(f"memory leak = {round(get_memory_usage() - start_mem)}MB")
memory leak = 48.0MB
```

The leak seems to disappear if `ZZ` is replaced with  `QQ` or `RR` or `GF(2^10)`. 


---

Comment by @DaveWitteMorris created at 2021-02-04 21:14:10

It seems that a substantial part of the leak arises from just making a list of integers:

```
def memleak():
    for i in range(10000):
        [ZZ(1) for j in range(200)]
memleak()
start_mem = get_memory_usage()
memleak()
print(f"memory leak = {get_memory_usage() - start_mem}MB")
```

I got different results on different runs, but on MacOS they were usually around 15MB, though I also got 26MB and 31MB. `CoCalc` seemed to be consistently about 30MB. I usually (but not always) got 0.0MB for `QQ`, `RR`, and `GF(2^10)`.

There is also usually no leak if I change `[ZZ(1) for j in range(200)]` to `[ZZ(1)] * 200`.


---

Comment by @DaveWitteMorris created at 2021-02-05 17:19:50

As an additional data point, perhaps I should have also said that the leak seems to stay the same if I replace `ZZ(1)` with `ZZ(n)`, where `n` is an integer with several digits. 

The leak also stays about the same if I change `ZZ(1)` to `ZZ(1 - 1)`. However, the leak is much smaller if I change `ZZ(1)` to `ZZ(0)`. (After this change, the leak is usually 0.0MB on MacOS, but consistently 0.2578125MB on `CoCalc`.)


---

Comment by @DaveWitteMorris created at 2021-02-05 18:14:56

On `CoCalc`: I replaced `j in range(200)` with `j in range(100 + n)` for each `n` in `range(0,101)`. The number of megabytes leaked was almost exactly `0.3 * n`. That is, the leak for `range(100)` was 0.0MB, and it went up by about 0.3MB each time that that limit of the range was increased by 1. (For example, the leak with `j in range(150)` was 15.34MB, which is very close to 50 * 0.3.)


---

Comment by vbraun created at 2021-02-11 20:25:39

The significance of the 100 is surely the Sage integer allocation pool in sage/rings/integer.pyx


---

Comment by mderickx created at 2021-03-18 21:32:38

I discovered the same problem at #31511

For what it is worth, in my process of debugging it I found that the leak is not caused by objects the python garbage collector knows about:

The code:

```
import sys
import gc

for x in range(4):
    M = 3000
    mem = get_memory_usage()

    objs1 = None
    objs2 = None
    gc.collect()
    gc.collect()
    objs1 = gc.get_objects()
    for i in range(10^3):
        R = [ZZ(10) for z in range(500,500+M)]
    R = None
    gc.collect()
    gc.collect()
    objs2 = gc.get_objects()
    print("memory usage %s:"%M, get_memory_usage(mem))
    obj1_ids = {id(obj) for obj in objs1}
    objs_new = [obj for obj in objs2 if (not id(obj) in obj1_ids) and not obj is objs1]
    print("nr_new_objects",len(objs_new))
    print("new_objects", objs_new)
    print("new_object_types", [type(obj) for obj in objs_new])
    print("new_object_sizes", [sys.getsizeof(obj)  for obj in objs_new])
    objs1 = None
    objs2 = None
    obj1_ids = None
    objs_new = None
    gc.collect()
```

produces:

```
memory usage 3000: 90.3125
nr_new_objects 3
new_objects [[], [], <bound method Condition.__exit__ of <Condition(<unlocked _thread.lock object at 0x7fb77dfcf090>, 1)>>]
new_object_types [<class 'list'>, <class 'list'>, <class 'method'>]
new_object_sizes [56, 56, 64]
memory usage 3000: 90.8984375
nr_new_objects 0
new_objects []
new_object_types []
new_object_sizes []
memory usage 3000: 90.8828125
nr_new_objects 0
new_objects []
new_object_types []
new_object_sizes []
memory usage 3000: 90.8828125
nr_new_objects 0
new_objects []
new_object_types []
new_object_sizes []
```

Showing that the only new uncollectible items produced during the loop is some negligible threading stuff that is not relevant for the leak.

My knowledge of memory leaks in python stops where the knowledge of the gc stops, so I am throwing my hat in the ring.


---

Comment by mderickx created at 2021-03-18 22:20:14

Changing priority from major to critical.


---

Comment by mderickx created at 2021-03-18 22:20:14

p.s. I think a memory leak involving basically only the creation and destruction of lists of integers should be more then the default major priority. Since this is likely to affect most sage users.


---

Comment by vdelecroix created at 2021-03-19 09:38:13

`Integer` is the only class that has a custom memory allocation/deallocation `fast_tp_new`/`fast_tp_dealloc` implemented in the sage source code. One should try to simply deactivate it and see what happens.


---

Comment by mderickx created at 2021-03-19 10:10:22

I wouldn't even know how to deactivate the highly optimized memory allocation using the pool. I mean some custom memory allocation/deallocation (or at least mpz integer creation destruction code) is still needs to happen since we do need the mpz integer in the background.

However it seems that the current source code also contains some hints that might fix the problem:


```
        #  "This chapter is provided only for informational purposes and the
        #  various internals described here may change in future GMP releases.
        #  Applications expecting to be compatible with future releases should use
        #  only the documented interfaces described in previous chapters."
        #
        # If this line is used Sage is not such an application.
        #
        # The clean version of the following line is:
        #
        #  mpz_init( <mpz_t>(<char *>new + mpz_t_offset) )
        #
        # We save time both by avoiding an extra function call and
        # because the rest of the mpz struct was already initialized
        # fully using the memcpy above.

        (<__mpz_struct *>( <char *>new + mpz_t_offset) )._mp_d = <mp_ptr>mpz_alloc(GMP_LIMB_BITS >> 3)
```



```
    # Again, we move to the mpz_t and clear it. See above, why this is evil.
    # The clean version of this line would be:
    #   mpz_clear(<mpz_t>(<char *>o + mpz_t_offset))

    mpz_free((<__mpz_struct *>( <char *>o + mpz_t_offset) )._mp_d, 0)
```


However I don't have a sage development environment on a machine where I can test if replacing those lines with the clean versions in the comments actually solves the problem.


---

Comment by mderickx created at 2021-03-19 10:20:57

Additionally does anyone now what other function might need to get called according to this comment:


```
    # Free the object. This assumes that Py_TPFLAGS_HAVE_GC is not
    # set. If it was set another free function would need to be
    # called.
```


And I guess the comment above also begs for a doctest/unit test that asserts that `Py_TPFLAGS_HAVE_GC` is not set.


---

Comment by dimpase created at 2021-03-19 11:00:28

I guess deactivating the allocator means removing the code - then a genereric implemenation from a class upstairs will be used instead.


---

Comment by dimpase created at 2021-03-19 11:16:26

indeed, removing

```diff
--- a/src/sage/rings/integer.pyx
+++ b/src/sage/rings/integer.pyx
@@ -7327,139 +7327,6 @@ cdef int integer_pool_count = 0
 cdef int total_alloc = 0
 cdef int use_pool = 0
 
-
-cdef PyObject* fast_tp_new(type t, args, kwds) except NULL:
-    global integer_pool, integer_pool_count, total_alloc, use_pool
-
...
-cdef hook_fast_tp_functions():
-    """
-    Initialize the fast integer creation functions.
-    """
-    global global_dummy_Integer, sizeof_Integer, integer_pool
-
-    integer_pool = <PyObject**>check_allocarray(integer_pool_size, sizeof(PyObject*))
-
-    cdef PyObject* o
-    o = <PyObject *>global_dummy_Integer
-
-    # store how much memory needs to be allocated for an Integer.
-    sizeof_Integer = o.ob_type.tp_basicsize
-
-    # Finally replace the functions called when an Integer needs
-    # to be constructed/destructed.
-    hook_tp_functions(global_dummy_Integer, <newfunc>(&fast_tp_new), <destructor>(&fast_tp_dealloc), False)
-
 cdef integer(x):
     if isinstance(x, Integer):
         return x
@@ -7483,8 +7350,6 @@ def free_integer_pool():
     integer_pool_count = 0
     sig_free(integer_pool)
 
-# Replace default allocation and deletion with faster custom ones
-hook_fast_tp_functions()
 
 # zero and one initialization
 initialized = False

```


makes the memory leak on Debian to go away


---

Comment by dimpase created at 2021-03-19 12:37:29

Replying to [comment:8 vdelecroix]:
> `Integer` is the only class that has a custom memory allocation/deallocation `fast_tp_new`/`fast_tp_dealloc` implemented in the sage source code. One should try to simply deactivate it and see what happens.

there is one more such class, `RDF`:

```
src/sage/rings/real_double.pyx:cdef PyObject* fast_tp_new(type t, args, kwds):
```



---

Comment by dimpase created at 2021-03-19 12:39:45

with comment:12 patch applied, none of the examples showing the leak on Debian show it any more.

Should we just apply it (and probably also the same for `RDF`)?


---

Comment by vdelecroix created at 2021-03-19 15:19:05

Replying to [comment:14 dimpase]:
> with comment:12 patch applied, none of the examples showing the leak on Debian show it any more.
> 
> Should we just apply it (and probably also the same for `RDF`)?

No.

- Not using the pool is very likely to be a huge speed regression. This is the reason why there were introduced in the first place. The pool is critical, when one accesses at a Python/Cython level to the coefficient of a matrix or a polynomial. It is important to perform benchmarks here.
- Reproduce the memory leak with `RDF`
- Instead of removing the pool, see where the leak comes from. It is not unreasonable to have these kind of pools in a language such as Python.


---

Comment by nbruin created at 2021-03-19 16:16:57

Replying to [comment:10 mderickx]:
> Additionally does anyone now what other function might need to get called according to this comment:
> 
> {{{
>     # Free the object. This assumes that Py_TPFLAGS_HAVE_GC is not
>     # set. If it was set another free function would need to be
>     # called.
> }}}
> 
> And I guess the comment above also begs for a doctest/unit test that asserts that `Py_TPFLAGS_HAVE_GC` is not set.

See https://docs.python.org/3/c-api/typeobj.html#Py_TPFLAGS_HAVE_GC . If an object has pointers to other python objects, then the cyclic garbage collector needs to trace through it, and reference counts need to be propagated upon deletion of the object. Integers shouldn't do that; at least not a light-weight mpz wrapper. (objects without these flags aren't tracked, so `gc_objects` wouldn't find them. The leak can still easily be on the python heap.

**Concerning memory pools:**

Having something like memory pools in place is probably necessary, but Python has also evolved since this code was written and it may no longer be the fastest solution. For instance, Python does have an arena allocator https://docs.python.org/3/c-api/memory.html#the-pymalloc-allocator that has a lot of the gains of a dedicated memory pool (there's a little bit of initialization cost that you can save with a type-specific pool, so testing would be required). In any case, it is the kind of thing that may be worth reviewing. Now that we've transitioned to Py3, we do have a different system design underlying some of our code now and what used to be a good idea might not anymore and/or may need retuning.

It can also be that what used to be correctly functioning code now has a bug, because we might have been relying on an undocumented implementation detail that has changed.


---

Comment by dimpase created at 2021-03-19 16:44:31

these memory pools are only for 2 primitive types - while I can imagine a very specific benchmark showing a speed regression, I can also imagine that overall it's not that crucial, as many/most computations need speed from objects for which these pools aren't implemented.


---

Comment by mderickx created at 2021-03-19 19:46:05

I wanted to start working on this. But I seem to have one problem. The only machine where I can reproduce the problem, is a machine where I don't have a big enough diskspace quota to set up my own development version of sage.

I thought I would be able to reproduce it on a machine using Ubuntu 18.04 using sage-9.3beta9 however:


```
derickxm@capella:~$ ./sage2/sage/sage
┌────────────────────────────────────────────────────────────────────┐
│ SageMath version 9.3.beta9, Release Date: 2021-03-14               │
│ Using Python 3.9.2. Type "help()" for help.                        │
└────────────────────────────────────────────────────────────────────┘
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Warning: this is a prerelease version, and it may be unstable.     ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
sage: def memleak(): 
....:     for i in range(10000): 
....:         [ZZ(1) for j in range(200)] 
....: memleak() 
....: start_mem = get_memory_usage() 
....: memleak() 
....: print(f"memory leak = {get_memory_usage() - start_mem}MB")                                           
memory leak = 0.0MB
```



---

Comment by vbraun created at 2021-03-19 21:19:46

https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=964848 suggests that this is a gmp<->mpir change; If you use gmp then it drops limbs, if you use mpir then no leak


---

Comment by dimpase created at 2021-03-19 23:04:35

wow, and that's why on Fedora 30 there is no leak - they are on gmp 6.1.2, whereas
the trouble starts with gmp 6.2.

From that link

```
I investigated this more, and came to the conclusion that Sage is
incompatible with libgmp 6.2 due to the following commit:

https://gmplib.org/repo/gmp-6.2/rev/299ec6187305

This invalidates the assumptions Sage's rings/integer.pyx makes about
libgmp internals. The "global_dummy_Integer" object created there
without an explicit value will now be created using libgmp's new lazy
allocation, and Sage's fast_tp_new() forcibly changing the _mp_d member
of such an object to a new allocation will lead to an inconsistent
state (allocation size still set to 0, but non-dummy pointer set). This
causes libgmp to disregard the existing pointer and leak the memory
when setting a new value in the object.

I believe (haven't tested though) that a quick workaround would be to
change

global_dummy_Integer = Integer()
to 
global_dummy_Integer = Integer(1)
which would force the global default object to use non-lazy allocation.
```



---

Comment by vbraun created at 2021-03-19 23:40:45

The suggested fix avoids the memleak for me (Fedora 33), but there are a few test failures

You should consider updating, Fedora 30 is EOL ;)
----
New commits:


---

Comment by dimpase created at 2021-03-20 00:05:51

Replying to [comment:22 vbraun]:
> The suggested fix avoids the memleak for me (Fedora 33), but there are a few test failures
> 
> You should consider updating, Fedora 30 is EOL ;)

tell this to our brilliant sysadmins at the uni :-)


---

Comment by git created at 2021-03-20 00:15:26

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by vbraun created at 2021-03-20 00:16:31

Apparently the dummy integer must be zero, so 1-1 = 0 and is definitely not lazily allocated. Now all tests pass!


---

Comment by vbraun created at 2021-03-20 00:16:49

Changing status from new to needs_review.


---

Comment by dimpase created at 2021-03-20 08:50:38

how about a doctest?


---

Comment by embray created at 2021-03-20 13:34:24

In any case don't go breaking things at random without knowing exactly what's causing the memory leak issue.


---

Comment by vbraun created at 2021-03-20 15:23:03

`@`dimpase: What do you want to doctest? That the mp limb is actually allocated? How to even do that in an os/malloc-independent way?

`@`embray: I think we have a pretty clear understanding, libgmp introduced an optimization where limbs are not allocated in `mpz_init`, only on first use.


---

Comment by mderickx created at 2021-03-20 19:43:14

Replying to [comment:29 embray]:
> In any case don't go breaking things at random without knowing exactly what's causing the memory leak issue. 

The removal in [comment 12](https://trac.sagemath.org/ticket/31340?replyto=29#comment:12) is maybe removing things at random without knowing the exact cause. 

However the current change:
 

```diff
cdef Integer global_dummy_Integer
-global_dummy_Integer = Integer()
+global_dummy_Integer = Integer(1) - Integer(1)
```


is based on the analysis in https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=964848#10 that even pinpoints the exact commit in gmp that caused sage to become incompatible with gmp. And just makes sure that the global dummy integer is correctly initialized even with gmp's new lazy allocation mechanism.  I really don't know what else there is you want to know.


---

Comment by mkoeppe created at 2021-03-20 20:00:47

I find these explanations sufficient.


---

Comment by mkoeppe created at 2021-03-20 20:00:47

Changing status from needs_review to positive_review.


---

Comment by dimpase created at 2021-03-20 23:14:44

Replying to [comment:31 mderickx]:
> I really don't know what else there is you want to know.


```

Integer(1) - Integer(1)
```


is a weird hack. E.g. how do we know that a clever C compiler won't optimise this nonsense away, with a predictable result (resurfacing of the bug).


---

Comment by vbraun created at 2021-03-21 12:12:16

Since subtraction is a external gmp/mpir library call it can't be optimized away. Of course its possible that gmp internals change in the future, e.g. by inlining small ints, but thats just a hypothetical what-if question about future library development where all kinds of things could introduce incompatibilities.


---

Comment by vbraun created at 2021-03-22 23:54:10

Resolution: fixed


---

Comment by embray created at 2021-03-29 07:55:42

Replying to [comment:31 mderickx]:
> Replying to [comment:29 embray]:
> > In any case don't go breaking things at random without knowing exactly what's causing the memory leak issue. 
> 
> The removal in [comment 12](https://trac.sagemath.org/ticket/31340?replyto=29#comment:12) is maybe removing things at random without knowing the exact cause. 
> 
> However the current change:
>  
> {{{#!diff
> cdef Integer global_dummy_Integer
> -global_dummy_Integer = Integer()
> +global_dummy_Integer = Integer(1) - Integer(1)
> }}}
> 
> is based on the analysis in https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=964848#10 that even pinpoints the exact commit in gmp that caused sage to become incompatible with gmp. And just makes sure that the global dummy integer is correctly initialized even with gmp's new lazy allocation mechanism.  I really don't know what else there is you want to know.

Sorry, I haven't had time last week to keep up with Sage, but thank you for pointing this out.  I was reacting earlier to proposals to remove the integer pool.


---

Comment by embray created at 2021-03-29 08:00:22

The lazy allocation optimization in GMP is a good idea in general, but it's a shame it didn't come with an API addition for bypassing this and making immediate allocations without some kind of hack like this.


---

Comment by vdelecroix created at 2021-03-29 08:48:17

Replying to [comment:37 embray]:
> The lazy allocation optimization in GMP is a good idea in general, but it's a shame it didn't come with an API addition for bypassing this and making immediate allocations without some kind of hack like this.

Agreed. On the good side GMP welcomes very much comments, wishes and patches. For example they implemented `mpq_cmp_z(mpq_t, mpz_t)` at my demand not so long ago.


---

Comment by tornaria created at 2021-12-26 15:24:01

The fix here is incorrect, see #33081.

In fact `Integer(1) - Integer(1)` creates an integer 0 but with `_mp_alloc == 2`. However, `fast_tp_new()` only allocates one limb, creating bad integers.

See #33081 for more details and a new fix.

TL;DR: the API is to use `_mpz_realloc(..., 1)` to reallocate the dummy integer to exactly one limb.

BTW: reusing an integer from the pool is about 10 times faster than creating a new one (something like 5ns vs 50ns)


---

Comment by mkoeppe created at 2021-12-26 18:12:16

This probably also explains the errors reported by `valgrind` that can be seen in #33073#comment:13
