# Issue 26126: Polyhedron_normaliz.save

archive/issues_026126.json:
```json
{
    "body": "CC:  @jplab winfried @tscrim @kliem\n\n\n```\nsage: P = polytopes.dodecahedron(backend='normaliz')\nsage: P\nA 3-dimensional polyhedron in (Number Field in sqrt5 with defining polynomial x^2 - 5)^3 defined as the convex hull of 20 vertices\nsage: P.save('dodecahedron.sobj')\nTypeError: can't pickle PyCapsule objects\n```\n\n\nIssue created by migration from https://trac.sagemath.org/ticket/26363\n\n",
    "created_at": "2018-09-29T21:59:54Z",
    "labels": [
        "geometry",
        "major",
        "bug"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.1",
    "title": "Polyhedron_normaliz.save",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/26126",
    "user": "@mkoeppe"
}
```
CC:  @jplab winfried @tscrim @kliem


```
sage: P = polytopes.dodecahedron(backend='normaliz')
sage: P
A 3-dimensional polyhedron in (Number Field in sqrt5 with defining polynomial x^2 - 5)^3 defined as the convex hull of 20 vertices
sage: P.save('dodecahedron.sobj')
TypeError: can't pickle PyCapsule objects
```


Issue created by migration from https://trac.sagemath.org/ticket/26363





---

archive/issue_comments_368391.json:
```json
{
    "body": "One could define in `Polyhedron_normaliz`:\n\n\n```\ndef __getstate__(self):\n     state = super(Polyhedron_normaliz, self).__getstate__()\n     # Remove the unpicklable entries.\n     del state[1]['_normaliz_cone']\n     return state\n```\n\n\nThis constructs an object just as\n\n```\nsage: P = P.base_extend(P.base_ring(),backend='field')\nsage: P.base_extend(P.base_ring(),backend='normaliz')\n```\n\n(but saving computed results)\n\nHowever, one would need a method to recover `_normaliz_cone` (this method is needed anyway, to make the second thing work).",
    "created_at": "2019-07-05T19:11:13Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368391",
    "user": "@kliem"
}
```

One could define in `Polyhedron_normaliz`:


```
def __getstate__(self):
     state = super(Polyhedron_normaliz, self).__getstate__()
     # Remove the unpicklable entries.
     del state[1]['_normaliz_cone']
     return state
```


This constructs an object just as

```
sage: P = P.base_extend(P.base_ring(),backend='field')
sage: P.base_extend(P.base_ring(),backend='normaliz')
```

(but saving computed results)

However, one would need a method to recover `_normaliz_cone` (this method is needed anyway, to make the second thing work).



---

archive/issue_comments_368392.json:
```json
{
    "body": "I think I know what to do about it.\n\n1. As mentioned one can just remove the cone on pickling. Then the loaded object is just as good as changing backend back and forth (and changing backend to normaliz should also work and still give us a cone or a way to retrive the cone).\n\n2. Next step would be do allow initialization of a cone from `Vrepresentation` and `Hrepresentation`. This works by homogenization of the input and explicitly giving a dehomogenization (this is the only way that Normaliz accepts precomputed data).\n\n   Note: I'm not proposing to allow to give both representations to normaliz by the user, but when changing fields or loading a stored object I think we should trust them to be correct.\n\n3. Once this is done, one can set up normaliz cone to be a lazy attribute.",
    "created_at": "2019-10-19T13:29:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368392",
    "user": "@kliem"
}
```

I think I know what to do about it.

1. As mentioned one can just remove the cone on pickling. Then the loaded object is just as good as changing backend back and forth (and changing backend to normaliz should also work and still give us a cone or a way to retrive the cone).

2. Next step would be do allow initialization of a cone from `Vrepresentation` and `Hrepresentation`. This works by homogenization of the input and explicitly giving a dehomogenization (this is the only way that Normaliz accepts precomputed data).

   Note: I'm not proposing to allow to give both representations to normaliz by the user, but when changing fields or loading a stored object I think we should trust them to be correct.

3. Once this is done, one can set up normaliz cone to be a lazy attribute.



---

archive/issue_comments_368393.json:
```json
{
    "body": "In #28639 I will implement a method that generates the cone from both Vrep and Hrep (recomputing the lines, but thats ok I guess). I have tested this with a few polyhedra, but I have no idea, which examples can be tricky.",
    "created_at": "2019-10-19T20:38:36Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368393",
    "user": "@kliem"
}
```

In #28639 I will implement a method that generates the cone from both Vrep and Hrep (recomputing the lines, but thats ok I guess). I have tested this with a few polyhedra, but I have no idea, which examples can be tricky.



---

archive/issue_comments_368394.json:
```json
{
    "body": "New commits:",
    "created_at": "2019-12-02T09:02:25Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368394",
    "user": "@kliem"
}
```

New commits:



---

archive/issue_comments_368395.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2019-12-02T09:56:28Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368395",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_368396.json:
```json
{
    "body": "New commits:",
    "created_at": "2019-12-02T10:03:46Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368396",
    "user": "@kliem"
}
```

New commits:



---

archive/issue_comments_368397.json:
```json
{
    "body": "Changing status from new to needs_review.",
    "created_at": "2019-12-02T10:03:46Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368397",
    "user": "@kliem"
}
```

Changing status from new to needs_review.



---

archive/issue_comments_368398.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2019-12-02T10:12:31Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368398",
    "user": "git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_368399.json:
```json
{
    "body": "I plan to extend Normaliz as suggested. The essential point is that Normaliz can skip a convex hull computation, but nevertheless can produce the facet/ectreme ray incidence vectors and keep them for further operations, like the modification of the original cone.\n\nI am not sure whether one can give up the requirement to input the precomputed data in homogenized form with an added dehomogenization if they come from an inhomogeneous computation.",
    "created_at": "2019-12-02T21:48:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368399",
    "user": "Winfried"
}
```

I plan to extend Normaliz as suggested. The essential point is that Normaliz can skip a convex hull computation, but nevertheless can produce the facet/ectreme ray incidence vectors and keep them for further operations, like the modification of the original cone.

I am not sure whether one can give up the requirement to input the precomputed data in homogenized form with an added dehomogenization if they come from an inhomogeneous computation.



---

archive/issue_comments_368400.json:
```json
{
    "body": "Ticket retargeted after milestone closed",
    "created_at": "2020-01-06T14:10:03Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368400",
    "user": "@embray"
}
```

Ticket retargeted after milestone closed



---

archive/issue_comments_368401.json:
```json
{
    "body": "New commits:",
    "created_at": "2020-01-27T14:26:07Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368401",
    "user": "@kliem"
}
```

New commits:



---

archive/issue_comments_368402.json:
```json
{
    "body": "If there are any extensions to Normaliz that can be utilized in the future (as per comment:11), we can do further changes then. This is a nice improvement. LGTM.",
    "created_at": "2020-01-29T06:03:12Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368402",
    "user": "@tscrim"
}
```

If there are any extensions to Normaliz that can be utilized in the future (as per comment:11), we can do further changes then. This is a nice improvement. LGTM.



---

archive/issue_comments_368403.json:
```json
{
    "body": "Changing status from needs_review to positive_review.",
    "created_at": "2020-01-29T06:03:12Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368403",
    "user": "@tscrim"
}
```

Changing status from needs_review to positive_review.



---

archive/issue_comments_368404.json:
```json
{
    "body": "Resolution: fixed",
    "created_at": "2020-01-31T23:49:53Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368404",
    "user": "@vbraun"
}
```

Resolution: fixed



---

archive/issue_comments_368405.json:
```json
{
    "body": "Meanwhile I have implemented the use of precomputed data. Version 3.8.4 will very soon be published. Normaliz accepts \n\n```\nType::extreme_rays\n```\n\nand \n\n```\nType::support_hyperplanes\n```\n\nas precomputed data. \n\nHowevber, these do not always define the cone (or polyhedron) in which we want to compute since nontrivial coordinate transformations may come into play. These are restored via \n\n```\nType::generated_lattice\n```\n\n(also in the algebraic case) and \n\n```\nType::maximal_subspace\n```\n\n\nIt is also important that these precomputed data are considered homogeneous input types so that the polyhedron must be defined via Type::dehomogenization or Type::grading.\n\nSee Sections 6.21 and D.8.16 in Normaliz.pdf (as of today).",
    "created_at": "2020-02-02T10:15:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/26126",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/26126#issuecomment-368405",
    "user": "Winfried"
}
```

Meanwhile I have implemented the use of precomputed data. Version 3.8.4 will very soon be published. Normaliz accepts 

```
Type::extreme_rays
```

and 

```
Type::support_hyperplanes
```

as precomputed data. 

Howevber, these do not always define the cone (or polyhedron) in which we want to compute since nontrivial coordinate transformations may come into play. These are restored via 

```
Type::generated_lattice
```

(also in the algebraic case) and 

```
Type::maximal_subspace
```


It is also important that these precomputed data are considered homogeneous input types so that the polyhedron must be defined via Type::dehomogenization or Type::grading.

See Sections 6.21 and D.8.16 in Normaliz.pdf (as of today).
