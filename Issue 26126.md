# Issue 26126: Polyhedron_normaliz.save

Issue created by migration from https://trac.sagemath.org/ticket/26363

Original creator: mkoeppe

Original creation time: 2018-09-29 21:59:54

CC:  jipilab winfried tscrim @kliem


```
sage: P = polytopes.dodecahedron(backend='normaliz')
sage: P
A 3-dimensional polyhedron in (Number Field in sqrt5 with defining polynomial x^2 - 5)^3 defined as the convex hull of 20 vertices
sage: P.save('dodecahedron.sobj')
TypeError: can't pickle PyCapsule objects
```



---

Comment by @kliem created at 2019-07-05 19:11:13

One could define in `Polyhedron_normaliz`:


```
def __getstate__(self):
     state = super(Polyhedron_normaliz, self).__getstate__()
     # Remove the unpicklable entries.
     del state[1]['_normaliz_cone']
     return state
```


This constructs an object just as

```
sage: P = P.base_extend(P.base_ring(),backend='field')
sage: P.base_extend(P.base_ring(),backend='normaliz')
```

(but saving computed results)

However, one would need a method to recover `_normaliz_cone` (this method is needed anyway, to make the second thing work).


---

Comment by @kliem created at 2019-10-19 13:29:48

I think I know what to do about it.

1. As mentioned one can just remove the cone on pickling. Then the loaded object is just as good as changing backend back and forth (and changing backend to normaliz should also work and still give us a cone or a way to retrive the cone).

2. Next step would be do allow initialization of a cone from `Vrepresentation` and `Hrepresentation`. This works by homogenization of the input and explicitly giving a dehomogenization (this is the only way that Normaliz accepts precomputed data).

  Note: I'm not proposing to allow to give both representations to normaliz by the user, but when changing fields or loading a stored object I think we should trust them to be correct.

3. Once this is done, one can set up normaliz cone to be a lazy attribute.


---

Comment by @kliem created at 2019-10-19 20:38:36

In #28639 I will implement a method that generates the cone from both Vrep and Hrep (recomputing the lines, but thats ok I guess). I have tested this with a few polyhedra, but I have no idea, which examples can be tricky.


---

Comment by @kliem created at 2019-12-02 09:02:25

New commits:


---

Comment by git created at 2019-12-02 09:56:28

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2019-12-02 10:03:46

New commits:


---

Comment by @kliem created at 2019-12-02 10:03:46

Changing status from new to needs_review.


---

Comment by git created at 2019-12-02 10:12:31

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by Winfried created at 2019-12-02 21:48:11

I plan to extend Normaliz as suggested. The essential point is that Normaliz can skip a convex hull computation, but nevertheless can produce the facet/ectreme ray incidence vectors and keep them for further operations, like the modification of the original cone.

I am not sure whether one can give up the requirement to input the precomputed data in homogenized form with an added dehomogenization if they come from an inhomogeneous computation.


---

Comment by embray created at 2020-01-06 14:10:03

Ticket retargeted after milestone closed


---

Comment by @kliem created at 2020-01-27 14:26:07

New commits:


---

Comment by tscrim created at 2020-01-29 06:03:12

If there are any extensions to Normaliz that can be utilized in the future (as per comment:11), we can do further changes then. This is a nice improvement. LGTM.


---

Comment by tscrim created at 2020-01-29 06:03:12

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2020-01-31 23:49:53

Resolution: fixed


---

Comment by Winfried created at 2020-02-02 10:15:51

Meanwhile I have implemented the use of precomputed data. Version 3.8.4 will very soon be published. Normaliz accepts 

```
Type::extreme_rays
```

and 

```
Type::support_hyperplanes
```

as precomputed data. 

Howevber, these do not always define the cone (or polyhedron) in which we want to compute since nontrivial coordinate transformations may come into play. These are restored via 

```
Type::generated_lattice
```

(also in the algebraic case) and 

```
Type::maximal_subspace
```


It is also important that these precomputed data are considered homogeneous input types so that the polyhedron must be defined via Type::dehomogenization or Type::grading.

See Sections 6.21 and D.8.16 in Normaliz.pdf (as of today).
