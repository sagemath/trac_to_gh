# Issue 24280: random failure in sage/geometry/cone.py

Issue created by migration from https://trac.sagemath.org/ticket/24517

Original creator: vdelecroix

Original creation time: 2018-01-11 16:06:45

CC:  mjo

Sometimes, patchbot reports a `sig_error() without sig_on()` when tests are run on `sage/geometry/cone.py`.

See:

- [quasar 2018-01-11%2015:44:33](https://patchbot.sagemath.org/log/24511/Ubuntu/16.04/x86_64/4.4.0-104-generic/quasar/2018-01-11%2015:44:33?short)


---

Comment by jdemeyer created at 2018-01-11 16:26:52

This traceback line

```
#8  __pyx_f_4sage_3ext_6memory_alloc_error (__pyx_v_size=__pyx_v_size@entry=8) at build/cythonized/sage/ext/memory.c:1169
```

shows that it is happening because of lack of memory. So either this particular patchbot is low on memory or this doctest does something which requires lots of memory.

Now this out-of-memory situation is not gracefully handled. I could change the code to `raise MemoryError` instead of crashing, but that won't solve the underlying issue that it is running out of memory.


---

Comment by vdelecroix created at 2018-01-11 16:30:50

quasar is 8G memory (though tests are run in parallel). For me these are two problems:
- the crash when doctesting
- the fact that the `random_cone` method is not careful with the size of the generated cone

Which one do you want to deal with on this ticket?


---

Comment by jdemeyer created at 2018-01-11 16:32:59

Replying to [comment:2 vdelecroix]:
> quasar is 8G memory (though tests are run in parallel). For me these are two problems:
> - the crash when doctesting
> - the fact that the `random_cone` method is not careful with the size of the generated cone
> 
> Which one do you want to deal with on this ticket?

The last one.


---

Comment by jdemeyer created at 2018-01-11 16:36:39

I would argue not to the fix the first issue. It happened in `Rational.__new__()`. If there is not enough space for that, then surely stuff will break anyway sooner or later. Second, adding error checking would mean adding `sig_on()` and `sig_off()` which causes some loss of performance every time that a new `Rational` is created.


---

Comment by jdemeyer created at 2018-01-16 22:57:48

Changing component from documentation to doctest coverage.


---

Comment by jdemeyer created at 2018-01-16 22:57:48

Changing priority from major to blocker.


---

Comment by git created at 2018-01-16 23:16:00

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by jdemeyer created at 2018-01-16 23:16:18

Changing status from new to needs_review.


---

Comment by mjo created at 2018-01-16 23:42:43

This test shouldn't generate a cone with an unbounded number of rays. The `random_cone` method is capable of "resetting" itself, and this test is meant to check that "reset" behavior.

If I want a random cone in three dimensions with 7 extreme rays, I will start generating random rays and adding them to my cone, starting with the trivial cone. But eventually, what will likely happen is that I'll wind up with all of `R^3`, which needs only six generators. At that point, the approach is doomed: if I generate more rays and add them to `R^3`, I still get `R^3`. So what can be done? Throw it out and start over!

There do exist cones in `R^3` with 7 extreme rays, it just takes some luck to find them using the approach that `random_cone` takes. So this test ensures that we will throw out a "doomed" cone and start over when we arrive at one.

The test should succeed as soon as it gets 7 rays, and not an unbounded number. Instead, I think there's probably a little bit of memory that isn't getting freed when we throw out the old, doomed random cone and start over.

Do I need some trick like `K = None` when we're going to loop?


---

Comment by mjo created at 2018-01-16 23:52:23

To be specific, we loop until the random cone (in progress) K "is valid",


```
if is_valid(K):
    # Loop if we don't have a valid cone.
    return K
```


But that will be true as soon as this is true,


```
all([
    K.nrays() >= min_rays,
    K.nrays() <= max_rays or max_rays is None,
    K.is_solid() == solid or solid is None,
    K.is_strictly_convex() == strictly_convex or strictly_convex is None
])
```


And in this case, since everything but `min_rays` is `None`, all we need is `K.nrays() >= min_rays`. Since we add them one-at-a-time, I would hope that we always get 7.


---

Comment by mjo created at 2018-01-17 00:09:57

Aaaaaaaaand nevermind. The default for `max_rays` is `None`, which is a priori fine. The random cone generation works like I said it does above: if you try to add a million rays, you'll usually get to the sixth or seventh before you've got all of `R^3`, and then the whole thing will restart.

But there's this little snippet, which tries to construct a decent number of starting rays before we begin to generate and add them one at a time:


```
r = random_min_max(min_rays, max_rays)
rays = [L.random_element() for i in range(r)]
```


I would bet that the problem is right there, where we can generate a huge list of random rays, even if rays 7 through N will get thrown out as being redundant.

I think that's a bug in my implementation.


---

Comment by mjo created at 2018-01-17 00:41:49

Proposed fix: replace,


```
rays = [L.random_element() for i in range(r)]
```


with


```
rays = [L.random_element() for i in range(min_rays)]
```


That will be a tiny bit slower in some cases, but far more sane when `max_rays` is `None`.

It will take me a day or two to build sage (it's been a while), so you have plenty of time to let me know if that does the trick =)


---

Comment by git created at 2018-01-17 13:00:16

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by jdemeyer created at 2018-01-17 13:17:54

I tried to fix it using your suggestion. Needs review.


---

Comment by mjo created at 2018-01-18 18:24:44

I think we should keep your original fix, too, and specify `max_rays=9` in the test. The code for `max_rays=None` should not go insane, obviously, but the test case shouldn't loop more than is necessary either.

I also added an "if" statement to retain the original behavior when `max_rays` is not `None`. There's a trade-off here; namely that the user can still cause himself pain and suffering by specifying e.g. `max_rays=1000000` in a three-dimensional space (basically, the problem in this ticket). On the other hand, we would like to avoid generating rays and checking the cone's validity one ray at a time if `min_rays=1` and `max_rays=1000` in a thousand-dimensional space. If we pick `r=500` in that case, we're much better off generating 500 random rays at the start.

In any case, this shouldn't cause problems for the test suite any longer.

I didn't notice the rest of your comment changes before working on this, but it looks like they all followed from the `r` -> `min_rays` change, which is now only conditional, so please let me know if there's any other comment fix I should pull.
----
New commits:


---

Comment by git created at 2018-01-18 18:30:02

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mjo created at 2018-01-18 18:31:33

I can squash that later, I wasn't thinking straight when I wrote the comment.


---

Comment by git created at 2018-01-18 20:50:16

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mjo created at 2018-01-18 20:53:27

> There's a trade-off here; namely that the user can still cause himself pain and suffering by specifying e.g. max_rays=1000000 in a three-dimensional space (basically, the problem in this ticket). On the other hand, we would like to avoid generating rays and checking the cone's validity one ray at a time if min_rays=1 and max_rays=1000 in a thousand-dimensional space. If we pick r=500 in that case, we're much better off generating 500 random rays at the start. 
\\
I think I was able to address both of these in the latest commit. Naturally, there's some other side-effect -- more predictability when you ask for more than `2*dim` rays -- but that's an edge case and I think it's better to have the implementation less crashy.


---

Comment by jdemeyer created at 2018-01-18 21:36:26

Changing status from needs_review to needs_info.


---

Comment by jdemeyer created at 2018-01-18 21:36:26

Replying to [comment:17 mjo]:
> I think we should keep your original fix, too, and specify `max_rays=9` in the test.

Why? You convinced me that it was not the right fix and I agreed with you :-)

Note that without `max_rays=9` the number of rays won't get very large anyway since the `while` loop which adds rays will early abort once we reach the full space. In a 3-dimensional space it would be quite unlikely already to have even 10 rays not spanning the whole space. So I do think that my original fix was sufficient. And also a better test because there is no need to artificially limit the number of rays.


---

Comment by jdemeyer created at 2018-01-18 21:38:46

Also: use normal plain text syntax, not Sphinx syntax like `:trac:`24517`` in comments.


---

Comment by mjo created at 2018-01-18 22:12:58

Replying to [comment:22 jdemeyer]:
> Replying to [comment:17 mjo]:
> > I think we should keep your original fix, too, and specify `max_rays=9` in the test.
> 
> Why? You convinced me that it was not the right fix and I agreed with you :-)
> 
> Note that without `max_rays=9` the number of rays won't get very large anyway since the `while` loop which adds rays will early abort once we reach the full space. In a 3-dimensional space it would be quite unlikely already to have even 10 rays not spanning the whole space. So I do think that my original fix was sufficient. And also a better test because there is no need to artificially limit the number of rays.

It's just faster for the poor folks who run ptestlong. I saw about a 5 second average improvement with `max_rays=9`, mentioned at the bottom of the commit message.

The problem with leaving it unbounded is that you wind up looping...

 1. A random, large upper bound is chosen.
 2. We choose `r` between 7 and that large number; also generally large.
 3. We try to attain `r` rays, by adding one at a time...
 4. We wind up with the full space, and GOTO 1.

With `max_rays=9`, the picture is only a little different:

 1. We choose `r` between 7 and 9 (much more reasonable!)
 2. We try to attain `r` rays, by adding one at a time...
 3. We usually wind up with the full space and GOTO 1, but, more often than before we can achieve 7, 8, or 9 rays.

In the second scenario, you don't have to get lucky and choose a small random number.


---

Comment by git created at 2018-01-22 20:47:05

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by git created at 2018-01-22 20:50:12

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by mjo created at 2018-01-22 20:51:37

Trying to mention a commit hash during `rebase -i` is not working out for me...

I'll put the original test case back if you like, but I think it's making an already-long test even longer for not much benefit.


---

Comment by mjo created at 2018-01-22 20:52:01

Changing status from needs_info to needs_review.


---

Comment by chapoton created at 2018-01-27 12:33:01

looks good to me. Jeroen, do you agree ?


---

Comment by chapoton created at 2018-01-29 16:31:11

Changing status from needs_review to positive_review.


---

Comment by chapoton created at 2018-01-29 16:31:11

I am setting this to positive. If there are any objections, please shout out loud.


---

Comment by jdemeyer created at 2018-01-29 16:40:09

No objections here...


---

Comment by vbraun created at 2018-02-01 19:13:37

Resolution: fixed
