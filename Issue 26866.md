# Issue 26866: Farther improve Combinatorial Polyhedron for AVX but not AVX 2.

Issue created by migration from https://trac.sagemath.org/ticket/27103

Original creator: @kliem

Original creation time: 2019-01-23 20:11:47

CC:  selia stumpc5 hivert slelievre tscrim

Keywords: CombinatorialPolyhedron, polytopes, f-vector, edge graph, flag vector

The crucial calculation for pretty much everything in CombinatorialPolyhedron (#26887) is to check whether a face A is a (sub-)face of a different face B.

Once converted to Bits representing each vertex the question is A & ~B == 0.

This is fastly computed with processor intrinsics (_mm256_testc_si256 or _mm_testc_si128). AVX can do this with 256 bit chunks.

To compute e.g. the f-vector, one also needs the intersection of two faces or bitwise AND. AVX cannot do this with 256 bit chunks, but AVX2 can.

Currently the choice for AVX but not AVX2 is to compute with 128 bit chunks.

As the subset check is very much the bottle neck of the computations, we should split 256 bits again in two for intersection, when AVX is available but AVX2 is not. This should give a speedup of almost two on large Polyhedra.


---

Comment by @kliem created at 2019-01-25 22:57:09

Changing priority from minor to major.


---

Comment by @kliem created at 2019-01-25 22:57:09

Changing keywords from "CombinatorialPolyhedron, polytopes, f-vector, edge graph, flag vector" to "CombinatorialPolyhedron, SIMD, intrinsics".


---

Comment by @kliem created at 2019-02-20 15:45:27

This is a way, one could boost CombinatorialPolyhedron with intrinsics. Replace `is_subset`, `intersection` and `CountFaceBits` by those functions:


```
#if __AVX__
    #include <immintrin.h>
#elif __SSE4_1__
    #include <emmintrin.h>
    #include <smmintrin.h>
#endif

#if __POPCNT__
    #include <immintrin.h>
#endif


// as of now, 512bit does not have something like _mm256_testc_si256,
// which is the bottle neck of this function,
// so it does not make sense to implement it

// inline int is_subset(uint64_t *A, uint64_t *B, size_t face_length)
// the bottlen-neck is checking for subsets, which requires something as
// _mm256_testc_si256, trying to determine, what is the best way of doing it:
#if __AVX__
    // 256-bit commands, those operations are equivalent to the operations
    // defined in `#else`
    // intrics defined in immintrin.h
    const size_t chunksize = 256;
    inline int is_subset(uint64_t *A, uint64_t *B, size_t face_length){
        // A & ~B == 0
        // returns 1 if A is a subset of B, otherwise returns 0
        // this is done by checking if there is an element in A,
        // which is not in B
        // `face_length` is the length of A and B in terms of uint64_t
        // note that A,B need to be 32-byte-aligned
        size_t i;
        for (i = 0; i < face_length; i += 4){
            __m256i a = _mm256_load_si256((const __m256i*)&A[i]);
            __m256i b = _mm256_load_si256((const __m256i*)&B[i]);
            if (!_mm256_testc_si256(b, a)){ //need to be opposite order !!
                return 0;
            }
        }
        return 1;
    }

#elif __SSE4_1__
    // 128-bit commands, those operations are equivalent to the operations
    // defined in `#else`
    // intrics defined in smmintrin.h and emmintrin.h
    const size_t chunksize = 128;
    inline int is_subset(uint64_t *A, uint64_t *B, size_t face_length){
        // A & ~B == 0
        // returns 1 if A is a subset of B, otherwise returns 0
        // this is done by checking if there is an element in A,
        // which is not in B
        // `face_length` is the length of A and B in terms of uint64_t
        // note that A,B need to be 16-byte-aligned
        size_t i;
        for (i = 0; i < face_length; i += 2){
            __m128i a = _mm_load_si128((const __m128i*)&A[i]);
            __m128i b = _mm_load_si128((const __m128i*)&B[i]);
            if (!_mm_testc_si128(b, a)){ //need to be opposite order !!
                return 0;
            }
        }
        return 1;
    }

#else
    // no intrinsics
    const size_t chunksize = 64;
    inline int is_subset(uint64_t *A, uint64_t *B, size_t face_length){
        // A & ~B == 0
        // returns 1 if A is a subset of B, otherwise returns 0
        // this is done by checking if there is an element in A,
        // which is not in B
        // `face_length` is the length of A and B in terms of uint64_t
        size_t i;
        for (i = 0; i < face_length; i++){
            if (A[i] & ~B[i]){
                return 0;
            }
        }
        return 1;
    }

#endif

// inline void intersection(uint64_t *A, uint64_t *B, uint64_t *C,
//                          size_t face_length)
// now determining, how to do insersection
#if __AVX2__
    // 256-bit commands, those operations are equivalent to the operations
    // defined in `#else`
    // intrics defined in immintrin.h
    inline void intersection(uint64_t *A, uint64_t *B, uint64_t *C, \
                             size_t face_length){
        // C = A & B
        // will set C to be the intersection of A and B
        // `face_length` is the length of A, B and C in terms of uint64_t
        // note that A,B,C need to be 32-byte-aligned
        size_t i;
        for (i = 0; i < face_length; i += 4){
            __m256i a = _mm256_load_si256((const __m256i*)&A[i]);
            __m256i b = _mm256_load_si256((const __m256i*)&B[i]);
            __m256i c = _mm256_and_si256(a, b);
            _mm256_store_si256((__m256i*)&C[i],c);
        }
    }

#elif __SSE4_1__
    // actually SSE2 would be fine, but we don't want to force greater chunks,
    // because of intersection, which is not the bottleneck
    // 128-bit commands, those operations are equivalent to the operations
    // defined in `#else`
    // intrinsics defined in emmintrin.h
    inline void intersection(uint64_t *A, uint64_t *B, uint64_t *C, \
                             size_t face_length){
        // C = A & B
        // will set C to be the intersection of A and B
        // `face_length` is the length of A, B and C in terms of uint64_t
        // note that A,B,C need to be 16-byte-aligned
        size_t i;
        for (i = 0; i < face_length; i += 2){
            __m128i a = _mm_load_si128((const __m128i*)&A[i]);
            __m128i b = _mm_load_si128((const __m128i*)&B[i]);
            __m128i c = _mm_and_si128(a, b);
            _mm_store_si128((__m128i*)&C[i],c);
        }
    }

#else
    // commands, without intrinsics
    inline void intersection(uint64_t *A, uint64_t *B, uint64_t *C, \
                             size_t face_length){
        // C = A & B
        // will set C to be the intersection of A and B
        // `face_length` is the length of A, B and C in terms of uint64_t
        size_t i;
        for (i = 0; i < face_length; i++){
            C[i] = A[i] & B[i];
        }
    }

#endif

// inline size_t CountFaceBits(uint64_t* A, size_t face_length)
// determine the best way to count the set bits in uint64_t *
#if (__POPCNT__) && (INTPTR_MAX == INT64_MAX) // 64-bit and popcnt
    inline size_t CountFaceBits(uint64_t* A, size_t face_length) {
        // counts the number of vertices in a face by counting bits set to one
        // `face_length` is the length of A in terms of uint64_t
        size_t i;
        unsigned int count = 0;
        for (i=0; i<face_length; i++){
            count += (size_t) _mm_popcnt_u64(A[i]);
        }
        return count;
    }

#else // popcount without intrinsics
    inline size_t CountFaceBits(uint64_t* A, size_t face_length) {
        // counts the number of vertices in a face by counting bits set to one
        // `face_length` is the length of A in terms of uint64_t
        size_t i;
        unsigned int count = 0;
        for (i=0; i<face_length; i++){
            uint64_t a = A[i];
            while (a){
                count += a & 1;
                a >>= 1;
            }
        }
        return count;
    }

#endif
```



---

Comment by @kliem created at 2019-03-11 14:29:35

Last 10 new commits:


---

Comment by embray created at 2019-03-25 10:56:15

Ticket retargeted after milestone closed (if you don't believe this ticket is appropriate for the Sage 8.8 release please retarget manually)


---

Comment by embray created at 2019-06-14 14:54:19

As the Sage-8.8 release milestone is pending, we should delete the sage-8.8 milestone for tickets that are not actively being worked on or that still require significant work to move forward.  If you feel that this ticket should be included in the next Sage release at the soonest please set its milestone to the next release milestone (sage-8.9).


---

Comment by git created at 2019-10-23 07:47:56

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by dcoudert created at 2019-10-23 10:57:54

for popcount, you can use something like this when builtin popcount is not available:

```
cdef inline int popcount64(uint64_t i):
   """
   Return the number of '1' bits in a 64-bits integer.
   """
   i = i - ((i >> 1) & 0x5555555555555555ULL)
   i = (i & 0x3333333333333333ULL) + ((i >> 2) & 0x3333333333333333ULL)
   return ( ((i + (i >> 4)) & 0x0f0f0f0f0f0f0f0fULL) * 0x0101010101010101ULL ) >> 56
```


EDIT: removed inappropriate comment (bad copy/paste)


---

Comment by git created at 2019-10-23 11:44:15

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2019-10-23 11:45:39

Thanks. Was a nice exercise to go through it.

Replying to [comment:10 dcoudert]:
> for popcount, you can use something like this when builtin popcount is not available:
> {{{
> cdef inline int popcount64(uint64_t i):
>    """
>    Return the number of '1' bits in a 64-bits integer.
>    """
>    i = i - ((i >> 1) & 0x5555555555555555ULL)
>    i = (i & 0x3333333333333333ULL) + ((i >> 2) & 0x3333333333333333ULL)
>    return ( ((i + (i >> 4)) & 0x0f0f0f0f0f0f0f0fULL) * 0x0101010101010101ULL ) >> 56
> }}}
> 
> EDIT: removed inappropriate comment (bad copy/paste)


---

Comment by @kliem created at 2019-10-23 17:50:00

Changing status from new to needs_review.


---

Comment by dcoudert created at 2019-10-24 12:42:22

You could add an how to help reviewing this ticket. I can test it on OSX, but I assume it's far from enough.

PS: no need to add me as an author. I just gave a small trick from hakmem 169. See e.g. [http://www.dalkescientific.com/writings/diary/archive/2008/07/03/hakmem_and_other_popcounts.html](http://www.dalkescientific.com/writings/diary/archive/2008/07/03/hakmem_and_other_popcounts.html)


---

Comment by @kliem created at 2019-10-25 09:14:18

It works for me.

- Operating System: Debian GNU/Linux 10 (buster)
- Kernel: Linux 4.19.0-6-amd64
- Architecture: x86-64
- CPU Model name: Intel(R) Core(TM) i7-7700 CPU `@` 3.60GHz
- sse4.1, avx, avx2, popcnt


---

Comment by dcoudert created at 2019-10-25 16:30:40

System:
- Operating System: OS X 10.14.6
- Kernel: Darwin Kernel Version 18.7.0
- Architecture: x86-64
- CPU Model name: Intel(R) Core(TM) i7-4650U CPU `@` 1.70GHz
- sse4.1, avx2, popcnt


Timing using py3:
- 0.08155608177185059
With this ticket (`git trac checkout 27103`) and CFLAGS set:
- 0.024885177612304688


and all tests pass.


---

Comment by vdelecroix created at 2019-10-26 03:27:02

If you want reasonably fast implementation of bitsets, you should consider using `sage/data_structures/bitset.pxi`. That uses GMP which has a lot of asembly optimization. If it is not good enough, then you should rather make `bitset.pxi` faster than rewriting code for a very specialized application.


---

Comment by @kliem created at 2019-10-26 11:11:44

Works on

- Operating System: Ubuntu 18.04.3 LTS
- Kernel: Linux 4.15.0-65-generic
- Architecture: x86-64
- Model name:          Intel(R) Core(TM) i5-7200U CPU `@` 2.50GHz
- sse4_1 avx avx2 popcnt

Before:
- 0.05162477493286133
After
- 0.02220010757446289


---

Comment by @kliem created at 2019-10-26 11:52:16

Replying to [comment:19 vdelecroix]:
> If you want reasonably fast implementation of bitsets, you should consider using `sage/data_structures/bitset.pxi`. That uses GMP which has a lot of asembly optimization. If it is not good enough, then you should rather make `bitset.pxi` faster than rewriting code for a very specialized application.

Yes...

That is going to be some project, but it definitely makes sense.

One problem with bitset (to my understanding) so far is, that it is not overaligned. If it is overaligned, loads and stores are supposedly much faster (when using intrinsics).

Probably the best way to find out, is to see how the performance of my code actually changes, when I use bitsets instead of my own implementation.

Btw, if you have a specific example in mind, where calculations with bitsets are the bottle neck, I would be interested.


---

Comment by vdelecroix created at 2019-10-26 14:54:15

For examples, have a look in the graph code `sage/graphs/` (e.g. iteration over vertex covers, etc). Apparently there are stuff in `sage/combinat/` as well. More generally

```
$ git grep -l bitset
```



---

Comment by charpent created at 2019-10-26 15:56:17

Works on top of #27122, itrself on top of Python 3-based 9.0.beta2 on :

 - Operating System: Debian GNU/Linux bullseye/sid
 - Kernel: Linux 5.2.0-3-amd64
 - Architecture: x86-64
 - Nom de modèle : Intel(R) Core(TM) i7-8550U CPU `@` 1.80GHz

Before: `0.046399831771850586`

After: `0.016237735748291016`

All polyhedra-related tests pass.

HTH,


---

Comment by @kliem created at 2019-10-26 20:31:49

I'm almost certain that this is applicable to `sage/data_structures/bitset.pxi`. I ran the following test:

```
sage: def fill_bitset():
....:     for _ in range(1000000):
....:         yield randint(0,10000000000)
....:     yield 10000000000
....:     
sage: a = FrozenBitset(fill_bitset())
sage: cython('''
....: from sage.data_structures.bitset cimport bitset_t, FrozenBitset
....: def intersect(FrozenBitset a, FrozenBitset b):
....:     a.intersection(b)
....: ''')
sage: %timeit intersect(a,a)
```


With the current implementation via GMP this takes about 781 ms (best of 3).

```
mpn_and_n(r.bits, a.bits, b.bits, b.limbs)
```

I replaced this by:

```
cdef mp_size_t i
cdef mp_limb_t *rb = r.bits, *ab = a.bits, *bb = b.bits
for i from 0 <= i < a.limbs:
    rb[i] = ab[i] & bb[i]
```

and it seems to be about the same.

Of course this is a very brief naive test, but there seems to be no reason to use GMP for this specific function. Notice, that this doesn't even use intrinsics.


---

Comment by vdelecroix created at 2019-10-26 21:19:24

If you can make `data_structures/bitset.pxi` better this is wonderful. What I want to avoid is two different piece of code that achieve the same thing. Please put your code in `data_structures/` rather than in `polyhedron` and make it as generic as possible. That way it will benefit to others.

Also, note that not all functions in `bitset.pxi` use GMP/MPIR. Where it does it should be fast. That is comparison (`mpn_cmp`), intersection (`mpn_and_n`), union (`mpn_ior_n`), etc. You might want to have a look at how these are implemented [GMP](https://gmplib.org/)/[MPIR](http://mpir.org/).


---

Comment by @kliem created at 2019-10-28 10:17:01

Things are complicated and a lot more complicated than I thought.

- To my knowledge, I cannot do something as `#if __AVX__` in cython, can I? This would mean, we would have to rewrite bitset in C or C++ to use intrinsics.
- So far I'm only reimplementing very basic functions.
- I cannot write simple tests, where intrinsics are much faster. For intersection I don't see an improvement, for subset checks I'm maybe 10% faster. Here is, why I think this might be the case:

  Loading registers takes a lot of time. In many cases this might be the bottle neck. Sure intrinsics need only one processor step instead of four to do bitwise `AND` of two registers. However, when I'm mostly waiting on read/write this does not help. Without intrinsics the processor can still process registers as fast as they can be loaded and stored again (don't forget that the result blocks another register while it is being stored).

  Now in my scenario with subset checks things are a bit different. Nothings needs to be stored after the computation (this would explain why subset check with intrinsics is 10% faster than without). But more importantly seems to be the exact scenario I'm in:

  I'm checking whether `A` is a subset of any of `B_1,...,B_n`. In most cases only the first 256 vertices are needed to discover that `A` is not a subset of `B_i`. So the first register of `A` can be reused a lot of times.

  So, intrinsics might be useful in other scenarios for bitsets in sage, but it's definitly not a no-brainer. One won't lose. That I'm pretty sure of.


---

Comment by git created at 2019-11-04 15:09:43

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2020-04-27 07:14:04

I will probably move those improvements to `data_structures/bitset.pxi`, once #27122 is done.

In the mean time, one can of course still pull the attached branch, if one wants to improve the f-vector computation.


---

Comment by @kliem created at 2020-04-27 07:14:04

Changing status from needs_review to needs_info.


---

Comment by @kliem created at 2020-08-25 08:27:27

New commits:


---

Comment by @kliem created at 2020-08-25 12:42:49

Last 10 new commits:


---

Comment by git created at 2020-08-28 14:35:59

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2020-08-30 06:25:19

Changes in #30040.
----
Last 10 new commits:


---

Comment by git created at 2020-09-02 06:39:12

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2021-01-15 18:06:52

Changing keywords from "CombinatorialPolyhedron, SIMD, intrinsics" to "bitsets, CombinatorialPolyhedron, SIMD, intrinsics".


---

Comment by @kliem created at 2021-01-15 18:06:52

I will update the description yet, but this is basically ready for review.

I got three failing doctests, but I don't think they are related:


```
sage -t --long --random-seed=0 /srv/public/kliem/sage/src/sage/doctest/sources.py  # 1 doctest failed                                                                              
sage -t --long --random-seed=0 /srv/public/kliem/sage/src/sage/doctest/forker.py  # 1 doctest failed                                                                               
sage -t --long --random-seed=0 /srv/public/kliem/sage/src/sage/interfaces/singular.py  # Killed due to segmentation fault  
```


Otherwise on my machine everything passes (with AVX2).

(in the above two each time `FDS.in_lib` fails, so that doesn't seem to be related, also the singular timout doesn't seem to be related).
----
New commits:


---

Comment by @kliem created at 2021-01-15 18:06:52

Changing status from needs_info to needs_review.


---

Comment by git created at 2021-01-15 18:49:00

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2021-01-15 18:49:18

Replying to [comment:39 git]:
> Branch pushed to git repo; I updated commit sha1. New commits:
> ||[db11a45](https://git.sagemath.org/sage.git/commit/?id=db11a450cfb5d88d7c6e8c8e0837bfb0e506f09a)||`enable intrinsics for bitsets`||

Forgot to commit.


---

Comment by git created at 2021-01-15 18:55:35

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2021-01-15 21:19:10

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by tscrim created at 2021-01-17 02:13:38

Also works for me with

- Operating System: Ubuntu 18.04.5 LTS
- Kernel: Linux 4.15.0-112-generic
- Architecture: x86-64
- Model: Intel(R) Core(TM) i7-8700 CPU `@` 3.20GHz
- sse4_1, sse4_2, avx, avx2 in cpu flags

Based on this, the patchbots, and the previous comments, I am setting this to a positive review.


---

Comment by tscrim created at 2021-01-17 02:13:38

Changing status from needs_review to positive_review.


---

Comment by @kliem created at 2021-01-17 08:36:13

Thank you.


---

Comment by vbraun created at 2021-01-31 20:53:48

Resolution: fixed
