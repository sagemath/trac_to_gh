# Issue 34399: make sparsity a decision of the user

Issue created by migration from https://trac.sagemath.org/ticket/34636

Original creator: mantepse

Original creation time: 2022-10-07 16:24:34

CC:  tscrim chapoton




---

Comment by mantepse created at 2022-10-07 16:35:47

Changing component from PLEASE CHANGE to combinatorics.


---

Comment by mantepse created at 2022-10-07 16:35:47

Changing keywords from "" to "LazyPowerSeries".


---

Comment by mantepse created at 2022-10-07 16:35:47

Changing type from PLEASE CHANGE to enhancement.


---

Comment by mantepse created at 2022-10-07 16:35:47

Last 10 new commits:


---

Comment by mantepse created at 2022-10-11 08:54:44

A sparse representation does not make sense also for `Cauchy_invert`: `get_coefficient` accesses all previously computed values.


---

Comment by git created at 2022-10-11 10:11:37

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2022-10-11 13:57:09

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mantepse created at 2022-10-11 13:59:35

Changing status from new to needs_review.


---

Comment by mantepse created at 2022-10-12 06:51:23

From #34611: it may make sense to make `Stream_uninitialized` always dense, and doctest that the following then works:

```
sage: L.<z> = LazyPowerSeriesRing(ZZ); C = L.undefined(); C.define(1 + z*C^2)
sage: C[500]
```


Another question: it may make sense to make `Stream_XXX` always sparse, if the computation of `s[n]` _obviously_ does not benefit from the values of `s[n-1], s[n-2], ...`.  This is the case, for example, with `Stream_add`.

The benefit of a dense representation of `Stream_add` may be in storage and access: if we have a sparse representation and have to compute most values, the list representation uses (much) less memory, and access will be faster. For example, storing the first 600 values of a stream of integers as a list uses no more memory than storing 200 values in a dict.  Lookup in a list of about 1000 elements seems to take about 60%-70% of the time lookup in a dictionary takes.

In cases when we apply a very simple function to each coefficient, as in `Stream_neg`, we may not want to create a separate cache at all.  For `Stream_map_coefficients`, we currently only apply very costly functions.


---

Comment by tscrim created at 2022-10-12 08:53:13

I am wondering how much we even want the sparsity in the streams. We want it for the exact series in some cases (sparse versus dense for the (Laurent) polynomial ring), but it seems more like extra weight we are carrying around for the streams, which should instead simply be as dense or sparse as is most effective for them. Or maybe just for multiplication, which might be the only one that could matter? Do you remember if in any place we take advantage of the dense or sparse implementation currently?


---

Comment by mantepse created at 2022-10-12 09:29:39

I don't think we can decide for the user.  For multiplication it makes a big difference, whether you are only interested in `(f*g)[10000]` or in `(f*g)[:10000]`.  The former may be relevant, if you want to compute a few random coefficients, e.g., to quickly see how much they grow.  But I admit that I do not know any serious application.

Exact series are neither sparse nor dense in the sense of `Stream`, they implement their own (mixed) cache: an index and a list for the initial coefficients, and an index and a constant for the rest.

I am not completely sure whether I understand your question.  I think that `Stream_mul`, `Stream_cauchy_compose` and the like should have sparse and dense versions. `Stream_uninitialized` probably only a dense version, `Stream_add`, `Stream_sub`, etc. possibly only a sparse version.

The only two reasons for `Stream_add` etc. to have a dense version is, if the speedup in access and the memory overhead matter.  There is no additional code.  So I guess it is better to keep the option.  We currently only use subtraction, implicitly in `LazySymmetricFunction.revert`:

```
        g = P.undefined(valuation=1)
        g.define(b_inv * (X - (self - b * X)(g)))
```

I am guessing (but I am not sure) that it makes sense to use either only the sparse versions or only the dense versions of the operations here.


---

Comment by tscrim created at 2022-10-12 14:04:59

You’re right; for multiplication, we should have both a sparse and dense version.

Ah, right, we don’t store the polynomials themselves in the `Stream_exact`, although there can be uses for passing the sparsity to that. I don’t remember if we implement both types of exact storage. Perhaps we should, in case someone wants to create x<sup>10000</sup>.

I am mostly wondering how much we should drop within the streams to carry around the `_sparse` parameter altogether. The multiplication will likely be split into 2 classes (with not being too lazy with the dense multiplication); the rest, except possibly the exact, are all naturally either dense (e.g., inverse) or sparse (e.g., addition).

I will have to look at the code we currently have more closely I think to get a more clear picture. I don’t remember some of the details regarding this. Perhaps you already have a clear picture about this though.


---

Comment by mantepse created at 2022-10-12 14:36:44

Creating z<sup>10000</sup> is not a problem:

```
sage: L.<z> = LazyPowerSeriesRing(ZZ)
sage: f = z^100000
sage: f._coeff_stream.__dict__
{'_constant': 0,
 '_degree': 100001,
 '_initial_coefficients': (1,),
 '_true_order': True,
 '_approximate_order': 100000}
```

however, z<sup>100000</sup>-1 is:

```
sage: f = z^10 - 1
sage: f._coeff_stream.__dict__
{'_constant': 0,
 '_degree': 11,
 '_initial_coefficients': (-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1),
 '_true_order': True,
 '_approximate_order': 0}
```


Curiously, it takes a very long time to create, all of which is spent in the polynomial exponentiation - this is, because the underlying polynomial ring, for some reason is always dense:

```python
class LazyLaurentSeriesRing(LazySeriesRing):
    def __init__(self, base_ring, names, sparse=True, category=None):
        self._sparse = sparse
        # We always use the dense because our CS_exact is implemented densely
        self._laurent_poly_ring = LaurentPolynomialRing(base_ring, names)
        self._internal_poly_ring = self._laurent_poly_ring
```


There are actually two easy improvements:

* a sparse version, so we can create 1 + x<sup>100000000</sup>
* I think that the polynomial ring should be dense or sparse depending on our input

and, orthogonal to this,

* a lazy version of `Stream_exact`, with coefficients provided by a function, might also help with the problem of computing large powers or large compositions of exact series.

I can imagine that having only sparse addition is bad.  I can try to create an example if you want.

What I'd be interested in is some help with van der Hoeven's algorithm in #34616.


---

Comment by git created at 2022-10-12 14:43:08

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2022-10-13 06:50:51

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mantepse created at 2022-10-13 07:04:02

I made the internal polynomial rings sparse or dense according to the lazy series ring, although this really is of very little consequence.

Essentially, creating z<sup>100000</sup> is now fast in the sparse case, and some computations for exact series (powers, compositions) are now carried out with sparse polynomials.  I did not check, however, whether this makes sense.  We actually do not use the polynomial rings so much, not even for multiplication.

A completely sparse version of `Stream_exact` should be easy, I think - in the non-lazy setting we would simply have a dictionary of non-zero coefficients.


---

Comment by mantepse created at 2022-10-13 07:08:07

Although having a sparse version of `Stream_exact` should be easy, taking advantage of it may be much more work, because currently we always compute a _list_ of initial coefficients in `lazy_ring.py`.

I very much doubt that it is worth the effort right now.


---

Comment by tscrim created at 2022-10-14 01:38:11

The dense Laurent polynomials might be a bit better at creating z<sup>10000</sup> than the usual polynomials as they factor out the valuation portion.

It might not be so prudent to spend time implementing a sparse version of `Stream_exact` right now actually. There will be some additional changes needed throughout the code (this might indicate that we need to refactor so the stream can handle more things in its construction method). There are plenty of other things that we can do to improve the speed and efficiency that are likely to be more useful to people.


---

Comment by tscrim created at 2022-10-28 07:13:52

Let us move along. This is definitely an improvement, and we can do further work on subsequent tickets.


---

Comment by tscrim created at 2022-10-28 07:13:52

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2022-11-07 20:26:18

Resolution: fixed
