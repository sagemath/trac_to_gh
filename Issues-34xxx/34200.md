# Issue 34200: Implement custom matrix-type row reduction scheme for exterior algebra Gröbner basis

archive/issues_034200.json:
```json
{
    "body": "CC:  @trevorkarn\n\nKeywords: Gr\u00f6bner basis, exterior algebra\n\nThis is the main bottleneck in the the Gr\u00f6bner basis code implemented in #34138. Sage's implementation is very inefficient (at least over `QQ`) as it creates a dense copy (when sparse, which the implementation in #34138 uses) and then does the row reduction on that. Doing the row reduction on that matrix also produces a copy that is then entry-wise copied back into the original matrix!\n\nWe implement a custom version of row reduction tailored to the GB computation. We also become very careful about our data structure:\n\nWe realize the matrix as a dictionary of the leading supports whose elements are lists of elements of the exterior algebra with that leading support. This way we do not need to create a lot of transient elements. It also makes swapping rows much faster and makes it clear which rows should do the reduction. This effectively \"triangularizes\" the matrix too.\n\nIssue created by migration from https://trac.sagemath.org/ticket/34437\n\n",
    "created_at": "2022-08-26T02:03:46Z",
    "labels": [
        "component: linear algebra"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.8",
    "title": "Implement custom matrix-type row reduction scheme for exterior algebra Gr\u00f6bner basis",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/34200",
    "user": "https://github.com/tscrim"
}
```
CC:  @trevorkarn

Keywords: Gröbner basis, exterior algebra

This is the main bottleneck in the the Gröbner basis code implemented in #34138. Sage's implementation is very inefficient (at least over `QQ`) as it creates a dense copy (when sparse, which the implementation in #34138 uses) and then does the row reduction on that. Doing the row reduction on that matrix also produces a copy that is then entry-wise copied back into the original matrix!

We implement a custom version of row reduction tailored to the GB computation. We also become very careful about our data structure:

We realize the matrix as a dictionary of the leading supports whose elements are lists of elements of the exterior algebra with that leading support. This way we do not need to create a lot of transient elements. It also makes swapping rows much faster and makes it clear which rows should do the reduction. This effectively "triangularizes" the matrix too.

Issue created by migration from https://trac.sagemath.org/ticket/34437





---

archive/issue_comments_484669.json:
```json
{
    "body": "Here is my first attempt, which does seem to improve the speed that it does the linear algebra. There probably are much smarter things that could be done here, but even a na\u00efve version over QQ helps quite a bit, at least for the larger case from the ~7s in M2 test noted in #34138. At least I am getting to the 105 x 256 matrix much faster, but it still is taking ages to do the computation.\n\nI suspect you are correct that a much smarter selection algorithm is the key to getting the speed close to M2 speed.\n\n---\nLast 10 new commits:",
    "created_at": "2022-08-26T10:44:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/34200",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/34200#issuecomment-484669",
    "user": "https://github.com/tscrim"
}
```

Here is my first attempt, which does seem to improve the speed that it does the linear algebra. There probably are much smarter things that could be done here, but even a naïve version over QQ helps quite a bit, at least for the larger case from the ~7s in M2 test noted in #34138. At least I am getting to the 105 x 256 matrix much faster, but it still is taking ages to do the computation.

I suspect you are correct that a much smarter selection algorithm is the key to getting the speed close to M2 speed.

---
Last 10 new commits:



---

archive/issue_comments_484670.json:
```json
{
    "body": "F4 algorithm Peifer note",
    "created_at": "2022-08-31T02:21:10Z",
    "issue": "https://github.com/sagemath/sagetest/issues/34200",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/34200#issuecomment-484670",
    "user": "https://github.com/tscrim"
}
```

F4 algorithm Peifer note



---

archive/issue_comments_484671.json:
```json
{
    "body": "Attachment [math6140-2017a.pdf](tarball://root/attachments/some-uuid/ticket34437/math6140-2017a.pdf) by @tscrim created at 2022-08-31 02:26:42\n\nFrom Sectino 2.2.3 in the attached Peifer note, it seems like there is a much smarter algorithm that can be done. We can also reduce the computation to something involving more dense-like matrices too. So we have to pay some element conversions, but this seems small in comparison to the actual linear algebra involved.",
    "created_at": "2022-08-31T02:26:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/34200",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/34200#issuecomment-484671",
    "user": "https://github.com/tscrim"
}
```

Attachment [math6140-2017a.pdf](tarball://root/attachments/some-uuid/ticket34437/math6140-2017a.pdf) by @tscrim created at 2022-08-31 02:26:42

From Sectino 2.2.3 in the attached Peifer note, it seems like there is a much smarter algorithm that can be done. We can also reduce the computation to something involving more dense-like matrices too. So we have to pay some element conversions, but this seems small in comparison to the actual linear algebra involved.



---

archive/issue_events_089521.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-09-19T18:58:47Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/34200",
    "milestone": "sage-9.8",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/34200#event-89521"
}
```
