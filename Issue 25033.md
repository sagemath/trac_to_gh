# Issue 25033: Add option to only run --short doctests

Issue created by migration from https://trac.sagemath.org/ticket/25270

Original creator: saraedum

Original creation time: 2018-04-30 23:49:09

CC:  embray nthiery roed

The attached picture shows how many modules (horizontal) you can run in how many CPU minutes (vertical). So, for example, in 40 CPU minutes (i.e., 10 wall minutes on my laptop) I can run all the doctests in 80% of all Sage modules. In 4 CPU minutes, I can doctest 50% of all Sage modules.

It would be nice to add a `--limit=40:00` switch (or similar) to run doctests in each module for at most the corresponding fraction of time (I still have to try whether it's better to do the accounting by module or by line of doctest.)

In any case, this would give us a quick way to check that a change did not break things too terribly. For example, when I work on p-adics, I would probably do: `sage -tp --long --limit=4:00 src/sage/rings/padics` frequently. Once I am happy with my changes, I could do `sage -tp --limit=4:00 src/sage/rings` and a final `sage -tp --limit=40:00 src` before I leave the rest to the patchbot.


---

Attachment


---

Comment by saraedum created at 2018-05-01 01:58:50

I have a first prototype now. If I set it to try to finish in 30s walltime and run on `src/sage/rings/padics`, it does skip tests in only 4 files (here reported as failures at the moment):

```
sage -t ../rings/padics/padic_base_leaves.py  # 125 doctests failed
sage -t ../rings/padics/padic_lattice_element.py  # 303 doctests failed
sage -t ../rings/padics/padic_extension_leaves.py  # 72 doctests failed
sage -t ../rings/padics/padic_generic_element.pyx  # 172 doctests failed
----------------------------------------------------------------------
Total time for all tests: 35.8 seconds
```


As an extreme example, if let it test everything in `src/` in two minutes, it manages to finish within four minutes (because it does not actually abort any doctest line once it started) and runs 314607 docstring lines and skips 361692, so it skips roughly 50%.


---

Comment by saraedum created at 2018-06-09 21:22:50

New commits:


---

Comment by saraedum created at 2018-06-09 21:27:51

I still need some doctests for this but if somebody already wants to comment on this, I would be very happy about some feedback.

Somehow my counting is off quite a bit. I am not entirely sure what's the problem.


---

Comment by saraedum created at 2018-06-10 20:07:47

Changing status from new to needs_review.


---

Comment by saraedum created at 2018-06-10 20:09:37

I am quite unhappy with the spaghetti code that I added here but I don't really see a much better way. I have a feeling that we cramped a lot of features already into Python's doctesting framework and that it was never meant to be extended like that. A proper plugin system would have been cool but I guess it's too late for that ;)


---

Comment by git created at 2018-06-11 06:23:52

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by embray created at 2018-06-27 14:32:34

Needs to be rebased it looks like.  I can't wait to see this working though, and if I have some ideas how to improve the implementation I'll give them.


---

Comment by git created at 2018-07-03 15:54:47

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-07-03 15:55:14

Ok. I rebased this.


---

Comment by embray created at 2018-07-10 13:57:33

I wish we also had a way to prioritize tests.  E.g. we could mark a test as `# important` and ensure all such tests get run first.  That could be added later though.


---

Comment by embray created at 2018-07-10 13:57:33

Changing status from needs_review to positive_review.


---

Comment by git created at 2018-07-11 05:00:12

Branch pushed to git repo; I updated commit sha1 and set ticket back to needs_review. New commits:


---

Comment by git created at 2018-07-11 05:00:12

Changing status from positive_review to needs_review.


---

Comment by saraedum created at 2018-07-11 05:10:38

Rebased again. Somehow the patchbots fail to build this, probably they are broken atm.


---

Comment by embray created at 2018-07-11 11:09:02

My patchbot Ubuntu/18.04/x86_64/3.13.0-123-generic/sagemath-patchbot-docker was broken for a few days due to running out of disk space.  It should be working now though.


---

Comment by @timokau created at 2018-07-11 11:55:48

This will only work if the full testsuite is run at least once and allowed to save state right?

I'd love something like this for distribution CI (testing weather or not a dependency update breaks sage), but if thats the case it couldn't be used for that. Something like `#important` would be good in that case.


---

Comment by saraedum created at 2018-07-11 12:42:08

Replying to [comment:21 gh-timokau]:
> This will only work if the full testsuite is run at least once and allowed to save state right?
No. The changes proposed here do not rely on saved state.


---

Comment by @timokau created at 2018-07-11 13:22:05

Thats great! The code is a bit hard to read without context and I don't want to dig too deeply. How does it determine which tests to skip then? Just allocate equal time to each file and abort remaining tests if the time budged is used up?


---

Comment by saraedum created at 2018-07-11 13:33:08

Yes. It allocates equal time and starts testing from the top.


---

Comment by jhpalmieri created at 2018-07-11 15:15:30

This also needs to be documented in `src/doc/en/reference/repl/options.rst`.


---

Comment by jhpalmieri created at 2018-07-11 15:16:36

Does the `[SECONDS]` argument refer to the total time for testing, or the time for each file? When I ran `sage --tp --short 15 src/sage`, it spent much less than 15 seconds on each file before giving up.


---

Comment by saraedum created at 2018-07-11 15:18:57

The [SECONDS] are the total amount of time.


---

Comment by saraedum created at 2018-07-11 15:18:57

Changing status from needs_review to needs_work.


---

Comment by jhpalmieri created at 2018-07-11 15:52:31

I am getting a doctest failure from `sage -tp --short 600 src/sage/`:

```
sage -t src/sage/doctest/reporting.py
**********************************************************************
File "src/sage/doctest/reporting.py", line 324, in sage.doctest.reporting.DocTestReporter.report
Failed example:
    DTR.report(FDS, False, 0, (sum([len(t.examples) for t in doctests]), D), "Good tests")
Expected:
        1 unlabeled test not run
        4 long tests not run
        5 magma tests not run
        2 other tests skipped
        [... tests, ... s]
Got:
        1 unlabeled test not run
        4 long tests not run
        5 magma tests not run
        2 not tested tests not run
        0 tests not run because we ran out of time
        [126 tests, 0.00 s]
**********************************************************************
File "src/sage/doctest/reporting.py", line 350, in sage.doctest.reporting.DocTestReporter.report
Failed example:
    DTR.report(FDS, False, 0, (sum([len(t.examples) for t in doctests]), D), "Good tests")
Expected nothing
Got:
        [126 tests, 0.00 s]
**********************************************************************
1 item had failures:
   2 of  57 in sage.doctest.reporting.DocTestReporter.report
    [120 tests, 2 failures, 0.54 s]
```



---

Comment by git created at 2018-07-12 12:36:06

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-07-12 12:36:14

Changing status from needs_work to needs_review.


---

Comment by saraedum created at 2018-07-12 12:36:14

New commits:


---

Comment by embray created at 2018-07-12 12:53:04

Let's see what the patchbot says (if indeed I've actually managed to fix it...)


---

Comment by embray created at 2018-07-12 18:20:55

Got a few test failures in `sage.doctest` that seem relevant :)


---

Comment by embray created at 2018-07-12 18:21:02

Changing status from needs_review to needs_work.


---

Comment by git created at 2018-07-20 07:25:52

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2018-07-20 07:29:36

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-07-20 07:30:04

New commits:


---

Comment by saraedum created at 2018-07-20 07:30:04

Changing status from needs_work to needs_review.


---

Comment by embray created at 2018-07-23 16:41:47

Still getting a failure:


```
sage -t --long src/sage/doctest/reporting.py
**********************************************************************
File "src/sage/doctest/reporting.py", line 351, in sage.doctest.reporting.DocTestReporter.report
Failed example:
    DTR.report(FDS, False, 0, (sum([len(t.examples) for t in doctests]), D), "Good tests")
Expected nothing
Got:
        [126 tests, 0.00 s]
**********************************************************************
1 item had failures:
   1 of  57 in sage.doctest.reporting.DocTestReporter.report
    [120 tests, 1 failure, 0.52 s]
----------------------------------------------------------------------
sage -t --long src/sage/doctest/reporting.py  # 1 doctest failed
----------------------------------------------------------------------
```


I've seen some cases before where the expected output was accidentally left off a doctest, and the doctest runner just ignored that, seemingly buggily, whereas fixing/changing things in the doctest runner made it start expecting output again.  Could this be something like that?


---

Comment by embray created at 2018-07-23 16:42:30

Maybe?


---

Comment by embray created at 2018-07-23 16:42:30

Changing status from needs_review to needs_work.


---

Comment by embray created at 2018-07-23 16:43:15

If my fast build machine weren't down I'd test to see if I could reproduce this manually.


---

Comment by embray created at 2018-07-24 08:37:57

Yeah, I was able to reproduce the test failure locally just running `./sage -t src/sage/doctest/reporting.py`.


---

Comment by embray created at 2018-07-24 08:40:26

I think it's a legitimate regression, unlike my earlier guess.  The failure is here:


```
        The only-errors mode does not output anything on success::

            sage: DD = DocTestDefaults(only_errors=True)
            sage: FDS = FileDocTestSource(filename, DD)
            sage: DC = DocTestController(DD, [filename])
            sage: DTR = DocTestReporter(DC)
            sage: doctests, extras = FDS.create_doctests(globals())
            sage: runner = SageDocTestRunner(SageOutputChecker(), verbose=False, sage_options=DD, optionflags=doctest.NORMALIZE_WHITESPACE|doctest.ELLIPSIS)
            sage: Timer().start().stop().annotate(runner)
            sage: D = DictAsObject({'err':None})
            sage: runner.update_results(D)
            0
            sage: DTR.report(FDS, False, 0, (sum([len(t.examples) for t in doctests]), D), "Good tests")

        However, failures are still output in the errors-only mode::

            sage: runner.failures = 1
            sage: runner.update_results(D)
            1
            sage: DTR.report(FDS, False, 0, (sum([len(t.examples) for t in doctests]), D), "Failed test")
                [... tests, 1 failure, ... s]

```


The intent of the test is that nothing should be output by `DocTestRunner.report` if it's in "only-errors mode" (a thing I didn't know existed), and there are no failures.


---

Comment by git created at 2018-07-28 13:08:28

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-07-28 13:09:13

Ok. I had somehow removed the line about the `only_errors` flag. I put it back in.


---

Comment by saraedum created at 2018-07-28 13:09:13

Changing status from needs_work to needs_review.


---

Comment by embray created at 2018-07-30 16:45:27

Ok, positive review from me then pending results from a patchbot or two.

Mine were down all weekend so they'll probably need a day or 2 to get to it.


---

Comment by git created at 2018-08-07 16:37:24

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2018-08-16 22:32:12

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-08-20 20:14:45

Changing status from needs_review to positive_review.


---

Comment by saraedum created at 2018-08-20 20:14:45

Finally, there was a happy patchbot based off 8.4.beta1 :)


---

Comment by saraedum created at 2018-08-20 20:15:55

I am not sure that's the right approach here when looking at patchbot outputs but the patchbots are so flaky lately, that I just assume that "build fails" has nothing to do with this ticket.


---

Comment by git created at 2018-08-20 20:18:03

Branch pushed to git repo; I updated commit sha1 and set ticket back to needs_review. New commits:


---

Comment by git created at 2018-08-20 20:18:03

Changing status from positive_review to needs_review.


---

Comment by git created at 2018-08-20 20:19:14

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by saraedum created at 2018-08-20 20:19:54

Sorry for this noise. I had pushed something to the wrong ticket. Reverted back to the reviewed state.


---

Comment by saraedum created at 2018-08-20 20:19:54

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2018-08-26 09:40:24

Changing status from positive_review to needs_work.


---

Comment by vbraun created at 2018-08-26 09:40:24

Merge conflict


---

Comment by saraedum created at 2018-08-26 11:45:55

Changing status from needs_work to needs_review.


---

Comment by git created at 2018-08-26 11:46:42

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-09-02 12:42:42

Changing status from needs_review to needs_work.


---

Comment by saraedum created at 2018-09-02 12:44:25

embray: I sometimes get

```
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/sage/sage/local/lib/python2.7/atexit.py", line 24, in _run_exitfuncs
    func(*targs, **kargs)
  File "<doctest sage.cpython.atexit[2]>", line 2, in handler
NameError: global name 'print' is not defined
sage -t sage/src/sage/cpython/atexit.pyx
    NameError in doctesting framework
**********************************************************************
Tests run before doctest exception:
sage: import atexit ## line 54 ##
sage: from sage.cpython.atexit import restore_atexit ## line 55 ##
sage: def handler(*args, **kwargs):
    print((args, kwargs)) ## line 56 ##
sage: atexit.register(handler, 1, 2, c=3) ## line 58 ##
<function handler at 0x7f3ee18f41b8>

**********************************************************************
Traceback (most recent call last):
  File "/home/sage/sage/local/lib/python2.7/site-packages/sage/doctest/forker.py", line 2426, in __call__
    break
  File "sage/cpython/atexit.pyx", line 143, in sage.cpython.atexit.restore_atexit.__exit__ (build/cythonized/sage/cpython/atexit.c:1497)
    atexit._run_exitfuncs()
  File "/home/sage/sage/local/lib/python2.7/atexit.py", line 24, in _run_exitfuncs
    func(*targs, **kargs)
  File "<doctest sage.cpython.atexit[2]>", line 2, in handler
NameError: global name 'print' is not defined
```


I guess this happens when the tests in `atexit` are only run partially and maybe some cleanup that is part of the doctests did not happen.

Since you're listed as the author of that file, any clue what's going on here?


---

Comment by embray created at 2018-09-03 08:50:00

Hmm--I think maybe, if nothing else, there should be a `from __future__ import print_function` in that module, which there currently is not.  I'm not positive that that's the problem though.  I'd have to try to reproduce it first.


---

Comment by embray created at 2018-09-03 08:52:59

Strange, though, since looking at `sage.doctest.forker` all doctests are supposed to be compiled as though `from __future__ import print_function` was present.  Maybe, somehow, just including the required compile flags isn't enough to ensure that the `print()` function is in the globals?


---

Comment by embray created at 2018-09-05 15:23:26

For the atexit tests, does that happen if you test `src/sage/cpython/atexit.pyx` directly?  Or does it only happen when running the full test suite with `./sage --short`?


---

Comment by git created at 2018-10-08 11:30:29

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-10-08 11:30:41

embray, I don't think we should put too much effort into fixing this rather obscure problem as it is going to go away with Python 3 anyway. I added a simple workaround (untested.) Would you be fine with something like this?
----
New commits:


---

Comment by git created at 2018-10-08 11:31:07

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by saraedum created at 2018-10-08 11:31:35

Changing status from needs_work to needs_review.


---

Comment by embray created at 2018-10-08 11:46:23

Yes, I think this is entirely reasonable.  Relying on module-level globals that may already be set to `None` during interpreter shutdown can always be a problem.  Here instead you were getting a `NameError` which is a bit more strange to me, but almost certainly has some strange interplay with how `from __future__ import print_function` works.

Positive review from me pending patchbot results, but I think it will be fine...


---

Comment by embray created at 2018-10-08 11:46:23

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2018-10-20 19:02:31

Resolution: fixed


---

Comment by embray created at 2018-10-28 14:43:14

I assume?
