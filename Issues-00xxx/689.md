# Issue 689: modifying preparse.py to benefit from the Python standard tokenize module

archive/issues_000689.json:
```json
{
    "body": "\n```\nOn 9/18/07, Joel B. Mohler <joel@kiwistrawberry.us> wrote:\n\n    I don't have a strong opinion about the original proposition ... although I\n    have spent the last week being thoroughly vexed by python being zero-based\n    when all the things I would write on paper about math would index things\n    one-based.\n\n\nOne solution to that is to just write things 0-based in your math papers. \nI've been doing that in all the mathematics I write for the last 2 years\nor so, and nobody complains.   Consider an $n$-dimensional vector\nspace with basis $v_0, \\ldots, v_{n-1}$. \n\n    However, I did want to say this about the preparser.  My impression is that it\n    is written by doing very manual character/string reading.  I think\n    the 'tokenize' module could probably make the code much much cleaner.  It has\n    annoying (but understandable) semantics.  Here's an example below.\n\n\nTrue.  I've been looking forward to somebody rewriting the preparse(...)\nfunction for a long time.  Much to my surprise nobody has.  I don't think\nanybody has even tried, though people have suggested they would try\non a few occasions. I would certainly be happy if it were to happen, though\nI'm not going to do it myself, since the current version works perfectly\nwell, is easy to understand, and I have other things that I have to do.\nAlso, speeding up the preparser might have little noticeable effect on the\nspeed of Sage, since all actual work / computation happens after preparsing.\nThat said, if you do\n    sage: search_src('sage_eval')\nyou'll see 83 places in the Sage library code itself that involve calling\nthe Sage preparser, and those could be faster. \n\nUsing tokenize as you illustrate below would probably be a simple\nintermediate way to rewrite the preparser without going nuts trying\nto do something too complicated.  E.g., the preparser currently\nhas code to find string literals in the input line -- the tokenize module\ndoes the same thing, probably much better.   Also the preparser\nhas code to parse out valid identifiers, and again the tokenize module\nbelow does that automatically.\n\nI think \"modifying preparse.py to benefit from the Python standard tokenize module\" would be a good trac enhancement ticket:\n  \n\n    --\n    Joel\n\n    import tokenize\n\n    # helper class to generate a stream from a string\n    class line_token_stream:\n        def __init__( self, s ):\n            self.line = s\n\n        def __call__( self ):\n            if self.line:\n                s = self.line\n                self.line = None\n                return s\n            raise StopIteration\n\n    def tokenize_line( s ):\n        for t in  tokenize.generate_tokens( line_token_stream( s ) ):\n            yield t\n\n    for i in tokenize_line( \"func(1+2)\" ):\n        print i\n\n```\n\nAssignee: somebody\n\nStatus: new\n\nIssue created by migration from https://trac.sagemath.org/ticket/689\n\n",
    "created_at": "2007-09-18T15:51:23Z",
    "labels": [
        "component: basic arithmetic"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-wishlist",
    "title": "modifying preparse.py to benefit from the Python standard tokenize module",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/689",
    "user": "https://github.com/williamstein"
}
```

```
On 9/18/07, Joel B. Mohler <joel@kiwistrawberry.us> wrote:

    I don't have a strong opinion about the original proposition ... although I
    have spent the last week being thoroughly vexed by python being zero-based
    when all the things I would write on paper about math would index things
    one-based.


One solution to that is to just write things 0-based in your math papers. 
I've been doing that in all the mathematics I write for the last 2 years
or so, and nobody complains.   Consider an $n$-dimensional vector
space with basis $v_0, \ldots, v_{n-1}$. 

    However, I did want to say this about the preparser.  My impression is that it
    is written by doing very manual character/string reading.  I think
    the 'tokenize' module could probably make the code much much cleaner.  It has
    annoying (but understandable) semantics.  Here's an example below.


True.  I've been looking forward to somebody rewriting the preparse(...)
function for a long time.  Much to my surprise nobody has.  I don't think
anybody has even tried, though people have suggested they would try
on a few occasions. I would certainly be happy if it were to happen, though
I'm not going to do it myself, since the current version works perfectly
well, is easy to understand, and I have other things that I have to do.
Also, speeding up the preparser might have little noticeable effect on the
speed of Sage, since all actual work / computation happens after preparsing.
That said, if you do
    sage: search_src('sage_eval')
you'll see 83 places in the Sage library code itself that involve calling
the Sage preparser, and those could be faster. 

Using tokenize as you illustrate below would probably be a simple
intermediate way to rewrite the preparser without going nuts trying
to do something too complicated.  E.g., the preparser currently
has code to find string literals in the input line -- the tokenize module
does the same thing, probably much better.   Also the preparser
has code to parse out valid identifiers, and again the tokenize module
below does that automatically.

I think "modifying preparse.py to benefit from the Python standard tokenize module" would be a good trac enhancement ticket:
  

    --
    Joel

    import tokenize

    # helper class to generate a stream from a string
    class line_token_stream:
        def __init__( self, s ):
            self.line = s

        def __call__( self ):
            if self.line:
                s = self.line
                self.line = None
                return s
            raise StopIteration

    def tokenize_line( s ):
        for t in  tokenize.generate_tokens( line_token_stream( s ) ):
            yield t

    for i in tokenize_line( "func(1+2)" ):
        print i

```

Assignee: somebody

Status: new

Issue created by migration from https://trac.sagemath.org/ticket/689


