# Issue 24078: find all solutions using given rows + find first solution in parallel in dancing links + doc

Issue created by migration from Trac.

Original creator: slabbe

Original creation time: 2017-12-02 13:14:25


```
    sage: from sage.combinat.matrices.dancing_links import dlx_solver
    sage: rows = [[0,1,2], [3,4,5], [0,1], [2,3,4,5], [0], [1,2,3,4,5]]
    sage: x = dlx_solver(rows)
    sage: x
    Dancing links solver for 6 columns and 6 rows
```


Return the first solution found when the computation is done in parallel::

```
    sage: x.first_solution_found_in_parallel(ncpus=8)
    [0, 1]
```


Find all solutions using some specific rows::


```
    sage: x_using_row_2 = x.restrict([2])
    sage: list(x_using_row_2.solutions_iterator())
    [This is the Trac macro *2, 3* that was inherited from the migration](https://trac.sagemath.org/wiki/WikiMacros#2, 3-macro)
```



---

Comment by slabbe created at 2017-12-02 13:35:09

New commits:


---

Comment by slabbe created at 2017-12-02 13:35:09

Changing status from new to needs_review.


---

Comment by slabbe created at 2017-12-02 13:37:22

Changing keywords from "" to "thursdaysbdx".


---

Comment by git created at 2017-12-02 14:29:55

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by git created at 2017-12-02 14:35:45

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by git created at 2017-12-03 09:08:17

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by vdelecroix created at 2017-12-03 11:32:16

Changing status from needs_review to needs_work.


---

Comment by vdelecroix created at 2017-12-03 11:32:16

Adding `nrows`, `ncols` and `restrict` is a good idea. However in the doctest I would have used an example with a number of rows different from the number of columns.

I really don't like the name `first_solution_found_in_parallel`. Why should we care that it is in parallel? I would just call it `one_solution` and have a `ncpus` parameter (the very same way it is right now for `number_of_solutions`).

In the `first_solution_found_in_parallel` are you sure that the subprocesses are stopped once a solution has been found? (I don't believe it). If you want such a thing to happen you need to explicitely call `terminate()` on the pool. The ``@`parallel` interface is not nice enough. See [https://docs.python.org/2/library/multiprocessing.html](https://docs.python.org/2/library/multiprocessing.html). And this needs careful doctesting.

Having `number_of_solutions` exhausts the iterator but not `first_solution_found_in_parallel` is a mistake in the interface.


---

Comment by slabbe created at 2017-12-04 23:35:06

Merci pour ton review Vincent. Je vais me remettre sur ça dans quelques jours.


---

Comment by git created at 2017-12-11 20:37:01

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by slabbe created at 2017-12-11 20:42:46

Replying to [comment:8 vdelecroix]:
> Adding `nrows`, `ncols` and `restrict` is a good idea. However in the doctest I would have used an example with a number of rows different from the number of columns.

Done.

> I really don't like the name `first_solution_found_in_parallel`. Why should we care that it is in parallel? I would just call it `one_solution` and have a `ncpus` parameter (the very same way it is right now for `number_of_solutions`).

okay, I changed it to `one_solution`

Should `some_solution` be better following the idioms used in Sage like `some_element` ...

> In the `first_solution_found_in_parallel` are you sure that the subprocesses are stopped once a solution has been found? (I don't believe it). If you want such a thing to happen you need to explicitely call `terminate()` on the pool. The ``@`parallel` interface is not nice enough. See [https://docs.python.org/2/library/multiprocessing.html](https://docs.python.org/2/library/multiprocessing.html). And this needs careful doctesting.

No I am not sure. I was trusting the ``@`parallel` decorator. Why should this `terminate` handling not be done inside the ``@`parallel` code?
 
> Having `number_of_solutions` exhausts the iterator but not `first_solution_found_in_parallel` is a mistake in the interface.

I know. It has always been like that. I agree to take a look at this. My idea would be to implement a `reset` method inside the wrapped `C` or `C++` code and then call first when needed, like before starting an iteration of solutions... I suggest that this should be done in another independent ticket.


---

Comment by slabbe created at 2017-12-11 21:02:10

> In the `first_solution_found_in_parallel` are you sure that the subprocesses are stopped once a solution has been found? (I don't believe it). If you want such a thing to happen you need to explicitely call `terminate()` on the pool. The ``@`parallel` interface is not nice enough. See [https://docs.python.org/2/library/multiprocessing.html](https://docs.python.org/2/library/multiprocessing.html). And this needs careful doctesting.

I just double checked running code on examples (Wang tiling in a 12x12 box). And all cpus stop working at the same time when one solution is found and returned. So to me, the code is okay for this aspect.


```
sage: dlx
Dancing links solver for 1200 columns and 1872 rows
sage:
sage: %time sol = dlx.one_solution(ncpus=8)
CPU times: user 4 ms, sys: 60 ms, total: 64 ms
Wall time: 6.77 s
```



---

Comment by slabbe created at 2017-12-11 21:02:10

Changing status from needs_work to needs_review.


---

Comment by vdelecroix created at 2017-12-12 07:49:07

Replying to [comment:11 slabbe]:
> Replying to [comment:8 vdelecroix]:
> > In the `first_solution_found_in_parallel` are you sure that the subprocesses are stopped once a solution has been found? (I don't believe it). If you want such a thing to happen you need to explicitely call `terminate()` on the pool. The ``@`parallel` interface is not nice enough. See [https://docs.python.org/2/library/multiprocessing.html](https://docs.python.org/2/library/multiprocessing.html). And this needs careful doctesting.
> 
> No I am not sure. I was trusting the ``@`parallel` decorator. Why should this `terminate` handling not be done inside the ``@`parallel` code?

I think I do have a problem on my computer (parallel is not working at all). Could you write the following in a file

```
from sage.parallel.decorate import parallel
from time import sleep
from sage.misc.prandom import randint

`@`parallel
def f(n):
    sleep(randint(0,3))
    return n
```

and check whether the following runs in parallel

```
sage: from WHERE_IT_IS import f
sage: list(f(range(10)), ncpus=3)
```

On my computer, only one process is launched at a time.

Moreover, if the iterator is not exhausted and I quit Sage, I got an `OSError`such as

```
sage: F = f(range(10),ncpus=3)
sage: next(F)
(((0,), {}), 0)
sage: quit
Exiting Sage (CPU time 0m0.30s, Wall time 0m5.49s).
Exception OSError: OSError(2, 'No such file or directory') in <generator object __call__ at 0x7f5118bc7410> ignored
```



---

Comment by slabbe created at 2017-12-13 07:47:57

I see what you mean. I was able to reproduce. Calling next once launches all the processes, and it seems the user is expected to exhaust the generator before exiting. It may be a bug. In my case, I do not use the `next`.

But if I use it in a for loop, and call `return` inside the loop, then it is okay. Can you confirm?


```
r"""
EXAMPLES:

Not okay::

    sage: %runfile file.sage
    sage: it = g(range(360,370))
    sage: next(it)
    (((360,), {}),
    3^3 * 5^2 * 7 * 11 * 13 * 17 * 19 * 31 * 37 * 41 * 61 * 73 * 109 * 151 * 181 * 241 * 331 * 433 * 631 * 1321 * 23311 * 38737 * 54001 * 61681 * 18837001 * 29247661 * 4562284561 * 168692292721 * 469775495062434961)
    sage: exit
    Exiting Sage (CPU time 0m0.63s, Wall time 0m20.09s).
    Exception OSError: OSError(2, 'No such file or directory') in <generator object __call__ at 0x7fef19074f00> ignored

This is okay::

    sage: %runfile file.sage
    sage: h(range(500, 510))
    (((504,), {}),
    3^3 * 5 * 7^2 * 13 * 17 * 19 * 29 * 37 * 43 * 73 * 109 * 113 * 127 * 241 * 337 * 433 * 1009 * 1429 * 3361 * 5419 * 14449 * 21169 * 38737 * 92737 * 649657 * 2627857 * 15790321 * 269389009 * 40388473189 * 77158673929 * 88959882481 * 118750098349 * 1475204679190128571777)
    sage: exit
    Exiting Sage (CPU time 0m0.25s, Wall time 0m26.61s).

"""
from sage.parallel.decorate import parallel
from time import sleep
from sage.misc.prandom import randint

`@`parallel
def f(n):
    sleep(randint(0,3))
    return n

`@`parallel(ncpus=6)
def g(n):
    return factor(2^n-1)

def h(L):
    for result in g(L):
        return result

```



---

Comment by vdelecroix created at 2017-12-13 23:30:17

I guess that because of the two `é` in Sébastien Labbé, the patchbot is not so happy. Could you set the encoding?


---

Comment by slabbe created at 2017-12-14 07:29:53

For one patchbot result, I read `Error downloading gmpy2-2.1.0a1.tar.gz` and in the other, I read problem compiling `cmake`.


---

Comment by git created at 2017-12-14 07:32:46

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by slabbe created at 2017-12-14 19:30:10

I added the line

```
# -*- coding: utf-8 -*-
```


but is the following line still necessary in the header?

```
# distutils: language = c++
```



---

Comment by vdelecroix created at 2017-12-14 20:19:35

The distutils directive is mandatory in the `.pxd`/`.pyx` file when you want Cython to compile C++. This distutils directive for the `.pyx` file can also be manually be put in `module_list.py`. But removing it from a `.pxd` file would break external Cython code using it.


---

Comment by slabbe created at 2017-12-14 21:01:29

ok


---

Comment by vdelecroix created at 2017-12-15 12:05:22

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2017-12-18 19:39:25

Resolution: fixed


---

Comment by vbraun created at 2017-12-23 16:22:22

Followup at #24424
