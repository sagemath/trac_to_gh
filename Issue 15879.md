# Issue 15879: Multiplication of dense cyclotomic matrices should be faster

archive/issues_015879.json:
```json
{
    "body": "CC:  sage-combinat @nthiery stumpc5 @videlec @williamstein\n\nKeywords: cyclotomic field, matrix, multiplication, benchmark, days57\n\nThe multiplication of matrices with a (universal) cyclotomic fields as base ring could be optimized as the following profiling shows:\n\n\n```\n    sage: x,y=var('x,y')\n    sage: PR=PolynomialRing(QQ,[x,y])\n    sage: I=Ideal(x^2 - 1/2*x - 1/4, y^3 - 1/2*y^2 - 1/2*y + 1/8)\n    sage: Q=PR.quotient(I)\n    sage: a,b=Q.gens()\n    sage: elmt=matrix(Q,[[-1, 1, 2*x],[-4*x*y - 1, 4*x*y + 4*y^2, 4*x*y + 2*x],[-2*x, 2*x + 2*y, 2*x]])\n    sage: %timeit elmt^2\n    1000 loops, best of 3: 1.17 ms per loop\n\n    sage: UCF.<E>=UniversalCyclotomicField()\n    sage: ae=(E(10)+~E(10))/2  #same value as a\n    sage: be=(E(14)+~E(14))/2  #same value as b\n    sage: m=matrix(UCF,[[-1, 1, 2*ae],[-4*ae*be - 1, 4*ae*be + 4*be^2, 4*ae*be + 2*ae],[-2*ae, 2*ae + 2*be, 2*ae]])\n    sage: %timeit m^2\n    100 loops, best of 3: 8.13 ms per loop\n\n    sage: CFC.<F>=CyclotomicField(2*5*7)\n    sage: af=(F^7+~F^7)/2 #same value as a\n    sage: bf=(F^5+~F^5)/2 #same value as b\n    sage: m2=matrix(CFC,[[-1, 1, 2*af],[-4*af*bf - 1, 4*af*bf + 4*bf^2, 4*af*bf + 2*af],[-2*af, 2*af + 2*bf, 2*af]])\n    sage: %timeit m2^2\n    100 loops, best of 3: 4.99 ms per loop\n```\n\n\nThe three matrices elmt, m and m2 are the same encoded into 3 different base rings. It would be natural to think that the cyclotomic field be the optimal field to do computations, but it does not seem to be the case in practice.\n\nIssue created by migration from https://trac.sagemath.org/ticket/16116\n\n",
    "created_at": "2014-04-09T23:02:51Z",
    "labels": [
        "component: number fields"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-8.1",
    "title": "Multiplication of dense cyclotomic matrices should be faster",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/15879",
    "user": "https://github.com/jplab"
}
```
CC:  sage-combinat @nthiery stumpc5 @videlec @williamstein

Keywords: cyclotomic field, matrix, multiplication, benchmark, days57

The multiplication of matrices with a (universal) cyclotomic fields as base ring could be optimized as the following profiling shows:


```
    sage: x,y=var('x,y')
    sage: PR=PolynomialRing(QQ,[x,y])
    sage: I=Ideal(x^2 - 1/2*x - 1/4, y^3 - 1/2*y^2 - 1/2*y + 1/8)
    sage: Q=PR.quotient(I)
    sage: a,b=Q.gens()
    sage: elmt=matrix(Q,[[-1, 1, 2*x],[-4*x*y - 1, 4*x*y + 4*y^2, 4*x*y + 2*x],[-2*x, 2*x + 2*y, 2*x]])
    sage: %timeit elmt^2
    1000 loops, best of 3: 1.17 ms per loop

    sage: UCF.<E>=UniversalCyclotomicField()
    sage: ae=(E(10)+~E(10))/2  #same value as a
    sage: be=(E(14)+~E(14))/2  #same value as b
    sage: m=matrix(UCF,[[-1, 1, 2*ae],[-4*ae*be - 1, 4*ae*be + 4*be^2, 4*ae*be + 2*ae],[-2*ae, 2*ae + 2*be, 2*ae]])
    sage: %timeit m^2
    100 loops, best of 3: 8.13 ms per loop

    sage: CFC.<F>=CyclotomicField(2*5*7)
    sage: af=(F^7+~F^7)/2 #same value as a
    sage: bf=(F^5+~F^5)/2 #same value as b
    sage: m2=matrix(CFC,[[-1, 1, 2*af],[-4*af*bf - 1, 4*af*bf + 4*bf^2, 4*af*bf + 2*af],[-2*af, 2*af + 2*bf, 2*af]])
    sage: %timeit m2^2
    100 loops, best of 3: 4.99 ms per loop
```


The three matrices elmt, m and m2 are the same encoded into 3 different base rings. It would be natural to think that the cyclotomic field be the optimal field to do computations, but it does not seem to be the case in practice.

Issue created by migration from https://trac.sagemath.org/ticket/16116





---

archive/issue_comments_205974.json:
```json
{
    "body": "Hello,\n\nWith #18152, I got a 10x speedup\n\nold version:\n\n```\nsage: %timeit m^2\n100 loops, best of 3: 3.65 ms per loop\n```\n\n\nnew version:\n\n```\nsage: %timeit m^2\nThe slowest run took 69.65 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 336 \u00b5s per loop\n```\n\n\nVincent",
    "created_at": "2015-04-11T08:42:21Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205974",
    "user": "https://github.com/videlec"
}
```

Hello,

With #18152, I got a 10x speedup

old version:

```
sage: %timeit m^2
100 loops, best of 3: 3.65 ms per loop
```


new version:

```
sage: %timeit m^2
The slowest run took 69.65 times longer than the fastest. This could mean that an intermediate result is being cached 
1000 loops, best of 3: 336 µs per loop
```


Vincent



---

archive/issue_comments_205975.json:
```json
{
    "body": "And using libgap directly is even faster\n\n```\nsage: M = m._libgap_()\nsage: %timeit M^2\nThe slowest run took 9.57 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 183 \u00b5s per loop\n```\n\nSo, as written in the bottom of the description in #18152, we should wrap GAP matrices to deal with dense cyclotomics matrices in Sage.\n\nVincent",
    "created_at": "2015-04-11T08:44:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205975",
    "user": "https://github.com/videlec"
}
```

And using libgap directly is even faster

```
sage: M = m._libgap_()
sage: %timeit M^2
The slowest run took 9.57 times longer than the fastest. This could mean that an intermediate result is being cached 
1000 loops, best of 3: 183 µs per loop
```

So, as written in the bottom of the description in #18152, we should wrap GAP matrices to deal with dense cyclotomics matrices in Sage.

Vincent



---

archive/issue_comments_205976.json:
```json
{
    "body": "Hello,\n\nI reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).\n\nI had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)? On the other hand, I am pretty sure that some cleaning could be of great speed up. By cleaning I mean:\n- declare `cdef` variables wherever possible\n- let as minimum as possible `import` inside the methods\n- ...\nYou can also do some profiling on the code (using \"%prun\" and \"%crun\"), see #17689 which is not yet in the current development release.\n\nVincent",
    "created_at": "2015-04-11T13:15:35Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205976",
    "user": "https://github.com/videlec"
}
```

Hello,

I reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).

I had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)? On the other hand, I am pretty sure that some cleaning could be of great speed up. By cleaning I mean:
- declare `cdef` variables wherever possible
- let as minimum as possible `import` inside the methods
- ...
You can also do some profiling on the code (using "%prun" and "%crun"), see #17689 which is not yet in the current development release.

Vincent



---

archive/issue_comments_205977.json:
```json
{
    "body": "Replying to [comment:7 vdelecroix]:\n> Hello,\n> \n> I reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).\n> \n> I had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)?\n\nI designed and implemented the algorithm for dense cyclotomic matrices.  We were optimizing for larger matrices... which in the context of modular forms means at least 100 rows (and often much, much more).   GAP/pari on the other hand optimize for relatively tiny matrices.   The asymptotically fast algorithms for large matrices are totally different than for small...",
    "created_at": "2015-04-11T14:19:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205977",
    "user": "https://github.com/williamstein"
}
```

Replying to [comment:7 vdelecroix]:
> Hello,
> 
> I reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).
> 
> I had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)?

I designed and implemented the algorithm for dense cyclotomic matrices.  We were optimizing for larger matrices... which in the context of modular forms means at least 100 rows (and often much, much more).   GAP/pari on the other hand optimize for relatively tiny matrices.   The asymptotically fast algorithms for large matrices are totally different than for small...



---

archive/issue_comments_205978.json:
```json
{
    "body": "Replying to [comment:8 was]:\n> Replying to [comment:7 vdelecroix]:\n> > Hello,\n> > \n> > I reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).\n> > \n> > I had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)?\n> \n> I designed and implemented the algorithm for dense cyclotomic matrices.  We were optimizing for larger matrices... which in the context of modular forms means at least 100 rows (and often much, much more).   GAP/pari on the other hand optimize for relatively tiny matrices.   The asymptotically fast algorithms for large matrices are totally different than for small...\n\nAll right. I now understand better what I read! I see two possibilities.\n\n1) [easy one] We add a test in the matrix space constructor:\n- if the size is small -> use the generic implementation of dense matrices\n- if the size is large -> use your optimized version\nThis requires a bit of benchmark.\n\n2) [better one] Wrap pari matrices for small sizes or add a another multiplication on the current datatype that is fast on small matrices.\n\nVincent",
    "created_at": "2015-04-11T14:26:24Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205978",
    "user": "https://github.com/videlec"
}
```

Replying to [comment:8 was]:
> Replying to [comment:7 vdelecroix]:
> > Hello,
> > 
> > I reformatted your example such that they fit in less lines (it can easily switched back to your original version if you do not like it).
> > 
> > I had a quick look at the code for dense cyclotomic matrices. The implementation is quite old and uses a lot of reduction mod p (even for multiplication). The code calls a lot of Python code like creating a finite field, creating a matrix space, etc which are relatively slow compared to a small matrix multiplication. Did you try multiplying larger matrices (i.e. 10x10 or 15x15)?
> 
> I designed and implemented the algorithm for dense cyclotomic matrices.  We were optimizing for larger matrices... which in the context of modular forms means at least 100 rows (and often much, much more).   GAP/pari on the other hand optimize for relatively tiny matrices.   The asymptotically fast algorithms for large matrices are totally different than for small...

All right. I now understand better what I read! I see two possibilities.

1) [easy one] We add a test in the matrix space constructor:
- if the size is small -> use the generic implementation of dense matrices
- if the size is large -> use your optimized version
This requires a bit of benchmark.

2) [better one] Wrap pari matrices for small sizes or add a another multiplication on the current datatype that is fast on small matrices.

Vincent



---

archive/issue_comments_205979.json:
```json
{
    "body": "Hi,\n\nIn the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.\n\nHow to proceed to add another multiplication to the matrices that could be used for small matrices?\n\nAre there examples around with such a thing? I could look at it...",
    "created_at": "2015-04-13T13:19:29Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205979",
    "user": "https://github.com/jplab"
}
```

Hi,

In the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.

How to proceed to add another multiplication to the matrices that could be used for small matrices?

Are there examples around with such a thing? I could look at it...



---

archive/issue_comments_205980.json:
```json
{
    "body": "Replying to [comment:10 jipilab]:\n> Hi,\n> \n> In the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.\n> \n> How to proceed to add another multiplication to the matrices that could be used for small matrices?\n> \n> Are there examples around with such a thing? I could look at it...\n\nMatrices over ZZ used to have special code for small versus large.   I think now that variation in algorithms is mainly hidden by calling FLINT.      Look into it.",
    "created_at": "2015-04-13T13:21:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205980",
    "user": "https://github.com/williamstein"
}
```

Replying to [comment:10 jipilab]:
> Hi,
> 
> In the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.
> 
> How to proceed to add another multiplication to the matrices that could be used for small matrices?
> 
> Are there examples around with such a thing? I could look at it...

Matrices over ZZ used to have special code for small versus large.   I think now that variation in algorithms is mainly hidden by calling FLINT.      Look into it.



---

archive/issue_comments_205981.json:
```json
{
    "body": "Replying to [comment:11 was]:\n> Replying to [comment:10 jipilab]:\n> > Hi,\n> > \n> > In the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.\n> > \n> > How to proceed to add another multiplication to the matrices that could be used for small matrices?\n> > \n> > Are there examples around with such a thing? I could look at it...\n> \n> Matrices over ZZ used to have special code for small versus large.   I think now that variation in algorithms is mainly hidden by calling FLINT.      Look into it.\n\nWilliam, are you sure that the representation you used in `MatrixDense_cyclotomic` is the thing we want for small sizes? If that so, I would rather implement something for any number fields. I do not see why it might be different. Did you have something in mind?\n\nIn the present case, I would rather modify `MatrixSpace._get_matrix_class` to choose the generic dense matrices for small sizes and see if something break.",
    "created_at": "2015-04-13T13:37:18Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205981",
    "user": "https://github.com/videlec"
}
```

Replying to [comment:11 was]:
> Replying to [comment:10 jipilab]:
> > Hi,
> > 
> > In the case I'm interested in, it is definitely for small sizes. Say up to 20-25 at the very very biggest. Most commonly it is going to be between 3 and 10. This is related to the tickets #15703, #16087.
> > 
> > How to proceed to add another multiplication to the matrices that could be used for small matrices?
> > 
> > Are there examples around with such a thing? I could look at it...
> 
> Matrices over ZZ used to have special code for small versus large.   I think now that variation in algorithms is mainly hidden by calling FLINT.      Look into it.

William, are you sure that the representation you used in `MatrixDense_cyclotomic` is the thing we want for small sizes? If that so, I would rather implement something for any number fields. I do not see why it might be different. Did you have something in mind?

In the present case, I would rather modify `MatrixSpace._get_matrix_class` to choose the generic dense matrices for small sizes and see if something break.



---

archive/issue_comments_205982.json:
```json
{
    "body": "> William, are you sure that the representation you used in MatrixDense_cyclotomic is the thing we want for small sizes? \n\nNo.  In fact, I'm pretty sure they are *not* what you would want for small sizes.",
    "created_at": "2015-04-13T16:37:37Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205982",
    "user": "https://github.com/williamstein"
}
```

> William, are you sure that the representation you used in MatrixDense_cyclotomic is the thing we want for small sizes? 

No.  In fact, I'm pretty sure they are *not* what you would want for small sizes.



---

archive/issue_comments_205983.json:
```json
{
    "body": "We also have a related issue:\n\n```\nsage: R = CyclotomicField(12)\nsage: M = matrix.random(R, 40,40)\nsage: N = matrix.random(R, 3, 3)\nsage: %time K = M.tensor_product(N)\nCPU times: user 5.75 s, sys: 28.4 ms, total: 5.78 s\nWall time: 5.73 s\nsage: R.defining_polynomial()\nx^4 - x^2 + 1\nsage: type(M)\n<type 'sage.matrix.matrix_cyclo_dense.Matrix_cyclo_dense'>\n\nsage: R = NumberField(x^4 - x^2 + 1, 'a')\nsage: M = matrix.random(R, 40,40)\nsage: N = matrix.random(R, 3, 3)\nsage: %time K = M.tensor_product(N)\nCPU times: user 225 ms, sys: 16.4 ms, total: 241 ms\nWall time: 232 ms\nsage: type(M)\n<type 'sage.matrix.matrix_generic_dense.Matrix_generic_dense'>\n```\n\nWhere the issue is coming from having a scalar times a matrix. Here's some profiling info of doing it over the cyclotomic field:\n\n```\n   594202    1.806    0.000    3.620    0.000 number_field.py:9200(_element_constructor_)\n   594202    0.816    0.000    1.171    0.000 number_field.py:6628(_coerce_non_number_field_element_in)\n  4184691    0.569    0.000    0.569    0.000 {isinstance}\n```\n\nThis is nowhere to be found when doing it over the number field. (For very small matrices this isn't a problem per se, but it still is visible when profiling.)\n\nSo my conclusion is that we are doing something wrong with how we handle multiplication with cyclotomics in the matrix versus our generic dense cases.",
    "created_at": "2015-09-16T23:09:26Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205983",
    "user": "https://github.com/tscrim"
}
```

We also have a related issue:

```
sage: R = CyclotomicField(12)
sage: M = matrix.random(R, 40,40)
sage: N = matrix.random(R, 3, 3)
sage: %time K = M.tensor_product(N)
CPU times: user 5.75 s, sys: 28.4 ms, total: 5.78 s
Wall time: 5.73 s
sage: R.defining_polynomial()
x^4 - x^2 + 1
sage: type(M)
<type 'sage.matrix.matrix_cyclo_dense.Matrix_cyclo_dense'>

sage: R = NumberField(x^4 - x^2 + 1, 'a')
sage: M = matrix.random(R, 40,40)
sage: N = matrix.random(R, 3, 3)
sage: %time K = M.tensor_product(N)
CPU times: user 225 ms, sys: 16.4 ms, total: 241 ms
Wall time: 232 ms
sage: type(M)
<type 'sage.matrix.matrix_generic_dense.Matrix_generic_dense'>
```

Where the issue is coming from having a scalar times a matrix. Here's some profiling info of doing it over the cyclotomic field:

```
   594202    1.806    0.000    3.620    0.000 number_field.py:9200(_element_constructor_)
   594202    0.816    0.000    1.171    0.000 number_field.py:6628(_coerce_non_number_field_element_in)
  4184691    0.569    0.000    0.569    0.000 {isinstance}
```

This is nowhere to be found when doing it over the number field. (For very small matrices this isn't a problem per se, but it still is visible when profiling.)

So my conclusion is that we are doing something wrong with how we handle multiplication with cyclotomics in the matrix versus our generic dense cases.



---

archive/issue_comments_205984.json:
```json
{
    "body": "I should note that I get very different profiling when I reverse the orders of the tensor product, which from the naive implementation of the tensor product and thoughts about scalar multiplication surprises me:\n\n```\nsage: R = CyclotomicField(2)\nsage: M = matrix.random(R, 40,40)\nsage: N = matrix.random(R, 3, 3)\nsage: %time K = N.tensor_product(M)\nCPU times: user 337 ms, sys: 20.6 ms, total: 358 ms\nWall time: 335 ms\nsage: %time K = M.tensor_product(N)\nCPU times: user 3.99 s, sys: 32.5 ms, total: 4.02 s\nWall time: 3.97 s\n```\n\nThere are quite a lot more function calls (~10x) to the `_element_constructor_` in one ordering:\n\n```\n48023 for _element_constructor_\n577312 function calls (577311 primitive calls) in 0.421 seconds\n```\n\nversus\n\n```\n594198  for _element_constructor_\n7240514 function calls (7240513 primitive calls) in 4.992 seconds\n```\n",
    "created_at": "2015-09-16T23:18:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205984",
    "user": "https://github.com/tscrim"
}
```

I should note that I get very different profiling when I reverse the orders of the tensor product, which from the naive implementation of the tensor product and thoughts about scalar multiplication surprises me:

```
sage: R = CyclotomicField(2)
sage: M = matrix.random(R, 40,40)
sage: N = matrix.random(R, 3, 3)
sage: %time K = N.tensor_product(M)
CPU times: user 337 ms, sys: 20.6 ms, total: 358 ms
Wall time: 335 ms
sage: %time K = M.tensor_product(N)
CPU times: user 3.99 s, sys: 32.5 ms, total: 4.02 s
Wall time: 3.97 s
```

There are quite a lot more function calls (~10x) to the `_element_constructor_` in one ordering:

```
48023 for _element_constructor_
577312 function calls (577311 primitive calls) in 0.421 seconds
```

versus

```
594198  for _element_constructor_
7240514 function calls (7240513 primitive calls) in 4.992 seconds
```




---

archive/issue_comments_205985.json:
```json
{
    "body": "The part which handles speeding up the tensor product is now #19258.",
    "created_at": "2015-09-20T16:46:05Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205985",
    "user": "https://github.com/tscrim"
}
```

The part which handles speeding up the tensor product is now #19258.



---

archive/issue_comments_205986.json:
```json
{
    "body": "It seems that matrix multiplication over the universal cyclotomic field is on the same order as the polynomial ring (probably because it uses the generic matrix class):\n\n```\nsage: %timeit m * m\n1000 loops, best of 3: 224 \u00b5s per loop\nsage: %timeit elmt * elmt\n1000 loops, best of 3: 207 \u00b5s per loop\n```\n\nHowever for UCF matrices, I'm thinking we might benefit from either using (lib)GAP's matrix multiplication or internally storing the GAP element and only converting it to a Sage UCF element as necessary. See #19821 for a use-case.",
    "created_at": "2016-01-03T01:15:31Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205986",
    "user": "https://github.com/tscrim"
}
```

It seems that matrix multiplication over the universal cyclotomic field is on the same order as the polynomial ring (probably because it uses the generic matrix class):

```
sage: %timeit m * m
1000 loops, best of 3: 224 µs per loop
sage: %timeit elmt * elmt
1000 loops, best of 3: 207 µs per loop
```

However for UCF matrices, I'm thinking we might benefit from either using (lib)GAP's matrix multiplication or internally storing the GAP element and only converting it to a Sage UCF element as necessary. See #19821 for a use-case.



---

archive/issue_comments_205987.json:
```json
{
    "body": "At least on sage-7.0.beta2, wrapping GAP matrices for the examples mentioned in the ticket description will not bring any magic\n\n```\nsage: M = m._libgap_()\nsage: %timeit A = M^2\n1000 loops, best of 3: 181 \u00b5s per loop\nsage: %timeit A = M^3\n1000 loops, best of 3: 456 \u00b5s per loop\n```\n\nversus\n\n```\nsage: %timeit a = m^2\n1000 loops, best of 3: 298 \u00b5s per loop\nsage: %timeit a = m^3\n1000 loops, best of 3: 690 \u00b5s per loop\n```\n\nWe are below x2 speedup. But in this example the matrix is small and coefficients relatively dense (~25 nonzero coefficients). Though, the gain is significant with 10x10 dense matrices with small coefficients\n\n```\nsage: m1 = matrix(10, [E(randint(2,3)) for _ in range(100)]) \nsage: m2 = matrix(10, [E(randint(2,3)) for _ in range(100)])\nsage: %timeit m1*m2\n100 loops, best of 3: 4.51 ms per loop\nsage: %timeit M1*M2\n1000 loops, best of 3: 329 \u00b5s per loop\n```\n\nWe might update the ticket description accordingly. Two concrete propositions are:\n- below a certain threshold (to be determined) use generic matrices for cyclotomic fields\n- wrap GAP matrices for UCF\nWhat do you think?",
    "created_at": "2016-01-03T03:32:32Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205987",
    "user": "https://github.com/videlec"
}
```

At least on sage-7.0.beta2, wrapping GAP matrices for the examples mentioned in the ticket description will not bring any magic

```
sage: M = m._libgap_()
sage: %timeit A = M^2
1000 loops, best of 3: 181 µs per loop
sage: %timeit A = M^3
1000 loops, best of 3: 456 µs per loop
```

versus

```
sage: %timeit a = m^2
1000 loops, best of 3: 298 µs per loop
sage: %timeit a = m^3
1000 loops, best of 3: 690 µs per loop
```

We are below x2 speedup. But in this example the matrix is small and coefficients relatively dense (~25 nonzero coefficients). Though, the gain is significant with 10x10 dense matrices with small coefficients

```
sage: m1 = matrix(10, [E(randint(2,3)) for _ in range(100)]) 
sage: m2 = matrix(10, [E(randint(2,3)) for _ in range(100)])
sage: %timeit m1*m2
100 loops, best of 3: 4.51 ms per loop
sage: %timeit M1*M2
1000 loops, best of 3: 329 µs per loop
```

We might update the ticket description accordingly. Two concrete propositions are:
- below a certain threshold (to be determined) use generic matrices for cyclotomic fields
- wrap GAP matrices for UCF
What do you think?



---

archive/issue_comments_205988.json:
```json
{
    "body": "A 30-40% speed reduction is nothing to scoff at either. So from that data, I think for dense matrices over the UCF we should always just wrap GAP.",
    "created_at": "2016-01-03T03:57:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205988",
    "user": "https://github.com/tscrim"
}
```

A 30-40% speed reduction is nothing to scoff at either. So from that data, I think for dense matrices over the UCF we should always just wrap GAP.



---

archive/issue_comments_205989.json:
```json
{
    "body": "Changing keywords from \"cyclotomic field, matrix, multiplication, benchmark, days57\" to \"cyclotomic field, matrix, multiplication, benchmark, days57, days88\".",
    "created_at": "2017-08-24T21:09:46Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205989",
    "user": "https://github.com/videlec"
}
```

Changing keywords from "cyclotomic field, matrix, multiplication, benchmark, days57" to "cyclotomic field, matrix, multiplication, benchmark, days57, days88".



---

archive/issue_comments_205990.json:
```json
{
    "body": "Changing component from number fields to linear algebra.",
    "created_at": "2017-08-24T22:40:17Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205990",
    "user": "https://github.com/videlec"
}
```

Changing component from number fields to linear algebra.



---

archive/issue_comments_205991.json:
```json
{
    "body": "Changing type from enhancement to task.",
    "created_at": "2017-08-24T22:40:17Z",
    "issue": "https://github.com/sagemath/sagetest/issues/15879",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/15879#issuecomment-205991",
    "user": "https://github.com/videlec"
}
```

Changing type from enhancement to task.
