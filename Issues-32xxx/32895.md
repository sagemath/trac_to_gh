# Issue 32895: Improve `_matrix_times_vector_` and `_vector_times_matrix_` for dense rational and integer matrices

archive/issues_032658.json:
```json
{
    "body": "CC:  @videlec @seblabbe @orlitzky\n\nWe improve multiplication of matrix and vector for flint matrices by using that the rows of the matrix are already stored as vectors.\n\nWe also wrap the code in `try ... finally` such that memory is deallocated in case of interrupt.\n\nBefore, but with #32901 (note the skipped case, because of memory problems):\n\n```       \nsage: def timings():\n....:     set_random_seed(0)\n....:     for ring in (ZZ, QQ):\n....:         print('')\n....:         for size in (10, 100, 1000, 10000):\n....:             M = random_matrix(ring, size)\n....:             v = random_vector(ring, size)\n....:             if size in (10, 100):\n....:                 %timeit v*M\n....:                 %timeit M*v\n....:             else:\n....:                 %time _ = v*M\n....:                 if size == 10000: continue\n....:                 %time _ = M*v\n....:                 \nsage: timings()\n\n2.61 \u00b5s \u00b1 6.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n24.8 \u00b5s \u00b1 133 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n243 \u00b5s \u00b1 144 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.7 ms \u00b1 1.82 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nCPU times: user 26.7 ms, sys: 3.99 ms, total: 30.7 ms\nWall time: 30.7 ms\nCPU times: user 371 ms, sys: 12 ms, total: 383 ms\nWall time: 383 ms\nCPU times: user 4.28 s, sys: 74 \u00b5s, total: 4.28 s\nWall time: 4.28 s\n\n14.4 \u00b5s \u00b1 48.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n88.3 \u00b5s \u00b1 228 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n1.58 ms \u00b1 327 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n7.23 ms \u00b1 12.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nCPU times: user 164 ms, sys: 0 ns, total: 164 ms\nWall time: 164 ms\nCPU times: user 712 ms, sys: 4 ms, total: 716 ms\nWall time: 716 ms\nCPU times: user 23.3 s, sys: 65 \u00b5s, total: 23.3 s\nWall time: 23.3 s\n```\n\nAfter:\n\n```\nsage: def timings():\n....:     set_random_seed(0)\n....:     for ring in (ZZ, QQ):\n....:         print('')\n....:         for size in (10, 100, 1000, 10000):\n....:             M = random_matrix(ring, size)\n....:             v = random_vector(ring, size)\n....:             if size in (10, 100):\n....:                 %timeit v*M\n....:                 %timeit M*v\n....:             else:\n....:                 %time _ = v*M\n....:                 %time _ = M*v\nsage: timings()\n           \n2.38 \u00b5s \u00b1 9.93 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n2.32 \u00b5s \u00b1 8.26 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n158 \u00b5s \u00b1 81.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n205 \u00b5s \u00b1 137 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nCPU times: user 14.8 ms, sys: 22 \u00b5s, total: 14.9 ms\nWall time: 14.9 ms\nCPU times: user 20.5 ms, sys: 4 \u00b5s, total: 20.5 ms\nWall time: 20.6 ms\nCPU times: user 1.42 s, sys: 0 ns, total: 1.42 s\nWall time: 1.42 s\nCPU times: user 1.91 s, sys: 0 ns, total: 1.91 s\nWall time: 1.91 s\n\n5.49 \u00b5s \u00b1 17.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n5.54 \u00b5s \u00b1 351 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n608 \u00b5s \u00b1 508 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n640 \u00b5s \u00b1 430 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nCPU times: user 114 ms, sys: 24 \u00b5s, total: 114 ms\nWall time: 114 ms\nCPU times: user 115 ms, sys: 3.98 ms, total: 119 ms\nWall time: 119 ms\nCPU times: user 16.8 s, sys: 0 ns, total: 16.8 s\nWall time: 16.8 s\nCPU times: user 16.6 s, sys: 0 ns, total: 16.6 s\nWall time: 16.6 s\n```\n\nIssue created by migration from https://trac.sagemath.org/ticket/32895\n\n",
    "created_at": "2021-11-18T16:17:22Z",
    "labels": [
        "component: linear algebra"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.8",
    "title": "Improve `_matrix_times_vector_` and `_vector_times_matrix_` for dense rational and integer matrices",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/32895",
    "user": "https://github.com/kliem"
}
```
CC:  @videlec @seblabbe @orlitzky

We improve multiplication of matrix and vector for flint matrices by using that the rows of the matrix are already stored as vectors.

We also wrap the code in `try ... finally` such that memory is deallocated in case of interrupt.

Before, but with #32901 (note the skipped case, because of memory problems):

```       
sage: def timings():
....:     set_random_seed(0)
....:     for ring in (ZZ, QQ):
....:         print('')
....:         for size in (10, 100, 1000, 10000):
....:             M = random_matrix(ring, size)
....:             v = random_vector(ring, size)
....:             if size in (10, 100):
....:                 %timeit v*M
....:                 %timeit M*v
....:             else:
....:                 %time _ = v*M
....:                 if size == 10000: continue
....:                 %time _ = M*v
....:                 
sage: timings()

2.61 µs ± 6.76 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
24.8 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
243 µs ± 144 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.7 ms ± 1.82 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 26.7 ms, sys: 3.99 ms, total: 30.7 ms
Wall time: 30.7 ms
CPU times: user 371 ms, sys: 12 ms, total: 383 ms
Wall time: 383 ms
CPU times: user 4.28 s, sys: 74 µs, total: 4.28 s
Wall time: 4.28 s

14.4 µs ± 48.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
88.3 µs ± 228 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
1.58 ms ± 327 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
7.23 ms ± 12.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
CPU times: user 164 ms, sys: 0 ns, total: 164 ms
Wall time: 164 ms
CPU times: user 712 ms, sys: 4 ms, total: 716 ms
Wall time: 716 ms
CPU times: user 23.3 s, sys: 65 µs, total: 23.3 s
Wall time: 23.3 s
```

After:

```
sage: def timings():
....:     set_random_seed(0)
....:     for ring in (ZZ, QQ):
....:         print('')
....:         for size in (10, 100, 1000, 10000):
....:             M = random_matrix(ring, size)
....:             v = random_vector(ring, size)
....:             if size in (10, 100):
....:                 %timeit v*M
....:                 %timeit M*v
....:             else:
....:                 %time _ = v*M
....:                 %time _ = M*v
sage: timings()
           
2.38 µs ± 9.93 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
2.32 µs ± 8.26 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
158 µs ± 81.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
205 µs ± 137 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 14.8 ms, sys: 22 µs, total: 14.9 ms
Wall time: 14.9 ms
CPU times: user 20.5 ms, sys: 4 µs, total: 20.5 ms
Wall time: 20.6 ms
CPU times: user 1.42 s, sys: 0 ns, total: 1.42 s
Wall time: 1.42 s
CPU times: user 1.91 s, sys: 0 ns, total: 1.91 s
Wall time: 1.91 s

5.49 µs ± 17.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
5.54 µs ± 351 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
608 µs ± 508 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
640 µs ± 430 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 114 ms, sys: 24 µs, total: 114 ms
Wall time: 114 ms
CPU times: user 115 ms, sys: 3.98 ms, total: 119 ms
Wall time: 119 ms
CPU times: user 16.8 s, sys: 0 ns, total: 16.8 s
Wall time: 16.8 s
CPU times: user 16.6 s, sys: 0 ns, total: 16.6 s
Wall time: 16.6 s
```

Issue created by migration from https://trac.sagemath.org/ticket/32895





---

archive/issue_comments_465365.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2021-11-18T16:17:45Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465365",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_465366.json:
```json
{
    "body": "Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.",
    "created_at": "2021-11-18T17:27:07Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465366",
    "user": "https://github.com/nbruin"
}
```

Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.



---

archive/issue_comments_465367.json:
```json
{
    "body": "Thanks for the suggestion. I'll look into it.",
    "created_at": "2021-11-18T21:35:15Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465367",
    "user": "https://github.com/kliem"
}
```

Thanks for the suggestion. I'll look into it.



---

archive/issue_comments_465368.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2021-11-18T21:47:23Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465368",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_465369.json:
```json
{
    "body": "Replying to [comment:5 nbruin]:\n> Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.\n\n\nIt's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.\n\nI was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.\n\nNow `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.",
    "created_at": "2021-11-18T21:52:17Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465369",
    "user": "https://github.com/kliem"
}
```

Replying to [comment:5 nbruin]:
> Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.


It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.

I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.

Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.



---

archive/issue_comments_465370.json:
```json
{
    "body": "Changing status from new to needs_review.",
    "created_at": "2021-11-18T21:53:17Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465370",
    "user": "https://github.com/kliem"
}
```

Changing status from new to needs_review.



---

archive/issue_comments_465371.json:
```json
{
    "body": "Just for the fun, I played around a bit with parallelization:\n\nThis is the `ZZ` part with up to 4 threads, depening on the size (I only have 2 cores):\n\n```\n7.11 \u00b5s \u00b1 12.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n2.5 \u00b5s \u00b1 21.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n170 \u00b5s \u00b1 437 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n104 \u00b5s \u00b1 539 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 15.7 ms, sys: 0 ns, total: 15.7 ms\nWall time: 9.78 ms\nCPU times: user 26.3 ms, sys: 10 \u00b5s, total: 26.3 ms\nWall time: 7.69 ms\nCPU times: user 2.59 s, sys: 8.09 ms, total: 2.6 s\nWall time: 660 ms\nCPU times: user 2.87 s, sys: 3.85 ms, total: 2.87 s\nWall time: 737 ms\n```\n\nThis could be done with just a few changes:\n\n```diff\ndiff --git a/src/sage/libs/flint/fmpz.pxd b/src/sage/libs/flint/fmpz.pxd\nindex 01058d0f13..e44a5b4595 100644\n--- a/src/sage/libs/flint/fmpz.pxd\n+++ b/src/sage/libs/flint/fmpz.pxd\n@@ -97,7 +97,7 @@ cdef extern from \"flint_wrap.h\":\n     void fmpz_mul2_uiui(fmpz_t, fmpz_t, ulong, ulong)\n     void fmpz_mul_2exp(fmpz_t, fmpz_t, ulong)\n \n-    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t)\n+    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t) nogil\n     void fmpz_addmul_ui(fmpz_t, fmpz_t, ulong)\n \n     void fmpz_submul(fmpz_t, fmpz_t, fmpz_t)\ndiff --git a/src/sage/libs/flint/fmpz_vec.pxd b/src/sage/libs/flint/fmpz_vec.pxd\nindex 87614f973c..813d59672e 100644\n--- a/src/sage/libs/flint/fmpz_vec.pxd\n+++ b/src/sage/libs/flint/fmpz_vec.pxd\n@@ -10,4 +10,4 @@ cdef extern from \"flint_wrap.h\":\n     ulong _fmpz_vec_max_limbs(const fmpz *, slong)\n     void _fmpz_vec_scalar_mod_fmpz(fmpz *, fmpz *, long, fmpz_t)\n     void _fmpz_vec_scalar_smod_fmpz(fmpz *, fmpz *, long, fmpz_t)\n-    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong)\n+    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong) nogil\ndiff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx\nindex 9872b02338..b6afa85df0 100644\n--- a/src/sage/matrix/matrix_integer_dense.pyx\n+++ b/src/sage/matrix/matrix_integer_dense.pyx\n@@ -1,8 +1,8 @@\n # -*- coding: utf-8 -*-\n-# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS\n+# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS OPENMP_CFLAGS\n # distutils: libraries = iml NTL_LIBRARIES gmp m CBLAS_LIBRARIES\n # distutils: library_dirs = NTL_LIBDIR CBLAS_LIBDIR\n-# distutils: extra_link_args = NTL_LIBEXTRA\n+# distutils: extra_link_args = NTL_LIBEXTRA OPENMP_CFLAGS\n # distutils: include_dirs = NTL_INCDIR M4RI_INCDIR CBLAS_INCDIR\n \"\"\"\n Dense matrices over the integer ring\n@@ -131,6 +131,7 @@ from .matrix2 import decomp_seq\n from .matrix cimport Matrix\n \n cimport sage.structure.element\n+from cython.parallel cimport prange\n \n import sage.matrix.matrix_space as matrix_space\n \n@@ -1068,6 +1069,13 @@ cdef class Matrix_integer_dense(Matrix_dense):\n         cdef fmpz* w_flint\n         cdef fmpz* ans_flint\n \n+        cdef int num_threads = 1\n+        if self._ncols > 500:\n+            num_threads = 2\n+        if self._ncols > 5000:\n+            num_threads = 4\n+\n+\n         M = self._row_ambient_module()\n         w = <Vector_integer_dense> v\n         ans = M.zero_vector()\n@@ -1089,9 +1097,15 @@ cdef class Matrix_integer_dense(Matrix_dense):\n             # So in the inner loop we have very little pointer movement.\n             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has\n             # no depence on the previous step.\n-            for j in range(self._nrows):\n-                for i in range(self._ncols):\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+            if num_threads == 1:\n+                for j in range(self._nrows):\n+                    for i in range(self._ncols):\n+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+            else:\n+                for j in range(self._nrows):\n+                    #for i in range(self._ncols):\n+                    for i in prange(self._ncols, num_threads=num_threads, nogil=True):\n+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n \n             for i in range(self._ncols):\n                 fmpz_get_mpz(ans._entries[i], ans_flint + i)\n@@ -1124,6 +1138,13 @@ cdef class Matrix_integer_dense(Matrix_dense):\n         cdef Py_ssize_t i, j\n         cdef fmpz_t x\n         cdef fmpz* w_flint\n+        cdef fmpz* ans_flint\n+\n+        cdef int num_threads = 1\n+        if self._ncols > 20:\n+            num_threads = 2\n+        if self._ncols > 500:\n+            num_threads = 4\n \n         M = self._column_ambient_module()\n         w = <Vector_integer_dense> v\n@@ -1131,20 +1152,28 @@ cdef class Matrix_integer_dense(Matrix_dense):\n \n         fmpz_init(x)\n         w_flint = _fmpz_vec_init(self._ncols)\n+        ans_flint = _fmpz_vec_init(self._nrows)\n \n         try:\n             sig_on()\n             for j in range(self._ncols):\n                 fmpz_set_mpz(w_flint + j, w._entries[j])\n \n+            if num_threads == 1:\n+                for i in range(self._nrows):\n+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)\n+            else:\n+                for i in prange(self._nrows, num_threads=num_threads, nogil=True):\n+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)\n+\n             for i in range(self._nrows):\n-                _fmpz_vec_dot(x, self._matrix.rows[i], w_flint, self._ncols)\n-                fmpz_get_mpz(ans._entries[i], x)\n+                fmpz_get_mpz(ans._entries[i], ans_flint + i)\n \n             sig_off()\n         finally:\n             fmpz_clear(x)\n             _fmpz_vec_clear(w_flint, self._ncols)\n+            _fmpz_vec_clear(ans_flint, self._nrows)\n \n         return ans\n```\n\nThis is probably just to advertize that `prange` can easily be used nowadays.",
    "created_at": "2021-11-18T22:30:38Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465371",
    "user": "https://github.com/kliem"
}
```

Just for the fun, I played around a bit with parallelization:

This is the `ZZ` part with up to 4 threads, depening on the size (I only have 2 cores):

```
7.11 µs ± 12.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
2.5 µs ± 21.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
170 µs ± 437 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
104 µs ± 539 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 15.7 ms, sys: 0 ns, total: 15.7 ms
Wall time: 9.78 ms
CPU times: user 26.3 ms, sys: 10 µs, total: 26.3 ms
Wall time: 7.69 ms
CPU times: user 2.59 s, sys: 8.09 ms, total: 2.6 s
Wall time: 660 ms
CPU times: user 2.87 s, sys: 3.85 ms, total: 2.87 s
Wall time: 737 ms
```

This could be done with just a few changes:

```diff
diff --git a/src/sage/libs/flint/fmpz.pxd b/src/sage/libs/flint/fmpz.pxd
index 01058d0f13..e44a5b4595 100644
--- a/src/sage/libs/flint/fmpz.pxd
+++ b/src/sage/libs/flint/fmpz.pxd
@@ -97,7 +97,7 @@ cdef extern from "flint_wrap.h":
     void fmpz_mul2_uiui(fmpz_t, fmpz_t, ulong, ulong)
     void fmpz_mul_2exp(fmpz_t, fmpz_t, ulong)
 
-    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t)
+    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t) nogil
     void fmpz_addmul_ui(fmpz_t, fmpz_t, ulong)
 
     void fmpz_submul(fmpz_t, fmpz_t, fmpz_t)
diff --git a/src/sage/libs/flint/fmpz_vec.pxd b/src/sage/libs/flint/fmpz_vec.pxd
index 87614f973c..813d59672e 100644
--- a/src/sage/libs/flint/fmpz_vec.pxd
+++ b/src/sage/libs/flint/fmpz_vec.pxd
@@ -10,4 +10,4 @@ cdef extern from "flint_wrap.h":
     ulong _fmpz_vec_max_limbs(const fmpz *, slong)
     void _fmpz_vec_scalar_mod_fmpz(fmpz *, fmpz *, long, fmpz_t)
     void _fmpz_vec_scalar_smod_fmpz(fmpz *, fmpz *, long, fmpz_t)
-    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong)
+    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong) nogil
diff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx
index 9872b02338..b6afa85df0 100644
--- a/src/sage/matrix/matrix_integer_dense.pyx
+++ b/src/sage/matrix/matrix_integer_dense.pyx
@@ -1,8 +1,8 @@
 # -*- coding: utf-8 -*-
-# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS
+# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS OPENMP_CFLAGS
 # distutils: libraries = iml NTL_LIBRARIES gmp m CBLAS_LIBRARIES
 # distutils: library_dirs = NTL_LIBDIR CBLAS_LIBDIR
-# distutils: extra_link_args = NTL_LIBEXTRA
+# distutils: extra_link_args = NTL_LIBEXTRA OPENMP_CFLAGS
 # distutils: include_dirs = NTL_INCDIR M4RI_INCDIR CBLAS_INCDIR
 """
 Dense matrices over the integer ring
@@ -131,6 +131,7 @@ from .matrix2 import decomp_seq
 from .matrix cimport Matrix
 
 cimport sage.structure.element
+from cython.parallel cimport prange
 
 import sage.matrix.matrix_space as matrix_space
 
@@ -1068,6 +1069,13 @@ cdef class Matrix_integer_dense(Matrix_dense):
         cdef fmpz* w_flint
         cdef fmpz* ans_flint
 
+        cdef int num_threads = 1
+        if self._ncols > 500:
+            num_threads = 2
+        if self._ncols > 5000:
+            num_threads = 4
+
+
         M = self._row_ambient_module()
         w = <Vector_integer_dense> v
         ans = M.zero_vector()
@@ -1089,9 +1097,15 @@ cdef class Matrix_integer_dense(Matrix_dense):
             # So in the inner loop we have very little pointer movement.
             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has
             # no depence on the previous step.
-            for j in range(self._nrows):
-                for i in range(self._ncols):
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+            if num_threads == 1:
+                for j in range(self._nrows):
+                    for i in range(self._ncols):
+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+            else:
+                for j in range(self._nrows):
+                    #for i in range(self._ncols):
+                    for i in prange(self._ncols, num_threads=num_threads, nogil=True):
+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
 
             for i in range(self._ncols):
                 fmpz_get_mpz(ans._entries[i], ans_flint + i)
@@ -1124,6 +1138,13 @@ cdef class Matrix_integer_dense(Matrix_dense):
         cdef Py_ssize_t i, j
         cdef fmpz_t x
         cdef fmpz* w_flint
+        cdef fmpz* ans_flint
+
+        cdef int num_threads = 1
+        if self._ncols > 20:
+            num_threads = 2
+        if self._ncols > 500:
+            num_threads = 4
 
         M = self._column_ambient_module()
         w = <Vector_integer_dense> v
@@ -1131,20 +1152,28 @@ cdef class Matrix_integer_dense(Matrix_dense):
 
         fmpz_init(x)
         w_flint = _fmpz_vec_init(self._ncols)
+        ans_flint = _fmpz_vec_init(self._nrows)
 
         try:
             sig_on()
             for j in range(self._ncols):
                 fmpz_set_mpz(w_flint + j, w._entries[j])
 
+            if num_threads == 1:
+                for i in range(self._nrows):
+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)
+            else:
+                for i in prange(self._nrows, num_threads=num_threads, nogil=True):
+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)
+
             for i in range(self._nrows):
-                _fmpz_vec_dot(x, self._matrix.rows[i], w_flint, self._ncols)
-                fmpz_get_mpz(ans._entries[i], x)
+                fmpz_get_mpz(ans._entries[i], ans_flint + i)
 
             sig_off()
         finally:
             fmpz_clear(x)
             _fmpz_vec_clear(w_flint, self._ncols)
+            _fmpz_vec_clear(ans_flint, self._nrows)
 
         return ans
```

This is probably just to advertize that `prange` can easily be used nowadays.



---

archive/issue_comments_465372.json:
```json
{
    "body": "Replying to [comment:9 gh-kliem]:\n> Replying to [comment:5 nbruin]:\n> > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.\n\n> \n> It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.\n> \n> I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.\n> \n> Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.\n\n\nIn that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already \"hot\" because it is still in cache from the computation that produced it?\n\nParallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.",
    "created_at": "2021-11-19T01:13:13Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465372",
    "user": "https://github.com/nbruin"
}
```

Replying to [comment:9 gh-kliem]:
> Replying to [comment:5 nbruin]:
> > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.

> 
> It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.
> 
> I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.
> 
> Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.


In that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already "hot" because it is still in cache from the computation that produced it?

Parallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.



---

archive/issue_comments_465373.json:
```json
{
    "body": "Now here are some timings of `_vector_times_matrix_` with some changes that we have some data:\n\nAs it is:\n\n```\n7.31 \u00b5s \u00b1 30.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n174 \u00b5s \u00b1 346 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 16.2 ms, sys: 2 \u00b5s, total: 16.2 ms\nWall time: 16.3 ms\nCPU times: user 1.54 s, sys: 0 ns, total: 1.54 s\nWall time: 1.54 s\n```\n\nReading in the vectors as if they were transposed\n\n```diff\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[i] + j)\n```\n\n```\n7.5 \u00b5s \u00b1 63 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n171 \u00b5s \u00b1 256 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 19.7 ms, sys: 0 ns, total: 19.7 ms\nWall time: 19.7 ms\nCPU times: user 3.36 s, sys: 60 \u00b5s, total: 3.36 s\nWall time: 3.36 s\n```\n\nAlso moving in the vector:\n\n```diff\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+                    fmpz_addmul(ans_flint + i, w_flint + i, self._matrix.rows[i] + j)\n```\n\n```\n7.02 \u00b5s \u00b1 31.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n204 \u00b5s \u00b1 160 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nCPU times: user 25.6 ms, sys: 0 ns, total: 25.6 ms\nWall time: 25.6 ms\nCPU times: user 3.91 s, sys: 1e+03 ns, total: 3.91 s\nWall time: 3.91 s\n```\n\nHaving small movements, but lots of dependencies:\n\n```diff\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+                    fmpz_addmul(ans_flint + j, w_flint + j, self._matrix.rows[j] + i)\n```\n\n```\n7.37 \u00b5s \u00b1 34.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n146 \u00b5s \u00b1 497 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 12.8 ms, sys: 0 ns, total: 12.8 ms\nWall time: 12.8 ms\nCPU times: user 1.24 s, sys: 3.94 ms, total: 1.24 s\nWall time: 1.24 s\n```\n\nA bit more movement with dependency:\n\n```diff\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n+                    fmpz_addmul(ans_flint + j, w_flint + i, self._matrix.rows[j] + i)\n```\n\n```\n6.87 \u00b5s \u00b1 31.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n202 \u00b5s \u00b1 242 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nCPU times: user 21.1 ms, sys: 17 \u00b5s, total: 21.1 ms\nWall time: 21.1 ms\nCPU times: user 2.09 s, sys: 0 ns, total: 2.09 s\nWall time: 2.09 s\n```\n\nNow using `_fmpz_vec_dot` instead of our manual version:\n\n```\n7.19 \u00b5s \u00b1 34 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n188 \u00b5s \u00b1 2.71 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms\nWall time: 19.5 ms\nCPU times: user 1.9 s, sys: 43 \u00b5s, total: 1.9 s\nWall time: 1.9 s\n```\n\nApparently using `fmpz_mat_entry(self._matrix, j, i)` instead of `self._matrix.rows[j] + i` makes no difference.\n\nAnd here are timings, that jump around more but avoid creating the answer vector:\n\n```diff\ndiff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx\nindex 9872b02338..f2c6d0b01e 100644\n--- a/src/sage/matrix/matrix_integer_dense.pyx\n+++ b/src/sage/matrix/matrix_integer_dense.pyx\n@@ -1073,33 +1073,29 @@ cdef class Matrix_integer_dense(Matrix_dense):\n         ans = M.zero_vector()\n \n         w_flint = _fmpz_vec_init(self._nrows)\n-        ans_flint = _fmpz_vec_init(self._ncols)\n+        fmpz_init(x)\n \n         try:\n             sig_on()\n             for j in range(self._nrows):\n                 fmpz_set_mpz(w_flint + j, w._entries[j])\n \n-            for i in range(self._ncols):\n-                fmpz_zero(ans_flint + i)\n-\n             # The order is crucial:\n             # ``self._matrix.rows[j] + i`` is right next to ``self._matrix[j] + i + 1``\n             # but far away from ``self._matrix[j + 1] + i``.\n             # So in the inner loop we have very little pointer movement.\n             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has\n             # no depence on the previous step.\n-            for j in range(self._nrows):\n-                for i in range(self._ncols):\n-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)\n-\n             for i in range(self._ncols):\n-                fmpz_get_mpz(ans._entries[i], ans_flint + i)\n+                fmpz_zero(x)\n+                for j in range(self._nrows):\n+                    fmpz_addmul(x, w_flint + j, self._matrix.rows[j] + i)\n+                fmpz_get_mpz(ans._entries[i], x)\n \n             sig_off()\n         finally:\n             _fmpz_vec_clear(w_flint, self._nrows)\n-            _fmpz_vec_clear(ans_flint, self._ncols)\n+            fmpz_clear(x)\n```\n\n```\n7.02 \u00b5s \u00b1 19.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n199 \u00b5s \u00b1 4.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nCPU times: user 24.8 ms, sys: 5 \u00b5s, total: 24.8 ms\nWall time: 24.8 ms\nCPU times: user 3.8 s, sys: 16 \u00b5s, total: 3.8 s\nWall time: 3.8 s\n```\n\nExcept for the first and last version, all other versions are very incorrect and just here to show what effect different changes have.\n\nWhat really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:\n\n```\nsage: M = random_matrix(ZZ, 10, 10)\nsage: %timeit M._column_ambient_module().zero_vector()\n612 ns \u00b1 2.81 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\nsage: %timeit M._row_ambient_module().zero_vector()\n4.49 \u00b5s \u00b1 32.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```",
    "created_at": "2021-11-19T07:19:17Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465373",
    "user": "https://github.com/kliem"
}
```

Now here are some timings of `_vector_times_matrix_` with some changes that we have some data:

As it is:

```
7.31 µs ± 30.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
174 µs ± 346 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 16.2 ms, sys: 2 µs, total: 16.2 ms
Wall time: 16.3 ms
CPU times: user 1.54 s, sys: 0 ns, total: 1.54 s
Wall time: 1.54 s
```

Reading in the vectors as if they were transposed

```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[i] + j)
```

```
7.5 µs ± 63 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
171 µs ± 256 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 19.7 ms, sys: 0 ns, total: 19.7 ms
Wall time: 19.7 ms
CPU times: user 3.36 s, sys: 60 µs, total: 3.36 s
Wall time: 3.36 s
```

Also moving in the vector:

```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + i, w_flint + i, self._matrix.rows[i] + j)
```

```
7.02 µs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
204 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 25.6 ms, sys: 0 ns, total: 25.6 ms
Wall time: 25.6 ms
CPU times: user 3.91 s, sys: 1e+03 ns, total: 3.91 s
Wall time: 3.91 s
```

Having small movements, but lots of dependencies:

```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + j, w_flint + j, self._matrix.rows[j] + i)
```

```
7.37 µs ± 34.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
146 µs ± 497 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 12.8 ms, sys: 0 ns, total: 12.8 ms
Wall time: 12.8 ms
CPU times: user 1.24 s, sys: 3.94 ms, total: 1.24 s
Wall time: 1.24 s
```

A bit more movement with dependency:

```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + j, w_flint + i, self._matrix.rows[j] + i)
```

```
6.87 µs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
202 µs ± 242 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 21.1 ms, sys: 17 µs, total: 21.1 ms
Wall time: 21.1 ms
CPU times: user 2.09 s, sys: 0 ns, total: 2.09 s
Wall time: 2.09 s
```

Now using `_fmpz_vec_dot` instead of our manual version:

```
7.19 µs ± 34 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
188 µs ± 2.71 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms
Wall time: 19.5 ms
CPU times: user 1.9 s, sys: 43 µs, total: 1.9 s
Wall time: 1.9 s
```

Apparently using `fmpz_mat_entry(self._matrix, j, i)` instead of `self._matrix.rows[j] + i` makes no difference.

And here are timings, that jump around more but avoid creating the answer vector:

```diff
diff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx
index 9872b02338..f2c6d0b01e 100644
--- a/src/sage/matrix/matrix_integer_dense.pyx
+++ b/src/sage/matrix/matrix_integer_dense.pyx
@@ -1073,33 +1073,29 @@ cdef class Matrix_integer_dense(Matrix_dense):
         ans = M.zero_vector()
 
         w_flint = _fmpz_vec_init(self._nrows)
-        ans_flint = _fmpz_vec_init(self._ncols)
+        fmpz_init(x)
 
         try:
             sig_on()
             for j in range(self._nrows):
                 fmpz_set_mpz(w_flint + j, w._entries[j])
 
-            for i in range(self._ncols):
-                fmpz_zero(ans_flint + i)
-
             # The order is crucial:
             # ``self._matrix.rows[j] + i`` is right next to ``self._matrix[j] + i + 1``
             # but far away from ``self._matrix[j + 1] + i``.
             # So in the inner loop we have very little pointer movement.
             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has
             # no depence on the previous step.
-            for j in range(self._nrows):
-                for i in range(self._ncols):
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
-
             for i in range(self._ncols):
-                fmpz_get_mpz(ans._entries[i], ans_flint + i)
+                fmpz_zero(x)
+                for j in range(self._nrows):
+                    fmpz_addmul(x, w_flint + j, self._matrix.rows[j] + i)
+                fmpz_get_mpz(ans._entries[i], x)
 
             sig_off()
         finally:
             _fmpz_vec_clear(w_flint, self._nrows)
-            _fmpz_vec_clear(ans_flint, self._ncols)
+            fmpz_clear(x)
```

```
7.02 µs ± 19.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
199 µs ± 4.07 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 24.8 ms, sys: 5 µs, total: 24.8 ms
Wall time: 24.8 ms
CPU times: user 3.8 s, sys: 16 µs, total: 3.8 s
Wall time: 3.8 s
```

Except for the first and last version, all other versions are very incorrect and just here to show what effect different changes have.

What really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:

```
sage: M = random_matrix(ZZ, 10, 10)
sage: %timeit M._column_ambient_module().zero_vector()
612 ns ± 2.81 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
sage: %timeit M._row_ambient_module().zero_vector()
4.49 µs ± 32.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```



---

archive/issue_comments_465374.json:
```json
{
    "body": "Replying to [comment:12 nbruin]:\n> Replying to [comment:9 gh-kliem]:\n> > Replying to [comment:5 nbruin]:\n> > > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.\n\n> > \n> > It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.\n> > \n> > I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.\n> > \n> > Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.\n\n> \n> In that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already \"hot\" because it is still in cache from the computation that produced it?\n> \n> Parallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.\n\n\nFor the small cases the caching and jumping around doesn't matter, as you can see above.\nFor the big cases it matters a lot. The really fast caches aren't as big as a matrix. So it would be good to reuse stuff in the cache, before loading new things. If we do things in a nice order, the cache can be filled while we are going. If we do it in a bad order, it cannot be refilled while we are going.\n\nYes, I know that parallelisism is usually better higher up. I was just playing around. Usually you want to treat lots of vectors, matrices or other objects. A matrix and vector in which the product is hard to compute by itself hardly fits in memory.",
    "created_at": "2021-11-19T07:28:00Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465374",
    "user": "https://github.com/kliem"
}
```

Replying to [comment:12 nbruin]:
> Replying to [comment:9 gh-kliem]:
> > Replying to [comment:5 nbruin]:
> > > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.

> > 
> > It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.
> > 
> > I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.
> > 
> > Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.

> 
> In that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already "hot" because it is still in cache from the computation that produced it?
> 
> Parallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.


For the small cases the caching and jumping around doesn't matter, as you can see above.
For the big cases it matters a lot. The really fast caches aren't as big as a matrix. So it would be good to reuse stuff in the cache, before loading new things. If we do things in a nice order, the cache can be filled while we are going. If we do it in a bad order, it cannot be refilled while we are going.

Yes, I know that parallelisism is usually better higher up. I was just playing around. Usually you want to treat lots of vectors, matrices or other objects. A matrix and vector in which the product is hard to compute by itself hardly fits in memory.



---

archive/issue_comments_465375.json:
```json
{
    "body": "Changing status from needs_review to needs_work.",
    "created_at": "2021-11-19T08:04:44Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465375",
    "user": "https://github.com/kliem"
}
```

Changing status from needs_review to needs_work.



---

archive/issue_comments_465376.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2021-11-19T18:57:05Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465376",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_465377.json:
```json
{
    "body": "Changing status from needs_work to needs_review.",
    "created_at": "2021-11-19T18:58:49Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465377",
    "user": "https://github.com/kliem"
}
```

Changing status from needs_work to needs_review.



---

archive/issue_comments_465378.json:
```json
{
    "body": "Replying to [comment:13 gh-kliem]:\n> What really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:\n> \n> \n> ```\n> sage: M = random_matrix(ZZ, 10, 10)\n> sage: %timeit M._column_ambient_module().zero_vector()\n> 612 ns \u00b1 2.81 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n> sage: %timeit M._row_ambient_module().zero_vector()\n> 4.49 \u00b5s \u00b1 32.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n> ```\n\n\nYes, that's ridiculously bad, but we can see where that's coming from: there's a difference in implementation between the two:\n\n```\nsage: M._row_ambient_module??                                                                                                              \nSource:\n    def _row_ambient_module(self, base_ring=None):\n        if base_ring is None:\n            base_ring = self.base_ring()\n        x = self.fetch('row_ambient_module_%s'%base_ring)\n        if not x is None:\n            return x\n        x = sage.modules.free_module.FreeModule(base_ring, self.ncols(), sparse=self.is_sparse())\n        self.cache('row_ambient_module',x)\n        return x\nFile:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx\nType:   builtin_function_or_method\nsage: M._column_ambient_module??                                                                                                           \nSource:\n    def _column_ambient_module(self):\n        x = self.fetch('column_ambient_module')\n        if not x is None:\n            return x\n        x = sage.modules.free_module.FreeModule(self.base_ring(), self.nrows(),\n                                                sparse=self.is_sparse())\n        self.cache('column_ambient_module',x)\n        return x\nFile:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx\nType:   builtin_function_or_method\n```\nAs you can see, `_row_ambient_module` is doing string formatting; even in its fast path! So, I think what needs to happen here:\n- identify if `_row_ambient_module` even needs a `base_ring` optional argument. It seems to me this is not desirable. If it can simply be removed, that should happen.\n- determine testing the `None` argument (or having it!) is measurable. If so, split out between a fast path routine without a base ring optional parameter and another one that does allow it. It looks like the only place where this gets used is in `row_module` just below it, so that could just do what it needs to do there (cached or not)\n- make sure the caching actually WORKS! not that the code presently writes a different cache entry than what it uses. So I think the main time difference you're measuring at the moment is the expensive `UniqueRepresentation` lookup  that happens in the `FreeModule` invocation that happens every time.\n\nI suspect that fixing the third one will bring it much more in line already but that working with a string constant for cache entry lookup will still be measurable. Certainly, we should not be using string formatting if we're just concatenating two strings either (and note that getting a string representation of a base ring could be quite expensive too)\n\nFurthermore, I think these routines should be `cdef` accessible as well.\n\nFinally, it might be worth looking into cached_method lookups for these. Especially for parameterless methods, those should be pretty efficient.",
    "created_at": "2021-11-19T22:32:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465378",
    "user": "https://github.com/nbruin"
}
```

Replying to [comment:13 gh-kliem]:
> What really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:
> 
> 
> ```
> sage: M = random_matrix(ZZ, 10, 10)
> sage: %timeit M._column_ambient_module().zero_vector()
> 612 ns ± 2.81 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
> sage: %timeit M._row_ambient_module().zero_vector()
> 4.49 µs ± 32.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
> ```


Yes, that's ridiculously bad, but we can see where that's coming from: there's a difference in implementation between the two:

```
sage: M._row_ambient_module??                                                                                                              
Source:
    def _row_ambient_module(self, base_ring=None):
        if base_ring is None:
            base_ring = self.base_ring()
        x = self.fetch('row_ambient_module_%s'%base_ring)
        if not x is None:
            return x
        x = sage.modules.free_module.FreeModule(base_ring, self.ncols(), sparse=self.is_sparse())
        self.cache('row_ambient_module',x)
        return x
File:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx
Type:   builtin_function_or_method
sage: M._column_ambient_module??                                                                                                           
Source:
    def _column_ambient_module(self):
        x = self.fetch('column_ambient_module')
        if not x is None:
            return x
        x = sage.modules.free_module.FreeModule(self.base_ring(), self.nrows(),
                                                sparse=self.is_sparse())
        self.cache('column_ambient_module',x)
        return x
File:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx
Type:   builtin_function_or_method
```
As you can see, `_row_ambient_module` is doing string formatting; even in its fast path! So, I think what needs to happen here:
- identify if `_row_ambient_module` even needs a `base_ring` optional argument. It seems to me this is not desirable. If it can simply be removed, that should happen.
- determine testing the `None` argument (or having it!) is measurable. If so, split out between a fast path routine without a base ring optional parameter and another one that does allow it. It looks like the only place where this gets used is in `row_module` just below it, so that could just do what it needs to do there (cached or not)
- make sure the caching actually WORKS! not that the code presently writes a different cache entry than what it uses. So I think the main time difference you're measuring at the moment is the expensive `UniqueRepresentation` lookup  that happens in the `FreeModule` invocation that happens every time.

I suspect that fixing the third one will bring it much more in line already but that working with a string constant for cache entry lookup will still be measurable. Certainly, we should not be using string formatting if we're just concatenating two strings either (and note that getting a string representation of a base ring could be quite expensive too)

Furthermore, I think these routines should be `cdef` accessible as well.

Finally, it might be worth looking into cached_method lookups for these. Especially for parameterless methods, those should be pretty efficient.



---

archive/issue_comments_465379.json:
```json
{
    "body": "Let's move this discussion to #32901.",
    "created_at": "2021-11-19T22:38:16Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465379",
    "user": "https://github.com/kliem"
}
```

Let's move this discussion to #32901.



---

archive/issue_comments_465380.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2021-11-30T08:14:18Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465380",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_465381.json:
```json
{
    "body": "There are failing doctest. Not directly related to this ticket, but appearently something goes wrong:\n\nOn develop:\n\n```\nsage: dP6 = toric_varieties.dP6()\nsage: Kc = dP6.Kaehler_cone()\nsage: D = Kc.ray(1)\nsage: p = D.parent()\nsage: M = p._lift_matrix\nsage: M*D\n(1, 0, 0, 0, 0, 1)\nsage: D*M.transpose()\n(1, 2, 0, 0, -1, 0)\nsage: vector(list(D))*M.transpose()\n(1, 0, 0, 0, 0, 1)\n```\n\nNow I implement an innocent method for `Matrix_integer_dense`:\n\n```\n    cdef _matrix_times_vector_(self, Vector v):\n        cdef Matrix_integer_dense transposed = self.transpose()\n        return transposed._vector_times_matrix_(v)\n```\n\nAnd the result is different now:\n\n```\nsage: dP6 = toric_varieties.dP6()\nsage: Kc = dP6.Kaehler_cone()\nsage: D = Kc.ray(1)\nsage: p = D.parent()\nsage: M = p._lift_matrix\nsage: M*D\n(1, 2, 0, 0, -1, 0)\n```\n\nApparently this is the problem:\n\n```\nsage: D.base_ring()\nInteger Ring\n```\n\nSo `D` is set up wrong. It inherits from `Vector_rational_dense` but the base ring is wrong and the parent is at least strange. I think the parent should have the same base ring as the element.\n\nThe above failure is due to an incorrect type case.",
    "created_at": "2021-12-07T11:17:52Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465381",
    "user": "https://github.com/kliem"
}
```

There are failing doctest. Not directly related to this ticket, but appearently something goes wrong:

On develop:

```
sage: dP6 = toric_varieties.dP6()
sage: Kc = dP6.Kaehler_cone()
sage: D = Kc.ray(1)
sage: p = D.parent()
sage: M = p._lift_matrix
sage: M*D
(1, 0, 0, 0, 0, 1)
sage: D*M.transpose()
(1, 2, 0, 0, -1, 0)
sage: vector(list(D))*M.transpose()
(1, 0, 0, 0, 0, 1)
```

Now I implement an innocent method for `Matrix_integer_dense`:

```
    cdef _matrix_times_vector_(self, Vector v):
        cdef Matrix_integer_dense transposed = self.transpose()
        return transposed._vector_times_matrix_(v)
```

And the result is different now:

```
sage: dP6 = toric_varieties.dP6()
sage: Kc = dP6.Kaehler_cone()
sage: D = Kc.ray(1)
sage: p = D.parent()
sage: M = p._lift_matrix
sage: M*D
(1, 2, 0, 0, -1, 0)
```

Apparently this is the problem:

```
sage: D.base_ring()
Integer Ring
```

So `D` is set up wrong. It inherits from `Vector_rational_dense` but the base ring is wrong and the parent is at least strange. I think the parent should have the same base ring as the element.

The above failure is due to an incorrect type case.



---

archive/issue_comments_465382.json:
```json
{
    "body": "Changing status from needs_review to needs_work.",
    "created_at": "2021-12-07T11:17:52Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465382",
    "user": "https://github.com/kliem"
}
```

Changing status from needs_review to needs_work.



---

archive/issue_comments_465383.json:
```json
{
    "body": "Fixing this failure is the job of #32991.\n\nFor this ticket, I'll just do a workaround for the doctest.",
    "created_at": "2021-12-07T13:25:56Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465383",
    "user": "https://github.com/kliem"
}
```

Fixing this failure is the job of #32991.

For this ticket, I'll just do a workaround for the doctest.



---

archive/issue_comments_465384.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:",
    "created_at": "2021-12-07T13:32:29Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465384",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:



---

archive/issue_comments_465385.json:
```json
{
    "body": "Changing status from needs_work to needs_review.",
    "created_at": "2021-12-07T13:32:35Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465385",
    "user": "https://github.com/kliem"
}
```

Changing status from needs_work to needs_review.



---

archive/issue_comments_465386.json:
```json
{
    "body": "Ready for review. The bots are morally green now.",
    "created_at": "2021-12-10T07:29:09Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465386",
    "user": "https://github.com/kliem"
}
```

Ready for review. The bots are morally green now.



---

archive/issue_events_086946.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2021-12-27T01:03:43Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "milestone": "sage-9.6",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/32895#event-86946"
}
```



---

archive/issue_comments_465387.json:
```json
{
    "body": "Could you rebase on #32984?",
    "created_at": "2021-12-27T15:45:36Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465387",
    "user": "https://github.com/videlec"
}
```

Could you rebase on #32984?



---

archive/issue_comments_465388.json:
```json
{
    "body": "Shouldn't the `sig_off` be inside the `finally`?",
    "created_at": "2021-12-27T15:46:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465388",
    "user": "https://github.com/videlec"
}
```

Shouldn't the `sig_off` be inside the `finally`?



---

archive/issue_comments_465389.json:
```json
{
    "body": "Replying to [comment:29 vdelecroix]:\n> Shouldn't the `sig_off` be inside the `finally`?\n\n\nI don't think so. If there is an error, it will jump back to `sig_on` and raise an error there. In this case it should not call `sig_off`.",
    "created_at": "2021-12-27T16:49:11Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465389",
    "user": "https://github.com/kliem"
}
```

Replying to [comment:29 vdelecroix]:
> Shouldn't the `sig_off` be inside the `finally`?


I don't think so. If there is an error, it will jump back to `sig_on` and raise an error there. In this case it should not call `sig_off`.



---

archive/issue_comments_465390.json:
```json
{
    "body": "Replying to [comment:28 vdelecroix]:\n> Could you rebase on #32984?\n\n\nCan't do that ATM. Once #32984 is positively reviewed, feel free to do so.",
    "created_at": "2021-12-27T16:51:42Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465390",
    "user": "https://github.com/kliem"
}
```

Replying to [comment:28 vdelecroix]:
> Could you rebase on #32984?


Can't do that ATM. Once #32984 is positively reviewed, feel free to do so.



---

archive/issue_comments_465391.json:
```json
{
    "body": "Or just put this here on needs work. Whatever works.",
    "created_at": "2021-12-27T16:52:54Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465391",
    "user": "https://github.com/kliem"
}
```

Or just put this here on needs work. Whatever works.



---

archive/issue_comments_465392.json:
```json
{
    "body": "This following tests\n\n```\n            sage: A = random_matrix(QQ, 100)\n            sage: v = random_vector(QQ, 100)\n            sage: A*v == v*A.transpose()\n            True\n```\nwould better be part of a generic `TestSuite` rather than inside doctests. Though, in this case it would make sense to parametrize the size. `100 x 100` over rationals is fine but over `QQ[x]` this is unlikely to finish.",
    "created_at": "2021-12-28T13:08:25Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465392",
    "user": "https://github.com/videlec"
}
```

This following tests

```
            sage: A = random_matrix(QQ, 100)
            sage: v = random_vector(QQ, 100)
            sage: A*v == v*A.transpose()
            True
```
would better be part of a generic `TestSuite` rather than inside doctests. Though, in this case it would make sense to parametrize the size. `100 x 100` over rationals is fine but over `QQ[x]` this is unlikely to finish.



---

archive/issue_comments_465393.json:
```json
{
    "body": "Changing status from needs_review to needs_info.",
    "created_at": "2021-12-28T16:29:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465393",
    "user": "https://github.com/videlec"
}
```

Changing status from needs_review to needs_info.



---

archive/issue_comments_465394.json:
```json
{
    "body": "I don't think that having this kind of optimization on flint data structures inside sage is a good idea. The underlying design of flint is to only expose type names (ie `fmpz_mat_t` and `fmpq_mat_t`) and interfaces (ie functions `fmpz_mat_mul` and `fmpq_mat_mul`). It would make more sense to make a merge request to [flint developers](https://github.com/wbhart/flint2) and just use plain flint code inside sage. Doing so would\n- benefit to all flint users and not only sagemath users\n- only use the public API of flint (so that flint upgrades will remain smooth)",
    "created_at": "2021-12-28T16:29:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465394",
    "user": "https://github.com/videlec"
}
```

I don't think that having this kind of optimization on flint data structures inside sage is a good idea. The underlying design of flint is to only expose type names (ie `fmpz_mat_t` and `fmpq_mat_t`) and interfaces (ie functions `fmpz_mat_mul` and `fmpq_mat_mul`). It would make more sense to make a merge request to [flint developers](https://github.com/wbhart/flint2) and just use plain flint code inside sage. Doing so would
- benefit to all flint users and not only sagemath users
- only use the public API of flint (so that flint upgrades will remain smooth)



---

archive/issue_comments_465395.json:
```json
{
    "body": "Branch pushed to git repo; I updated commit sha1. New commits:",
    "created_at": "2022-01-03T21:03:04Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465395",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Branch pushed to git repo; I updated commit sha1. New commits:



---

archive/issue_comments_465396.json:
```json
{
    "body": "I removed the explicit dependence on the flint structures.\n\nI don't know how good this would fit into flint and whether we should force and brand new flint version because of it.\n\nIf we use a brand new feature of flint, it means lots of compile time for many people.\n\nPutting this into flint, might not be what we desire:\n- We use GMP-vectors instead of FLINT-vectors.\n- The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.\n\nThose are some of the issues, I can think of. I sure can try putting it in FLINT.\n\nThe doctest is mainly there for coverage (and of course as sanity check).",
    "created_at": "2022-01-03T21:10:36Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465396",
    "user": "https://github.com/kliem"
}
```

I removed the explicit dependence on the flint structures.

I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.

If we use a brand new feature of flint, it means lots of compile time for many people.

Putting this into flint, might not be what we desire:
- We use GMP-vectors instead of FLINT-vectors.
- The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.

Those are some of the issues, I can think of. I sure can try putting it in FLINT.

The doctest is mainly there for coverage (and of course as sanity check).



---

archive/issue_comments_465397.json:
```json
{
    "body": "Changing status from needs_info to needs_review.",
    "created_at": "2022-01-03T21:10:36Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465397",
    "user": "https://github.com/kliem"
}
```

Changing status from needs_info to needs_review.



---

archive/issue_comments_465398.json:
```json
{
    "body": "Replying to [comment:36 gh-kliem]:\n> I removed the explicit dependence on the flint structures.\n> \n> I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.\n> \n> If we use a brand new feature of flint, it means lots of compile time for many people.\n\n\nWhy is that ? The inclusion in sage might wait for a flint release to appear.\n \n> Putting this into flint, might not be what we desire:\n> - We use GMP-vectors instead of FLINT-vectors\n\n\nMight make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.\n\nAlternatively, the following could directly be proposed upstream\n\n```\nvoid fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)\n```\n\n> - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.\n\n\n`fmpz_set_mpz` might allocate memory...\n\n> Those are some of the issues, I can think of. I sure can try putting it in FLINT.\n> \n> The doctest is mainly there for coverage (and of course as sanity check).",
    "created_at": "2022-01-04T09:40:22Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465398",
    "user": "https://github.com/videlec"
}
```

Replying to [comment:36 gh-kliem]:
> I removed the explicit dependence on the flint structures.
> 
> I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.
> 
> If we use a brand new feature of flint, it means lots of compile time for many people.


Why is that ? The inclusion in sage might wait for a flint release to appear.
 
> Putting this into flint, might not be what we desire:
> - We use GMP-vectors instead of FLINT-vectors


Might make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.

Alternatively, the following could directly be proposed upstream

```
void fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)
```

> - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.


`fmpz_set_mpz` might allocate memory...

> Those are some of the issues, I can think of. I sure can try putting it in FLINT.
> 
> The doctest is mainly there for coverage (and of course as sanity check).



---

archive/issue_comments_465399.json:
```json
{
    "body": "Replying to [comment:37 vdelecroix]:\n> Replying to [comment:36 gh-kliem]:\n> > I removed the explicit dependence on the flint structures.\n> > \n> > I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.\n> > \n> > If we use a brand new feature of flint, it means lots of compile time for many people.\n\n> \n> Why is that ? The inclusion in sage might wait for a flint release to appear.\n\n\nIf we use new functions that were introduced in the latest version of flint, we will have to raise the requirements in `spkg-configure`. Currently, debian stable has flint 2.6.3, which will be accepted by sage (and the situation should be similar for other systems). If we raise this to the most recent version, almost everyone building sage will have to build flint as well for a while. That is fine, if there is some great new feature, but `matrix_times_vector` can be achieved with the existing FLINT Api without much work. This would be what I would expect `matrix_times_vector` to take care of:\n\n```diff\n+            for i in range(self._nrows):\n+                fmpz_zero(x)\n+                for j in range(self._ncols):\n+                    fmpz_addmul(x, w_flint + j, fmpz_mat_entry(self._matrix, i, j))\n+                fmpz_get_mpz(ans._entries[i], x)\n```\n\n>  \n\n> > Putting this into flint, might not be what we desire:\n> > - We use GMP-vectors instead of FLINT-vectors\n \n> \n> Might make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.\n> \n> Alternatively, the following could directly be proposed upstream\n> \n> ```\n> void fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)\n> ```\n> \n> > - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.\n \n> \n> `fmpz_set_mpz` might allocate memory...\n\n\nThat is different though. The memory is accounted for, and when calling `mpz_clear` it will be released (this requires that allocating memory and accounting for it is atomic, I don't know if this is the case but even if not and interrupt in between is not very likely). Whereas something as\n\n```\nsig_on()\nvoid* foo = malloc(100)\n# Do something\nfree(foo)\nsig_off()\n```\nis not clean at all. `free(foo)` will not be called in case of interrupt. If we wrap a function that behaves like this with `sig_on`, `sig_off` an interrupt will usually result in memory leak and there is nothing we can do about it.\n\nHowever, if all flint does is multiplication of a flint matrix and a flint vector and assign this to a flint vector, it will work just fine (or as fine as other code).\n> \n> > Those are some of the issues, I can think of. I sure can try putting it in FLINT.\n> > \n> > The doctest is mainly there for coverage (and of course as sanity check).",
    "created_at": "2022-01-05T07:11:01Z",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/32895#issuecomment-465399",
    "user": "https://github.com/kliem"
}
```

Replying to [comment:37 vdelecroix]:
> Replying to [comment:36 gh-kliem]:
> > I removed the explicit dependence on the flint structures.
> > 
> > I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.
> > 
> > If we use a brand new feature of flint, it means lots of compile time for many people.

> 
> Why is that ? The inclusion in sage might wait for a flint release to appear.


If we use new functions that were introduced in the latest version of flint, we will have to raise the requirements in `spkg-configure`. Currently, debian stable has flint 2.6.3, which will be accepted by sage (and the situation should be similar for other systems). If we raise this to the most recent version, almost everyone building sage will have to build flint as well for a while. That is fine, if there is some great new feature, but `matrix_times_vector` can be achieved with the existing FLINT Api without much work. This would be what I would expect `matrix_times_vector` to take care of:

```diff
+            for i in range(self._nrows):
+                fmpz_zero(x)
+                for j in range(self._ncols):
+                    fmpz_addmul(x, w_flint + j, fmpz_mat_entry(self._matrix, i, j))
+                fmpz_get_mpz(ans._entries[i], x)
```

>  

> > Putting this into flint, might not be what we desire:
> > - We use GMP-vectors instead of FLINT-vectors
 
> 
> Might make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.
> 
> Alternatively, the following could directly be proposed upstream
> 
> ```
> void fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)
> ```
> 
> > - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.
 
> 
> `fmpz_set_mpz` might allocate memory...


That is different though. The memory is accounted for, and when calling `mpz_clear` it will be released (this requires that allocating memory and accounting for it is atomic, I don't know if this is the case but even if not and interrupt in between is not very likely). Whereas something as

```
sig_on()
void* foo = malloc(100)
# Do something
free(foo)
sig_off()
```
is not clean at all. `free(foo)` will not be called in case of interrupt. If we wrap a function that behaves like this with `sig_on`, `sig_off` an interrupt will usually result in memory leak and there is nothing we can do about it.

However, if all flint does is multiplication of a flint matrix and a flint vector and assign this to a flint vector, it will work just fine (or as fine as other code).
> 
> > Those are some of the issues, I can think of. I sure can try putting it in FLINT.
> > 
> > The doctest is mainly there for coverage (and of course as sanity check).



---

archive/issue_events_086947.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-04-02T17:54:54Z",
    "event": "demilestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "milestone": "sage-9.6",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/32895#event-86947"
}
```



---

archive/issue_events_086948.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-04-02T17:54:54Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "milestone": "sage-9.7",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/32895#event-86948"
}
```



---

archive/issue_events_086949.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-08-31T05:25:02Z",
    "event": "demilestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "milestone": "sage-9.7",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/32895#event-86949"
}
```



---

archive/issue_events_086950.json:
```json
{
    "actor": "https://github.com/mkoeppe",
    "created_at": "2022-08-31T05:25:02Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/32895",
    "milestone": "sage-9.8",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/32895#event-86950"
}
```
