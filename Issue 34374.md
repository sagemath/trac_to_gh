# Issue 34374: fast implementation of exp

Issue created by migration from https://trac.sagemath.org/ticket/34611

Original creator: mantepse

Original creation time: 2022-09-29 10:00:15

CC:  tscrim




---

Comment by mantepse created at 2022-09-29 10:13:23

Changing component from PLEASE CHANGE to combinatorics.


---

Comment by mantepse created at 2022-09-29 10:13:23

Last 10 new commits:


---

Comment by mantepse created at 2022-09-29 10:13:23

Changing type from PLEASE CHANGE to enhancement.


---

Comment by mantepse created at 2022-09-29 10:13:23

Changing keywords from "" to "LazyPowerSeries".


---

Comment by mantepse created at 2022-09-29 10:14:18

Here is a comparison:

```
sage: L.<z> = LazyLaurentSeriesRing(QQ)
sage: f = L(lambda n: randint(1, 100), valuation=1)
sage: g = exp(f)
sage: %time a = g[200]
CPU times: user 86.4 ms, sys: 29 µs, total: 86.4 ms
Wall time: 86.4 ms

sage: e = L(coefficients=lambda n: 1/factorial(ZZ(n)), valuation=0)
sage: h = e(f)
sage: %time b = h[200]
CPU times: user 1.7 s, sys: 20 µs, total: 1.7 s
Wall time: 1.7 s

sage: a == b
True
```



---

Comment by git created at 2022-09-29 14:02:32

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by mantepse created at 2022-10-10 19:18:00

There is a slight problem:

```
sage: L.<z> = LazyLaurentSeriesRing(QQ)
sage: f = L(lambda n: randint(1, 10), valuation=1)
sage: g = exp(f)
sage: %time _ = g[500]
...
RecursionError: maximum recursion depth exceeded while calling a Python object
```



---

Comment by mantepse created at 2022-10-10 19:57:01

This is not a problem if we use dense series (of course) - so there are at least three solutions:

* we could create a dense ring in `exp`,

* we could modify `Stream_uninitialized.get_coefficient` to compute coefficients in order,

* we could ignore the problem, and tell the user to use dense series.


---

Comment by tscrim created at 2022-10-11 05:39:17

There is also another solution: Tell the user to compute some earlier coefficients:

```
sage: L.<z> = LazyLaurentSeriesRing(QQ)
sage: f = L(lambda n: randint(1, 10), valuation=1)
sage: g = exp(f)
sage: %time _ = g[100]
CPU times: user 8.87 ms, sys: 18 µs, total: 8.88 ms
Wall time: 8.74 ms
sage: %time _ = g[200]
CPU times: user 38.1 ms, sys: 0 ns, total: 38.1 ms
Wall time: 37.8 ms
sage: %time _ = g[500]
CPU times: user 678 ms, sys: 0 ns, total: 678 ms
Wall time: 678 ms
```


However, I like option (2) the best because there is not a reasonable situation where we would not essentially end up computing all of the coefficients (ultimately in order):

```
sage: len(g._coeff_stream._cache)
501
```



---

Comment by mantepse created at 2022-10-11 06:58:16

A similar technique works for computing the Cauchy inverse.


---

Comment by tscrim created at 2022-10-11 07:40:57

We might be able to limit the number of recursion calls by being careful about the order of multiplication too.


---

Comment by mantepse created at 2022-10-11 08:03:19

You mean `Stream_cauchy_mul(d_self, f._coeff_stream)` vs. `Stream_cauchy_mul(f._coeff_stream, d_self)`?

All of this may also be affected by #34616.


---

Comment by tscrim created at 2022-10-11 08:34:59

Yes, that's right. Although since #34616 might only apply to the dense case, it might not be relevant to the issue we have with the sparse case.


---

Comment by mantepse created at 2022-10-11 20:25:36

What do you think about making `Stream_uninitialized` always dense.  This is essentially the same as (2) above, just with less overhead.

This could be done either here or in #34636, which is ready for review. I could then make #34636 a dependency for this ticket.  I could even do a doctest there, because the problem arises already with, for example, the implicit definition of the Catalan numbers:

```
sage: L.<z> = LazyPowerSeriesRing(ZZ); C = L.undefined(); C.define(1 + z*C^2)
sage: %time _ = C[500]
...
RecursionError: maximum recursion depth exceeded while calling a Python object

sage: L.<z> = LazyPowerSeriesRing(ZZ, sparse=True); C = L.undefined(); C.define(1 + z*C^2)
sage: %time _ = C[350]
CPU times: user 201 ms, sys: 0 ns, total: 201 ms
Wall time: 201 ms
sage: L.<z> = LazyPowerSeriesRing(ZZ, sparse=False); C = L.undefined(); C.define(1 + z*C^2)
sage: %time _ = C[350]
CPU times: user 160 ms, sys: 0 ns, total: 160 ms
Wall time: 160 ms
```



---

Comment by tscrim created at 2022-10-12 08:48:28

Replying to [comment:13 Martin Rubey]:
> What do you think about making `Stream_uninitialized` always dense.  This is essentially the same as (2) above, just with less overhead.

+1

> This could be done either here or in #34636, which is ready for review. I could then make #34636 a dependency for this ticket.

That would probably be better.


---

Comment by git created at 2022-10-13 11:10:17

Branch pushed to git repo; I updated commit sha1. Last 10 new commits:


---

Comment by mantepse created at 2022-10-13 11:12:32

Changing status from new to needs_review.


---

Comment by mantepse created at 2022-10-13 11:12:32

A fast algorithm for inversion can be found in van der Hoeven's paper, but is not completely trivial to implement, so it should be left for another ticket.


---

Comment by mantepse created at 2022-10-13 15:40:05

As an aside, it should be very easy to get rid of the duplicated cache in `Stream_uninitialized`: currently we have a cache for `self._target`, and create another one for `self`.  I have an experimental branch, which removes `_target`, and simply sets `self._iter` to the iterator of the target.  However, I get a strange doctest failure when there are several interlinked uninitialized streams which, additionally, have positive valuation.  I don't understand the problem yet.  However, considering #34616, we probably want a slightly more advanced cache anyway, i.e., a cache whose size grows in chunks and not just one by one.  So it may be better to consider these things together.


---

Comment by tscrim created at 2022-10-28 07:18:31

Changing status from needs_review to positive_review.


---

Comment by tscrim created at 2022-10-28 07:18:31

Or on a separate ticket altogether.

I think this is good to go for now as it greatly improves the performance (asymptotically; I didn't check for small values, but there is likely at worst a negligible difference).


---

Comment by vbraun created at 2022-11-07 20:25:52

Resolution: fixed
