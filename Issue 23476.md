# Issue 23476: Parallel map/reduce doctests fail on machine with lots of CPUs

Issue created by migration from https://trac.sagemath.org/ticket/23713

Original creator: jdemeyer

Original creation time: 2017-08-25 13:56:18

CC:  hivert

The number of files that the parallel map/reduce doctests requires is `O(N)` where `N` is the number of processors, while the number of open files allowed is typically `O(1)`. So, on a machine with 192 processors:

```
sage -t --long --warn-long 91.0 src/sage/parallel/map_reduce.py
**********************************************************************
File "src/sage/parallel/map_reduce.py", line 228, in sage.parallel.map_reduce
Failed example:
    try:
        res = EX.run(timeout=60)
    except AbortError:
        print("Computation Timeout")
    else:
        print("Computation normally finished")
        res
Exception raised:
    Traceback (most recent call last):
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/site-packages/sage/doctest/forker.py", line 515, in _run
        self.compile_and_execute(example, compiler, test.globs)
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/site-packages/sage/doctest/forker.py", line 885, in compile_and_execute
        exec(compiled, globs)
      File "<doctest sage.parallel.map_reduce[33]>", line 2, in <module>
        res = EX.run(timeout=Integer(60))
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/site-packages/sage/parallel/map_reduce.py", line 1394, in run
        self.setup_workers(max_proc, reduce_locally)
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/site-packages/sage/parallel/map_reduce.py", line 1070, in setup_workers
        for i in range(self._nprocess)]
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/site-packages/sage/parallel/map_reduce.py", line 1513, in __init__
        self._request = SimpleQueue()  # Faster than Queue
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/multiprocessing/queues.py", line 353, in __init__
        self._reader, self._writer = Pipe(duplex=False)
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/multiprocessing/__init__.py", line 107, in Pipe
        return Pipe(duplex)
      File "/home/jdemeyer/sage-patchbot/local/lib/python2.7/multiprocessing/connection.py", line 196, in Pipe
        fd1, fd2 = os.pipe()
    OSError: [Errno 24] Too many open files
**********************************************************************
```


A simple solution would be to restrict the number of processors while doctesting.


---

Comment by jdemeyer created at 2017-09-04 12:05:43

New commits:


---

Comment by jdemeyer created at 2017-09-04 12:05:43

Changing status from new to needs_review.


---

Comment by jdemeyer created at 2017-10-05 10:17:46

Changing priority from critical to blocker.


---

Comment by jdemeyer created at 2017-10-05 10:17:46

Changing component from doctest coverage to doctest framework.


---

Comment by jdemeyer created at 2017-10-05 10:17:46

Changing status from needs_review to needs_work.


---

Comment by git created at 2017-10-05 10:32:57

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by jdemeyer created at 2017-10-05 11:08:38

Changing status from needs_work to needs_review.


---

Comment by hivert created at 2017-10-05 13:04:55

Hi Jeroen, 

Thanks for taking care of this one (and all the others) ! It's good to go for me. I assume that the limit `SAGE_NUM_THREADS=2` si set up to be able to doctests on small machines. It seems a little low to me. I'd rather put 4 just to shake up a little the parallel feature. Except for that, everything loos ok to me.

Florent


---

Comment by hivert created at 2017-10-05 13:04:55

Changing status from needs_review to positive_review.


---

Comment by vbraun created at 2017-10-15 11:45:12

Resolution: fixed


---

Comment by egourgoulhon created at 2018-05-26 13:47:17

See #24937 for a follow up.
