# Issue 28512: reading a large expression from a file takes quadratic time

archive/issues_028275.json:
```json
{
    "body": "consider the following with Sage 8.8:\n\n```\nsage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]\nsage: p = R.random_element(degree=10,terms=10^4)\nsage: p.number_of_terms()\n7983\nsage: f = open(\"p.sage\",\"w\")\nsage: f.write(\"p=\"+str(p)+\"\\n\")\nsage: f.close()\nsage: t = cputime()\nsage: load(\"p.sage\")\nsage: cputime()-t\n3.1715750000000185\nsage: q = R.random_element(degree=10,terms=2*10^4)\nsage: q.number_of_terms()\n16055\nsage: f = open(\"q.sage\",\"w\")\nsage: f.write(\"q=\"+str(q)+\"\\n\")\nsage: f.close()\nsage: t = cputime()\nsage: load(\"q.sage\")\nsage: cputime()-t\n14.55525400000002\n```\nWe see that for a polynomial that is almost twice as large, the time to read the file is multiplied by a factor larger than 4! This makes it impossible to work with very large files.\n\nReviewer: Paul Zimmermann\n\nAuthor: Nils Bruin\n\nBranch: 94ae44653baaac3eb558ea9fdcb5c66ebde8510e\n\nResolution: fixed\n\nIssue created by migration from https://trac.sagemath.org/ticket/28512\n\n",
    "closed_at": "2019-10-05T07:58:06Z",
    "created_at": "2019-09-17T14:24:06Z",
    "labels": [
        "component: performance",
        "critical",
        "bug"
    ],
    "milestone": "https://github.com/sagemath/sagetest/milestones/sage-9.0",
    "title": "reading a large expression from a file takes quadratic time",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/28512",
    "user": "https://github.com/zimmermann6"
}
```
consider the following with Sage 8.8:

```
sage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]
sage: p = R.random_element(degree=10,terms=10^4)
sage: p.number_of_terms()
7983
sage: f = open("p.sage","w")
sage: f.write("p="+str(p)+"\n")
sage: f.close()
sage: t = cputime()
sage: load("p.sage")
sage: cputime()-t
3.1715750000000185
sage: q = R.random_element(degree=10,terms=2*10^4)
sage: q.number_of_terms()
16055
sage: f = open("q.sage","w")
sage: f.write("q="+str(q)+"\n")
sage: f.close()
sage: t = cputime()
sage: load("q.sage")
sage: cputime()-t
14.55525400000002
```
We see that for a polynomial that is almost twice as large, the time to read the file is multiplied by a factor larger than 4! This makes it impossible to work with very large files.

Reviewer: Paul Zimmermann

Author: Nils Bruin

Branch: 94ae44653baaac3eb558ea9fdcb5c66ebde8510e

Resolution: fixed

Issue created by migration from https://trac.sagemath.org/ticket/28512





---

archive/issue_comments_554449.json:
```json
{
    "body": "<a id='comment:1'></a>\nI'm not sure there's much we can do about it via this route, since the semantics of python prescribe that `a1+a2+a3+a4` gets evaluated as `((a1+a2)+a3)+a4`, which, due to the creation of intermediate results, should be quadratic in the number of terms [note that you're really just writing a python expression to a file and evaluating that] You could explicitly write your expression in balanced form to avoid this problem:\n\n```\ndef balanced_str(p):\n  return balanced_sum([c*t for c,t in list(p)])\ndef balanced_sum(L):\n  if len(L) <= 4:\n    return \"+\".join([str(a) for a in L])\n  else:\n    n= len(L) // 2\n    return \"(%s)+(%s)\"%(balanced_sum(L[:n]),balanced_sum(L[n:]))\n```\nWe can test this with some of your polynomials:\n\n```\nsage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]\nsage: Ss=[balanced_str(p) for p in Ps]\nsage: %time _=R(Ss[0])\nCPU times: user 139 ms, sys: 7.03 ms, total: 146 ms\nWall time: 143 ms\nsage: %time _=R(Ss[1])\nCPU times: user 245 ms, sys: 22 ms, total: 267 ms\nWall time: 262 ms\nsage: %time _=R(Ss[2])\nCPU times: user 487 ms, sys: 42.1 ms, total: 529 ms\nWall time: 519 ms\n```\nA little more worrisome is that for converting dictionary representation back into polynomials, the same quadratic behaviour occurs:\n\n```\nsage: Ds=[p.dict() for p in Ps]\nsage: %time _=R(Ds[0])\nCPU times: user 195 ms, sys: 0 ns, total: 195 ms\nWall time: 196 ms\nsage: %time _=R(Ds[1])\nCPU times: user 863 ms, sys: 0 ns, total: 863 ms\nWall time: 864 ms\nsage: %time _=R(Ds[2])\nCPU times: user 3.7 s, sys: 0 ns, total: 3.7 s\nWall time: 3.7 s\n```\nThat's rather hard to defend. That should definitely be fixed. Constructing a polynomial from a dictionary representation should be one of the fastest ways of constructing a polynomial.",
    "created_at": "2019-09-17T21:52:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554449",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:1'></a>
I'm not sure there's much we can do about it via this route, since the semantics of python prescribe that `a1+a2+a3+a4` gets evaluated as `((a1+a2)+a3)+a4`, which, due to the creation of intermediate results, should be quadratic in the number of terms [note that you're really just writing a python expression to a file and evaluating that] You could explicitly write your expression in balanced form to avoid this problem:

```
def balanced_str(p):
  return balanced_sum([c*t for c,t in list(p)])
def balanced_sum(L):
  if len(L) <= 4:
    return "+".join([str(a) for a in L])
  else:
    n= len(L) // 2
    return "(%s)+(%s)"%(balanced_sum(L[:n]),balanced_sum(L[n:]))
```
We can test this with some of your polynomials:

```
sage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]
sage: Ss=[balanced_str(p) for p in Ps]
sage: %time _=R(Ss[0])
CPU times: user 139 ms, sys: 7.03 ms, total: 146 ms
Wall time: 143 ms
sage: %time _=R(Ss[1])
CPU times: user 245 ms, sys: 22 ms, total: 267 ms
Wall time: 262 ms
sage: %time _=R(Ss[2])
CPU times: user 487 ms, sys: 42.1 ms, total: 529 ms
Wall time: 519 ms
```
A little more worrisome is that for converting dictionary representation back into polynomials, the same quadratic behaviour occurs:

```
sage: Ds=[p.dict() for p in Ps]
sage: %time _=R(Ds[0])
CPU times: user 195 ms, sys: 0 ns, total: 195 ms
Wall time: 196 ms
sage: %time _=R(Ds[1])
CPU times: user 863 ms, sys: 0 ns, total: 863 ms
Wall time: 864 ms
sage: %time _=R(Ds[2])
CPU times: user 3.7 s, sys: 0 ns, total: 3.7 s
Wall time: 3.7 s
```
That's rather hard to defend. That should definitely be fixed. Constructing a polynomial from a dictionary representation should be one of the fastest ways of constructing a polynomial.



---

archive/issue_comments_554450.json:
```json
{
    "body": "<a id='comment:2'></a>\nFor the dict we should do something along the lines of:\n\n```\ndef dicttopol(R,d):\n    L=[R({c:m}) for (c,m) in d.iteritems()]\n    j=0\n    for i in range(len(L)-1,0,-1):\n        j = (j-1) if j>0 else i//2 #using just j=i//2 already works well\n        L[j]+=L.pop()\n    return L[0]\n```\nwith examples of the type above, we already get:\n\n```\nsage: %time _=dicttopol(R,Ds[0])\nCPU times: user 91 ms, sys: 1.31 ms, total: 92.3 ms\nWall time: 83.6 ms\nsage: %time _=dicttopol(R,Ds[1])\nCPU times: user 162 ms, sys: 10.3 ms, total: 172 ms\nWall time: 158 ms\nsage: %time _=dicttopol(R,Ds[2])\nCPU times: user 337 ms, sys: 7.83 ms, total: 344 ms\nWall time: 317 ms\n```\nthe actual code there should probably allocate an array of pointers to directly hold the libsingular terms so that we can avoid the overhead of the python wrappers. Balanced summation should actually be used in loads of places for polynomials.\n\nWith that in place, the advice for saving a polynomial in text form would then basically be: save the dict representation instead. Creating dicts from strings seems roughly linear.",
    "created_at": "2019-09-18T07:00:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554450",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:2'></a>
For the dict we should do something along the lines of:

```
def dicttopol(R,d):
    L=[R({c:m}) for (c,m) in d.iteritems()]
    j=0
    for i in range(len(L)-1,0,-1):
        j = (j-1) if j>0 else i//2 #using just j=i//2 already works well
        L[j]+=L.pop()
    return L[0]
```
with examples of the type above, we already get:

```
sage: %time _=dicttopol(R,Ds[0])
CPU times: user 91 ms, sys: 1.31 ms, total: 92.3 ms
Wall time: 83.6 ms
sage: %time _=dicttopol(R,Ds[1])
CPU times: user 162 ms, sys: 10.3 ms, total: 172 ms
Wall time: 158 ms
sage: %time _=dicttopol(R,Ds[2])
CPU times: user 337 ms, sys: 7.83 ms, total: 344 ms
Wall time: 317 ms
```
the actual code there should probably allocate an array of pointers to directly hold the libsingular terms so that we can avoid the overhead of the python wrappers. Balanced summation should actually be used in loads of places for polynomials.

With that in place, the advice for saving a polynomial in text form would then basically be: save the dict representation instead. Creating dicts from strings seems roughly linear.



---

archive/issue_comments_554451.json:
```json
{
    "body": "<a id='comment:3'></a>\nmaybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)",
    "created_at": "2019-09-18T07:19:58Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554451",
    "user": "https://github.com/zimmermann6"
}
```

<a id='comment:3'></a>
maybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)



---

archive/issue_comments_554452.json:
```json
{
    "body": "<a id='comment:4'></a>\nIs this a regression, or can this be reproduced in Sage 8.8?  Otherwise it should probably be postponed until the next milestone.",
    "created_at": "2019-09-18T12:37:40Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554452",
    "user": "https://github.com/embray"
}
```

<a id='comment:4'></a>
Is this a regression, or can this be reproduced in Sage 8.8?  Otherwise it should probably be postponed until the next milestone.



---

archive/issue_comments_554453.json:
```json
{
    "body": "<a id='comment:5'></a>\nas indicated in the description, this is with Sage 8.8. It is probably there since the beginning of Sage, which probably means only few people play with large polynomials within Sage...",
    "created_at": "2019-09-18T12:41:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554453",
    "user": "https://github.com/zimmermann6"
}
```

<a id='comment:5'></a>
as indicated in the description, this is with Sage 8.8. It is probably there since the beginning of Sage, which probably means only few people play with large polynomials within Sage...



---

archive/issue_events_080881.json:
```json
{
    "actor": "https://github.com/embray",
    "created_at": "2019-09-18T13:15:07Z",
    "event": "milestoned",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "milestone": "sage-9.0",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/28512#event-80881"
}
```



---

archive/issue_comments_554454.json:
```json
{
    "body": "<a id='comment:7'></a>\nReplying to [zimmerma](#comment%3A3):\n> maybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)\n\n\nPickling already uses dicts, so if we improve that, \"sobj\" loading/saving would do that (since polynomials do get pickled via their dictionary representation). The test to do would be\n\n```\npcl=[dumps(p) for p in Ps]\n%time _=loads(pcl[0]) \n%time _=loads(pcl[1]) \n%time _=loads(pcl[2]) \n```\n(which shows the same behaviour as reconstructing polynomials from dicts).\n\nImproving reconstruction from dict will improve the loading of already existing pickles.\n\nNote that we once again see that we should try quite hard to keep backwards compatibility for pickles, because obvious alternatives (like saving text representatives) simple don't perform reasonably.",
    "created_at": "2019-09-18T15:57:23Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554454",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:7'></a>
Replying to [zimmerma](#comment%3A3):
> maybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)


Pickling already uses dicts, so if we improve that, "sobj" loading/saving would do that (since polynomials do get pickled via their dictionary representation). The test to do would be

```
pcl=[dumps(p) for p in Ps]
%time _=loads(pcl[0]) 
%time _=loads(pcl[1]) 
%time _=loads(pcl[2]) 
```
(which shows the same behaviour as reconstructing polynomials from dicts).

Improving reconstruction from dict will improve the loading of already existing pickles.

Note that we once again see that we should try quite hard to keep backwards compatibility for pickles, because obvious alternatives (like saving text representatives) simple don't perform reasonably.



---

archive/issue_comments_554455.json:
```json
{
    "body": "Changing branch from \"\" to \"u/nbruin/reading_a_large_expression_from_a_file_takes_quadratic_time\"",
    "created_at": "2019-09-22T05:25:24Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554455",
    "user": "https://github.com/nbruin"
}
```

Changing branch from "" to "u/nbruin/reading_a_large_expression_from_a_file_takes_quadratic_time"



---

archive/issue_comments_554456.json:
```json
{
    "body": "Changing commit from \"\" to \"76fa7df7e5f3923377ef9f32e8bc4a313d599a3e\"",
    "created_at": "2019-09-22T05:36:13Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554456",
    "user": "https://github.com/nbruin"
}
```

Changing commit from "" to "76fa7df7e5f3923377ef9f32e8bc4a313d599a3e"



---

archive/issue_comments_554457.json:
```json
{
    "body": "<a id='comment:9'></a>\nFirst try. With this branch I get:\n\n```\nsage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]\nsage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]\nsage: Ds=[p.dict() for p in Ps]\nsage: %time _=R(Ds[0])\nCPU times: user 36.8 ms, sys: 18 \u00b5s, total: 36.8 ms\nWall time: 37.1 ms\nsage: %time _=R(Ds[1])\nCPU times: user 81.7 ms, sys: 997 \u00b5s, total: 82.7 ms\nWall time: 83.1 ms\nsage: %time _=R(Ds[2])\nCPU times: user 132 ms, sys: 2.98 ms, total: 135 ms\nWall time: 136 ms\n```\n\nso that's nice behaviour. Unfortunately, for pickle it doesn't work yet:\n\n```\nsage: pickles=[dumps(p) for p in Ps]\nsage: %timeit _=loads(pickles[0])\n1 loop, best of 3: 193 ms per loop\nsage: %timeit _=loads(pickles[1])\n1 loop, best of 3: 920 ms per loop\nsage: %timeit _=loads(pickles[2])\n1 loop, best of 3: 3.92 s per loop\n```\nthat's because pickle doesn't actually go via dicts but via polydicts. That's another case in the `_element_constructor_` method, and can be changed in exactly the same way.\n\nFirst someone with knowledge of libsingular should have a look at the code to see if it can be written simpler/better (it's a little idiotic to use \"add\" when it's really just \"append term\")\n\n---\nNew commits:\n|                                                                                                                                          |                                                   |\n|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|\n|[76fa7df](https://github.com/sagemath/sagetrac-mirror/commit/76fa7df7e5f3923377ef9f32e8bc4a313d599a3e)|`Use balanced addition for converting dict to poly`|",
    "created_at": "2019-09-22T05:36:13Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554457",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:9'></a>
First try. With this branch I get:

```
sage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]
sage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]
sage: Ds=[p.dict() for p in Ps]
sage: %time _=R(Ds[0])
CPU times: user 36.8 ms, sys: 18 µs, total: 36.8 ms
Wall time: 37.1 ms
sage: %time _=R(Ds[1])
CPU times: user 81.7 ms, sys: 997 µs, total: 82.7 ms
Wall time: 83.1 ms
sage: %time _=R(Ds[2])
CPU times: user 132 ms, sys: 2.98 ms, total: 135 ms
Wall time: 136 ms
```

so that's nice behaviour. Unfortunately, for pickle it doesn't work yet:

```
sage: pickles=[dumps(p) for p in Ps]
sage: %timeit _=loads(pickles[0])
1 loop, best of 3: 193 ms per loop
sage: %timeit _=loads(pickles[1])
1 loop, best of 3: 920 ms per loop
sage: %timeit _=loads(pickles[2])
1 loop, best of 3: 3.92 s per loop
```
that's because pickle doesn't actually go via dicts but via polydicts. That's another case in the `_element_constructor_` method, and can be changed in exactly the same way.

First someone with knowledge of libsingular should have a look at the code to see if it can be written simpler/better (it's a little idiotic to use "add" when it's really just "append term")

---
New commits:
|                                                                                                                                          |                                                   |
|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------|
|[76fa7df](https://github.com/sagemath/sagetrac-mirror/commit/76fa7df7e5f3923377ef9f32e8bc4a313d599a3e)|`Use balanced addition for converting dict to poly`|



---

archive/issue_comments_554458.json:
```json
{
    "body": "<a id='comment:10'></a>\ngreat! I confirm the nice speedup. A minor typo: \"look\" should be \"loop\".",
    "created_at": "2019-09-23T09:16:43Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554458",
    "user": "https://github.com/zimmermann6"
}
```

<a id='comment:10'></a>
great! I confirm the nice speedup. A minor typo: "look" should be "loop".



---

archive/issue_comments_554459.json:
```json
{
    "body": "<a id='comment:11'></a>\nHans Schoenemann points out that \"geobuckets\" (available via the sbucket type in singular) are the way to go: [libsingular group](https://groups.google.com/d/msg/libsingular-devel/OCLBH-J7VU8/ahnPzdf1BQAJ). Someone should probably look into whether these have been interfaced already. There is quite a bit of code in the multivariate_polynomial that may end up processing lots of terms. All that code can potentially benefit significantly from conversion, and hopefully using sbuckets can be done with less boilerplate overhead than manually making an array of terms (and still wasting time on constructing intermediate results from scratch every time)",
    "created_at": "2019-09-27T15:47:02Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554459",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:11'></a>
Hans Schoenemann points out that "geobuckets" (available via the sbucket type in singular) are the way to go: [libsingular group](https://groups.google.com/d/msg/libsingular-devel/OCLBH-J7VU8/ahnPzdf1BQAJ). Someone should probably look into whether these have been interfaced already. There is quite a bit of code in the multivariate_polynomial that may end up processing lots of terms. All that code can potentially benefit significantly from conversion, and hopefully using sbuckets can be done with less boilerplate overhead than manually making an array of terms (and still wasting time on constructing intermediate results from scratch every time)



---

archive/issue_comments_554460.json:
```json
{
    "body": "<a id='comment:12'></a>\nBranch pushed to git repo; I updated commit sha1. New commits:\n|                                                                                                                                           |                                           |\n|-------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|\n|[950eca6](https://github.com/sagemath/sagetrac-mirror/commit/950eca6622042ed66c42f05b4c394b4d58b10349)|`Better approach via singular's \"sBucket\"s`|",
    "created_at": "2019-09-28T04:59:18Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554460",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

<a id='comment:12'></a>
Branch pushed to git repo; I updated commit sha1. New commits:
|                                                                                                                                           |                                           |
|-------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|
|[950eca6](https://github.com/sagemath/sagetrac-mirror/commit/950eca6622042ed66c42f05b4c394b4d58b10349)|`Better approach via singular's "sBucket"s`|



---

archive/issue_comments_554461.json:
```json
{
    "body": "Changing commit from \"76fa7df7e5f3923377ef9f32e8bc4a313d599a3e\" to \"950eca6622042ed66c42f05b4c394b4d58b10349\"",
    "created_at": "2019-09-28T04:59:18Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554461",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Changing commit from "76fa7df7e5f3923377ef9f32e8bc4a313d599a3e" to "950eca6622042ed66c42f05b4c394b4d58b10349"



---

archive/issue_comments_554462.json:
```json
{
    "body": "Changing commit from \"950eca6622042ed66c42f05b4c394b4d58b10349\" to \"c1bb521d9eff1f9af258960dd1f1ae270fdbc802\"",
    "created_at": "2019-09-28T05:57:28Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554462",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Changing commit from "950eca6622042ed66c42f05b4c394b4d58b10349" to "c1bb521d9eff1f9af258960dd1f1ae270fdbc802"



---

archive/issue_comments_554463.json:
```json
{
    "body": "<a id='comment:13'></a>\nBranch pushed to git repo; I updated commit sha1. New commits:\n|                                                                                                                                           |                                               |\n|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n|[c1bb521](https://github.com/sagemath/sagetrac-mirror/commit/c1bb521d9eff1f9af258960dd1f1ae270fdbc802)|`Also use sBuckets in unpickler and elsewhere.`|",
    "created_at": "2019-09-28T05:57:28Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554463",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

<a id='comment:13'></a>
Branch pushed to git repo; I updated commit sha1. New commits:
|                                                                                                                                           |                                               |
|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|
|[c1bb521](https://github.com/sagemath/sagetrac-mirror/commit/c1bb521d9eff1f9af258960dd1f1ae270fdbc802)|`Also use sBuckets in unpickler and elsewhere.`|



---

archive/issue_comments_554464.json:
```json
{
    "body": "Changing commit from \"c1bb521d9eff1f9af258960dd1f1ae270fdbc802\" to \"c93519ad5143739b8f56c0bb826d93fe8b9f8fa8\"",
    "created_at": "2019-09-29T16:56:23Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554464",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Changing commit from "c1bb521d9eff1f9af258960dd1f1ae270fdbc802" to "c93519ad5143739b8f56c0bb826d93fe8b9f8fa8"



---

archive/issue_comments_554465.json:
```json
{
    "body": "<a id='comment:14'></a>\nBranch pushed to git repo; I updated commit sha1. New commits:\n|                                                                                                                                           |                     |\n|-------------------------------------------------------------------------------------------------------------------------------------------|---------------------|\n|[c93519a](https://github.com/sagemath/sagetrac-mirror/commit/c93519ad5143739b8f56c0bb826d93fe8b9f8fa8)|`added some comments`|",
    "created_at": "2019-09-29T16:56:23Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554465",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

<a id='comment:14'></a>
Branch pushed to git repo; I updated commit sha1. New commits:
|                                                                                                                                           |                     |
|-------------------------------------------------------------------------------------------------------------------------------------------|---------------------|
|[c93519a](https://github.com/sagemath/sagetrac-mirror/commit/c93519ad5143739b8f56c0bb826d93fe8b9f8fa8)|`added some comments`|



---

archive/issue_comments_554466.json:
```json
{
    "body": "Changing status from new to needs_review.",
    "created_at": "2019-09-29T17:03:03Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554466",
    "user": "https://github.com/nbruin"
}
```

Changing status from new to needs_review.



---

archive/issue_comments_554467.json:
```json
{
    "body": "Changing author from \"\" to \"Nils Bruin\"",
    "created_at": "2019-09-29T17:03:03Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554467",
    "user": "https://github.com/nbruin"
}
```

Changing author from "" to "Nils Bruin"



---

archive/issue_comments_554468.json:
```json
{
    "body": "<a id='comment:15'></a>\nOK, using singular's sBuckets now; also in unpickling code. Perhaps someone wants to do some further testing that these changes don't cause regressions elsewhere (e.g., for assembling small polynomials), and possibly look at other places where using sBuckets would be an improvement. As-is, I think the code solves the problem raised in this ticket; so ... \"needs review\".",
    "created_at": "2019-09-29T17:03:03Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554468",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:15'></a>
OK, using singular's sBuckets now; also in unpickling code. Perhaps someone wants to do some further testing that these changes don't cause regressions elsewhere (e.g., for assembling small polynomials), and possibly look at other places where using sBuckets would be an improvement. As-is, I think the code solves the problem raised in this ticket; so ... "needs review".



---

archive/issue_comments_554469.json:
```json
{
    "body": "<a id='comment:16'></a>\nBranch pushed to git repo; I updated commit sha1. New commits:\n|                                                                                                                                           |                                                         |\n|-------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|\n|[94ae446](https://github.com/sagemath/sagetrac-mirror/commit/94ae44653baaac3eb558ea9fdcb5c66ebde8510e)|`we need to destroy buckets after use to not leak memory`|",
    "created_at": "2019-09-29T23:19:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554469",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

<a id='comment:16'></a>
Branch pushed to git repo; I updated commit sha1. New commits:
|                                                                                                                                           |                                                         |
|-------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|
|[94ae446](https://github.com/sagemath/sagetrac-mirror/commit/94ae44653baaac3eb558ea9fdcb5c66ebde8510e)|`we need to destroy buckets after use to not leak memory`|



---

archive/issue_comments_554470.json:
```json
{
    "body": "Changing commit from \"c93519ad5143739b8f56c0bb826d93fe8b9f8fa8\" to \"94ae44653baaac3eb558ea9fdcb5c66ebde8510e\"",
    "created_at": "2019-09-29T23:19:51Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554470",
    "user": "https://trac.sagemath.org/admin/accounts/users/git"
}
```

Changing commit from "c93519ad5143739b8f56c0bb826d93fe8b9f8fa8" to "94ae44653baaac3eb558ea9fdcb5c66ebde8510e"



---

archive/issue_comments_554471.json:
```json
{
    "body": "<a id='comment:17'></a>\nWith the following code you get a bit of an idea how performance is for various sizes of polynomials. For small polynomials, there's a lot of noise, and the difference with the old code is quite small, so I don't know it's significant. I think that from F[9] onwards in the code below, it's quite clear the buckets are faster (so that's for polynomials with about 60 terms or so). I'm not sure if we should avoid using buckets below that cutoff. Certainly, this kind of tuning would depend on the platform as well.\n\n```\nR.<x,y,z>=QQ[]\nF=[(x+y+z)^n for n in [1..100]]\nD=[f.dict() for f in F]\nfor i in range(100): print(timeit(\"R(D[%i])\"%i))\n```",
    "created_at": "2019-09-29T23:25:00Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554471",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:17'></a>
With the following code you get a bit of an idea how performance is for various sizes of polynomials. For small polynomials, there's a lot of noise, and the difference with the old code is quite small, so I don't know it's significant. I think that from F[9] onwards in the code below, it's quite clear the buckets are faster (so that's for polynomials with about 60 terms or so). I'm not sure if we should avoid using buckets below that cutoff. Certainly, this kind of tuning would depend on the platform as well.

```
R.<x,y,z>=QQ[]
F=[(x+y+z)^n for n in [1..100]]
D=[f.dict() for f in F]
for i in range(100): print(timeit("R(D[%i])"%i))
```



---

archive/issue_comments_554472.json:
```json
{
    "body": "<a id='comment:18'></a>\nI get a nice speedup. Here is what I got with Sage 8.8 for the last 10 loops:\n\n```\n5 loops, best of 3: 41.8 ms per loop\n5 loops, best of 3: 71.6 ms per loop\n5 loops, best of 3: 80.8 ms per loop\n5 loops, best of 3: 70.7 ms per loop\n5 loops, best of 3: 130 ms per loop\n5 loops, best of 3: 104 ms per loop\n5 loops, best of 3: 68.7 ms per loop\n5 loops, best of 3: 117 ms per loop\n5 loops, best of 3: 132 ms per loop\n5 loops, best of 3: 146 ms per loop\n```\nWith the new code:\n\n```\n125 loops, best of 3: 6.53 ms per loop\n125 loops, best of 3: 7.1 ms per loop\n125 loops, best of 3: 6.91 ms per loop\n25 loops, best of 3: 6.04 ms per loop\n125 loops, best of 3: 6.87 ms per loop\n125 loops, best of 3: 9.65 ms per loop\n25 loops, best of 3: 14.4 ms per loop\n25 loops, best of 3: 16.5 ms per loop\n25 loops, best of 3: 15.7 ms per loop\n25 loops, best of 3: 14.6 ms per loop\n```\nGreat work!",
    "created_at": "2019-09-30T07:55:53Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554472",
    "user": "https://github.com/zimmermann6"
}
```

<a id='comment:18'></a>
I get a nice speedup. Here is what I got with Sage 8.8 for the last 10 loops:

```
5 loops, best of 3: 41.8 ms per loop
5 loops, best of 3: 71.6 ms per loop
5 loops, best of 3: 80.8 ms per loop
5 loops, best of 3: 70.7 ms per loop
5 loops, best of 3: 130 ms per loop
5 loops, best of 3: 104 ms per loop
5 loops, best of 3: 68.7 ms per loop
5 loops, best of 3: 117 ms per loop
5 loops, best of 3: 132 ms per loop
5 loops, best of 3: 146 ms per loop
```
With the new code:

```
125 loops, best of 3: 6.53 ms per loop
125 loops, best of 3: 7.1 ms per loop
125 loops, best of 3: 6.91 ms per loop
25 loops, best of 3: 6.04 ms per loop
125 loops, best of 3: 6.87 ms per loop
125 loops, best of 3: 9.65 ms per loop
25 loops, best of 3: 14.4 ms per loop
25 loops, best of 3: 16.5 ms per loop
25 loops, best of 3: 15.7 ms per loop
25 loops, best of 3: 14.6 ms per loop
```
Great work!



---

archive/issue_comments_554473.json:
```json
{
    "body": "<a id='comment:19'></a>\nThanks. The contentious data is for the first few loops. For the first 15 I get:\nreference:\n\n```\n1000 loops, best of 10: 9.04 \u03bcs per loop\n1000 loops, best of 10: 12.7 \u03bcs per loop\n1000 loops, best of 10: 17.2 \u03bcs per loop\n1000 loops, best of 10: 23.9 \u03bcs per loop\n1000 loops, best of 10: 31.8 \u03bcs per loop\n1000 loops, best of 10: 41.4 \u03bcs per loop\n1000 loops, best of 10: 52.1 \u03bcs per loop\n1000 loops, best of 10: 64.9 \u03bcs per loop\n1000 loops, best of 10: 78.9 \u03bcs per loop\n1000 loops, best of 10: 94.6 \u03bcs per loop\n1000 loops, best of 10: 112 \u03bcs per loop\n1000 loops, best of 10: 132 \u03bcs per loop\n1000 loops, best of 10: 153 \u03bcs per loop\n1000 loops, best of 10: 176 \u03bcs per loop\n1000 loops, best of 10: 199 \u03bcs per loop\n```\nWith patch:\n\n```\n1000 loops, best of 10: 9.55 \u03bcs per loop\n1000 loops, best of 10: 12.6 \u03bcs per loop\n1000 loops, best of 10: 17.5 \u03bcs per loop\n1000 loops, best of 10: 24 \u03bcs per loop\n1000 loops, best of 10: 31.8 \u03bcs per loop\n1000 loops, best of 10: 41.5 \u03bcs per loop\n1000 loops, best of 10: 51.9 \u03bcs per loop\n1000 loops, best of 10: 63.8 \u03bcs per loop\n1000 loops, best of 10: 77.6 \u03bcs per loop\n1000 loops, best of 10: 92.5 \u03bcs per loop\n1000 loops, best of 10: 109 \u03bcs per loop\n1000 loops, best of 10: 127 \u03bcs per loop\n1000 loops, best of 10: 147 \u03bcs per loop\n1000 loops, best of 10: 168 \u03bcs per loop\n1000 loops, best of 10: 190 \u03bcs per loop\n```\nwith the problem that there can easily be a 1\u03bcs variation in the timings. Based on that (and looking at several runs) I believe that for `F[0]` the new code is probably indeed a little slower (but only by 0.5\u03bcs or so). From `F[8]` onwards or so I think the new code actually starts to win. So based on that I don't think tuning a case split (and incurring an overhead for deciding on a case split!) is going to be worth it. So my proposal would be to go with the code on this ticket.",
    "created_at": "2019-10-02T17:56:00Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554473",
    "user": "https://github.com/nbruin"
}
```

<a id='comment:19'></a>
Thanks. The contentious data is for the first few loops. For the first 15 I get:
reference:

```
1000 loops, best of 10: 9.04 μs per loop
1000 loops, best of 10: 12.7 μs per loop
1000 loops, best of 10: 17.2 μs per loop
1000 loops, best of 10: 23.9 μs per loop
1000 loops, best of 10: 31.8 μs per loop
1000 loops, best of 10: 41.4 μs per loop
1000 loops, best of 10: 52.1 μs per loop
1000 loops, best of 10: 64.9 μs per loop
1000 loops, best of 10: 78.9 μs per loop
1000 loops, best of 10: 94.6 μs per loop
1000 loops, best of 10: 112 μs per loop
1000 loops, best of 10: 132 μs per loop
1000 loops, best of 10: 153 μs per loop
1000 loops, best of 10: 176 μs per loop
1000 loops, best of 10: 199 μs per loop
```
With patch:

```
1000 loops, best of 10: 9.55 μs per loop
1000 loops, best of 10: 12.6 μs per loop
1000 loops, best of 10: 17.5 μs per loop
1000 loops, best of 10: 24 μs per loop
1000 loops, best of 10: 31.8 μs per loop
1000 loops, best of 10: 41.5 μs per loop
1000 loops, best of 10: 51.9 μs per loop
1000 loops, best of 10: 63.8 μs per loop
1000 loops, best of 10: 77.6 μs per loop
1000 loops, best of 10: 92.5 μs per loop
1000 loops, best of 10: 109 μs per loop
1000 loops, best of 10: 127 μs per loop
1000 loops, best of 10: 147 μs per loop
1000 loops, best of 10: 168 μs per loop
1000 loops, best of 10: 190 μs per loop
```
with the problem that there can easily be a 1μs variation in the timings. Based on that (and looking at several runs) I believe that for `F[0]` the new code is probably indeed a little slower (but only by 0.5μs or so). From `F[8]` onwards or so I think the new code actually starts to win. So based on that I don't think tuning a case split (and incurring an overhead for deciding on a case split!) is going to be worth it. So my proposal would be to go with the code on this ticket.



---

archive/issue_comments_554474.json:
```json
{
    "body": "Changing status from needs_review to positive_review.",
    "created_at": "2019-10-03T08:24:59Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554474",
    "user": "https://github.com/zimmermann6"
}
```

Changing status from needs_review to positive_review.



---

archive/issue_comments_554475.json:
```json
{
    "body": "<a id='comment:20'></a>\n> So my proposal would be to go with the code on this ticket. \n\n\nagreed. I give a positive review.",
    "created_at": "2019-10-03T08:24:59Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554475",
    "user": "https://github.com/zimmermann6"
}
```

<a id='comment:20'></a>
> So my proposal would be to go with the code on this ticket. 


agreed. I give a positive review.



---

archive/issue_comments_554476.json:
```json
{
    "body": "Changing reviewer from \"\" to \"Paul Zimmermann\"",
    "created_at": "2019-10-03T08:24:59Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554476",
    "user": "https://github.com/zimmermann6"
}
```

Changing reviewer from "" to "Paul Zimmermann"



---

archive/issue_comments_554477.json:
```json
{
    "body": "Resolution: fixed",
    "created_at": "2019-10-05T07:58:06Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554477",
    "user": "https://github.com/vbraun"
}
```

Resolution: fixed



---

archive/issue_events_080882.json:
```json
{
    "actor": "https://github.com/vbraun",
    "created_at": "2019-10-05T07:58:06Z",
    "event": "closed",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_event",
    "url": "https://github.com/sagemath/sagetest/issues/28512#event-80882"
}
```



---

archive/issue_comments_554478.json:
```json
{
    "body": "Changing branch from \"u/nbruin/reading_a_large_expression_from_a_file_takes_quadratic_time\" to \"94ae44653baaac3eb558ea9fdcb5c66ebde8510e\"",
    "created_at": "2019-10-05T07:58:06Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554478",
    "user": "https://github.com/vbraun"
}
```

Changing branch from "u/nbruin/reading_a_large_expression_from_a_file_takes_quadratic_time" to "94ae44653baaac3eb558ea9fdcb5c66ebde8510e"



---

archive/issue_comments_554479.json:
```json
{
    "body": "<a id='comment:22'></a>\nFWIW, evaluation of large arithmetic expressions can be quite problematic in Python.  You already wrote about that somewhat in this ticket, but it's something I also started to explore several years ago, and *started* to write about [in my blog](https://iguananaut.net/blog/programming/python-polynomials-parsers-1.html) but then got sidetracked and never wrote the follow-up.\n\nActually the source of the segfault I mention there has now been fixed in CPython--the same example will still crash but the Python interpreter raises a `RuntimeError: maximum recursion depth exceeded` exception instead of segfaulting.\n\nMy practical conclusion was that the Sage preparser should do some of its own pre-parsing of polynomial expressions to avoid evaluating them as Python expressions.",
    "created_at": "2019-12-16T16:19:38Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554479",
    "user": "https://github.com/embray"
}
```

<a id='comment:22'></a>
FWIW, evaluation of large arithmetic expressions can be quite problematic in Python.  You already wrote about that somewhat in this ticket, but it's something I also started to explore several years ago, and *started* to write about [in my blog](https://iguananaut.net/blog/programming/python-polynomials-parsers-1.html) but then got sidetracked and never wrote the follow-up.

Actually the source of the segfault I mention there has now been fixed in CPython--the same example will still crash but the Python interpreter raises a `RuntimeError: maximum recursion depth exceeded` exception instead of segfaulting.

My practical conclusion was that the Sage preparser should do some of its own pre-parsing of polynomial expressions to avoid evaluating them as Python expressions.



---

archive/issue_comments_554480.json:
```json
{
    "body": "Changing commit from \"94ae44653baaac3eb558ea9fdcb5c66ebde8510e\" to \"\"",
    "created_at": "2019-12-16T16:19:38Z",
    "issue": "https://github.com/sagemath/sagetest/issues/28512",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/28512#issuecomment-554480",
    "user": "https://github.com/embray"
}
```

Changing commit from "94ae44653baaac3eb558ea9fdcb5c66ebde8510e" to ""
