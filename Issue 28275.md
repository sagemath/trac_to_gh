# Issue 28275: reading a large expression from a file takes quadratic time

Issue created by migration from https://trac.sagemath.org/ticket/28512

Original creator: zimmerma

Original creation time: 2019-09-17 14:24:06

consider the following with Sage 8.8:

```
sage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]
sage: p = R.random_element(degree=10,terms=10^4)
sage: p.number_of_terms()
7983
sage: f = open("p.sage","w")
sage: f.write("p="+str(p)+"\n")
sage: f.close()
sage: t = cputime()
sage: load("p.sage")
sage: cputime()-t
3.1715750000000185
sage: q = R.random_element(degree=10,terms=2*10^4)
sage: q.number_of_terms()
16055
sage: f = open("q.sage","w")
sage: f.write("q="+str(q)+"\n")
sage: f.close()
sage: t = cputime()
sage: load("q.sage")
sage: cputime()-t
14.55525400000002
```

We see that for a polynomial that is almost twice as large, the time to read the file is multiplied by a factor larger than 4! This makes it impossible to work with very large files.


---

Comment by nbruin created at 2019-09-17 21:52:40

I'm not sure there's much we can do about it via this route, since the semantics of python prescribe that `a1+a2+a3+a4` gets evaluated as `((a1+a2)+a3)+a4`, which, due to the creation of intermediate results, should be quadratic in the number of terms [note that you're really just writing a python expression to a file and evaluating that] You could explicitly write your expression in balanced form to avoid this problem:

```
def balanced_str(p):
  return balanced_sum([c*t for c,t in list(p)])
def balanced_sum(L):
  if len(L) <= 4:
    return "+".join([str(a) for a in L])
  else:
    n= len(L) // 2
    return "(%s)+(%s)"%(balanced_sum(L[:n]),balanced_sum(L[n:]))
```

We can test this with some of your polynomials:

```
sage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]
sage: Ss=[balanced_str(p) for p in Ps]
sage: %time _=R(Ss[0])
CPU times: user 139 ms, sys: 7.03 ms, total: 146 ms
Wall time: 143 ms
sage: %time _=R(Ss[1])
CPU times: user 245 ms, sys: 22 ms, total: 267 ms
Wall time: 262 ms
sage: %time _=R(Ss[2])
CPU times: user 487 ms, sys: 42.1 ms, total: 529 ms
Wall time: 519 ms
```

A little more worrisome is that for converting dictionary representation back into polynomials, the same quadratic behaviour occurs:

```
sage: Ds=[p.dict() for p in Ps]
sage: %time _=R(Ds[0])
CPU times: user 195 ms, sys: 0 ns, total: 195 ms
Wall time: 196 ms
sage: %time _=R(Ds[1])
CPU times: user 863 ms, sys: 0 ns, total: 863 ms
Wall time: 864 ms
sage: %time _=R(Ds[2])
CPU times: user 3.7 s, sys: 0 ns, total: 3.7 s
Wall time: 3.7 s
```

That's rather hard to defend. That should definitely be fixed. Constructing a polynomial from a dictionary representation should be one of the fastest ways of constructing a polynomial.


---

Comment by nbruin created at 2019-09-18 07:00:51

For the dict we should do something along the lines of:

```
def dicttopol(R,d):
    L=[R({c:m}) for (c,m) in d.iteritems()]
    j=0
    for i in range(len(L)-1,0,-1):
        j = (j-1) if j>0 else i//2 #using just j=i//2 already works well
        L[j]+=L.pop()
    return L[0]
```

with examples of the type above, we already get:

```
sage: %time _=dicttopol(R,Ds[0])
CPU times: user 91 ms, sys: 1.31 ms, total: 92.3 ms
Wall time: 83.6 ms
sage: %time _=dicttopol(R,Ds[1])
CPU times: user 162 ms, sys: 10.3 ms, total: 172 ms
Wall time: 158 ms
sage: %time _=dicttopol(R,Ds[2])
CPU times: user 337 ms, sys: 7.83 ms, total: 344 ms
Wall time: 317 ms
```

the actual code there should probably allocate an array of pointers to directly hold the libsingular terms so that we can avoid the overhead of the python wrappers. Balanced summation should actually be used in loads of places for polynomials.

With that in place, the advice for saving a polynomial in text form would then basically be: save the dict representation instead. Creating dicts from strings seems roughly linear.


---

Comment by zimmerma created at 2019-09-18 07:19:58

maybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)


---

Comment by embray created at 2019-09-18 12:37:40

Is this a regression, or can this be reproduced in Sage 8.8?  Otherwise it should probably be postponed until the next milestone.


---

Comment by zimmerma created at 2019-09-18 12:41:27

as indicated in the description, this is with Sage 8.8. It is probably there since the beginning of Sage, which probably means only few people play with large polynomials within Sage...


---

Comment by nbruin created at 2019-09-18 15:57:23

Replying to [comment:3 zimmerma]:
> maybe Sage could provide a method to save polynomials in a file, so that we can read them back efficiently (as balanced expressions, or dicts, ...)

Pickling already uses dicts, so if we improve that, "sobj" loading/saving would do that (since polynomials do get pickled via their dictionary representation). The test to do would be

```
pcl=[dumps(p) for p in Ps]
%time _=loads(pcl[0]) 
%time _=loads(pcl[1]) 
%time _=loads(pcl[2]) 
```

(which shows the same behaviour as reconstructing polynomials from dicts).

Improving reconstruction from dict will improve the loading of already existing pickles.

Note that we once again see that we should try quite hard to keep backwards compatibility for pickles, because obvious alternatives (like saving text representatives) simple don't perform reasonably.


---

Comment by nbruin created at 2019-09-22 05:36:13

First try. With this branch I get:

```
sage: R.<x1, x2, x3, x4, x5, x6, y1, y2, y3, y4, y5, y6, z1, z2, z3, z4> = ZZ[]
sage: Ps=[R.random_element(degree=10,terms=2^i*10^4) for i in [0..2]]
sage: Ds=[p.dict() for p in Ps]
sage: %time _=R(Ds[0])
CPU times: user 36.8 ms, sys: 18 µs, total: 36.8 ms
Wall time: 37.1 ms
sage: %time _=R(Ds[1])
CPU times: user 81.7 ms, sys: 997 µs, total: 82.7 ms
Wall time: 83.1 ms
sage: %time _=R(Ds[2])
CPU times: user 132 ms, sys: 2.98 ms, total: 135 ms
Wall time: 136 ms
```


so that's nice behaviour. Unfortunately, for pickle it doesn't work yet:

```
sage: pickles=[dumps(p) for p in Ps]
sage: %timeit _=loads(pickles[0])
1 loop, best of 3: 193 ms per loop
sage: %timeit _=loads(pickles[1])
1 loop, best of 3: 920 ms per loop
sage: %timeit _=loads(pickles[2])
1 loop, best of 3: 3.92 s per loop
```

that's because pickle doesn't actually go via dicts but via polydicts. That's another case in the `_element_constructor_` method, and can be changed in exactly the same way.

First someone with knowledge of libsingular should have a look at the code to see if it can be written simpler/better (it's a little idiotic to use "add" when it's really just "append term")
----
New commits:


---

Comment by zimmerma created at 2019-09-23 09:16:43

great! I confirm the nice speedup. A minor typo: "look" should be "loop".


---

Comment by nbruin created at 2019-09-27 15:47:02

Hans Schoenemann points out that "geobuckets" (available via the sbucket type in singular) are the way to go: [libsingular group](https://groups.google.com/d/msg/libsingular-devel/OCLBH-J7VU8/ahnPzdf1BQAJ). Someone should probably look into whether these have been interfaced already. There is quite a bit of code in the multivariate_polynomial that may end up processing lots of terms. All that code can potentially benefit significantly from conversion, and hopefully using sbuckets can be done with less boilerplate overhead than manually making an array of terms (and still wasting time on constructing intermediate results from scratch every time)


---

Comment by git created at 2019-09-28 04:59:18

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2019-09-28 05:57:28

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by git created at 2019-09-29 16:56:23

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by nbruin created at 2019-09-29 17:03:03

Changing status from new to needs_review.


---

Comment by nbruin created at 2019-09-29 17:03:03

OK, using singular's sBuckets now; also in unpickling code. Perhaps someone wants to do some further testing that these changes don't cause regressions elsewhere (e.g., for assembling small polynomials), and possibly look at other places where using sBuckets would be an improvement. As-is, I think the code solves the problem raised in this ticket; so ... "needs review".


---

Comment by git created at 2019-09-29 23:19:51

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by nbruin created at 2019-09-29 23:25:00

With the following code you get a bit of an idea how performance is for various sizes of polynomials. For small polynomials, there's a lot of noise, and the difference with the old code is quite small, so I don't know it's significant. I think that from F[9] onwards in the code below, it's quite clear the buckets are faster (so that's for polynomials with about 60 terms or so). I'm not sure if we should avoid using buckets below that cutoff. Certainly, this kind of tuning would depend on the platform as well.

```
R.<x,y,z>=QQ[]
F=[(x+y+z)^n for n in [1..100]]
D=[f.dict() for f in F]
for i in range(100): print(timeit("R(D[%i])"%i))
```



---

Comment by zimmerma created at 2019-09-30 07:55:53

I get a nice speedup. Here is what I got with Sage 8.8 for the last 10 loops:

```
5 loops, best of 3: 41.8 ms per loop
5 loops, best of 3: 71.6 ms per loop
5 loops, best of 3: 80.8 ms per loop
5 loops, best of 3: 70.7 ms per loop
5 loops, best of 3: 130 ms per loop
5 loops, best of 3: 104 ms per loop
5 loops, best of 3: 68.7 ms per loop
5 loops, best of 3: 117 ms per loop
5 loops, best of 3: 132 ms per loop
5 loops, best of 3: 146 ms per loop
```

With the new code:

```
125 loops, best of 3: 6.53 ms per loop
125 loops, best of 3: 7.1 ms per loop
125 loops, best of 3: 6.91 ms per loop
25 loops, best of 3: 6.04 ms per loop
125 loops, best of 3: 6.87 ms per loop
125 loops, best of 3: 9.65 ms per loop
25 loops, best of 3: 14.4 ms per loop
25 loops, best of 3: 16.5 ms per loop
25 loops, best of 3: 15.7 ms per loop
25 loops, best of 3: 14.6 ms per loop
```

Great work!


---

Comment by nbruin created at 2019-10-02 17:56:00

Thanks. The contentious data is for the first few loops. For the first 15 I get:
reference:

```
1000 loops, best of 10: 9.04 μs per loop
1000 loops, best of 10: 12.7 μs per loop
1000 loops, best of 10: 17.2 μs per loop
1000 loops, best of 10: 23.9 μs per loop
1000 loops, best of 10: 31.8 μs per loop
1000 loops, best of 10: 41.4 μs per loop
1000 loops, best of 10: 52.1 μs per loop
1000 loops, best of 10: 64.9 μs per loop
1000 loops, best of 10: 78.9 μs per loop
1000 loops, best of 10: 94.6 μs per loop
1000 loops, best of 10: 112 μs per loop
1000 loops, best of 10: 132 μs per loop
1000 loops, best of 10: 153 μs per loop
1000 loops, best of 10: 176 μs per loop
1000 loops, best of 10: 199 μs per loop
```

With patch:

```
1000 loops, best of 10: 9.55 μs per loop
1000 loops, best of 10: 12.6 μs per loop
1000 loops, best of 10: 17.5 μs per loop
1000 loops, best of 10: 24 μs per loop
1000 loops, best of 10: 31.8 μs per loop
1000 loops, best of 10: 41.5 μs per loop
1000 loops, best of 10: 51.9 μs per loop
1000 loops, best of 10: 63.8 μs per loop
1000 loops, best of 10: 77.6 μs per loop
1000 loops, best of 10: 92.5 μs per loop
1000 loops, best of 10: 109 μs per loop
1000 loops, best of 10: 127 μs per loop
1000 loops, best of 10: 147 μs per loop
1000 loops, best of 10: 168 μs per loop
1000 loops, best of 10: 190 μs per loop
```

with the problem that there can easily be a 1μs variation in the timings. Based on that (and looking at several runs) I believe that for `F[0]` the new code is probably indeed a little slower (but only by 0.5μs or so). From `F[8]` onwards or so I think the new code actually starts to win. So based on that I don't think tuning a case split (and incurring an overhead for deciding on a case split!) is going to be worth it. So my proposal would be to go with the code on this ticket.


---

Comment by zimmerma created at 2019-10-03 08:24:59

Changing status from needs_review to positive_review.


---

Comment by zimmerma created at 2019-10-03 08:24:59

> So my proposal would be to go with the code on this ticket. 

agreed. I give a positive review.


---

Comment by vbraun created at 2019-10-05 07:58:06

Resolution: fixed


---

Comment by embray created at 2019-12-16 16:19:38

FWIW, evaluation of large arithmetic expressions can be quite problematic in Python.  You already wrote about that somewhat in this ticket, but it's something I also started to explore several years ago, and _started_ to write about [in my blog](https://iguananaut.net/blog/programming/python-polynomials-parsers-1.html) but then got sidetracked and never wrote the follow-up.

Actually the source of the segfault I mention there has now been fixed in CPython--the same example will still crash but the Python interpreter raises a `RuntimeError: maximum recursion depth exceeded` exception instead of segfaulting.

My practical conclusion was that the Sage preparser should do some of its own pre-parsing of polynomial expressions to avoid evaluating them as Python expressions.
