# Issue 12005: Prime slicing for matrix multiplication

archive/issues_012005.json:
```json
{
    "body": "Assignee: jason, was\n\nCC:  malb burcin robertwb boothby\n\nKeywords: prime slicing, Karatsuba\n\nIn Sage, matrix arithmetic over finite fields is fast in the following cases:\n\n* Prime fields, using linbox (#4260) resp. `M4RI` over `GF(2)`\n* `GF(2^e)`, using `M4RIE` (#9562)\n\nIn all other cases, it sucks. There is the suggestion to use a wrapper of a fork of `C-MeatAxe` (#12103), but this would only work up to field size 255 and might have further disadvantages.\n\nMartin Albrecht suggested to use \"prime slicing\" instead: \n\n* Represent matrices over `GF(p^n)` by a list of n matrices over `GF(p)` (with Linbox as backend)\n* Matrix multiplication over `GF(p^n)` is implemented by a series of multiplications over `GF(p)`\n* Karatsuba type formulae reduce the number of \"prime\" multiplications involved.\n\nOn [sage-devel](http://groups.google.com/group/sage-devel/browse_thread/thread/8a988a2f5d997a5a/ebcde393f8ef5240), Martin gave a couple of references:\n\n* [Boothby and Bradshaw](http://www.google.com/url?sa=D&q=http://arxiv.org/abs/0901.1413&usg=AFQjCNFg5cUrXjBTqkon1V6n4Yl_CVQ9yA) Bitslicing and the Method of Four Russians Over Larger Finite Fields\n* [Montgomery](http://bit.ly/r5PVgv) Five, Six, and Seven-Term\nKaratsuba-Like Formulae\n\nOn another occasion, Martin also pointed out how to compute echelon forms in that setting.\n\nThe purpose of this ticket is to make that idea real.\n\nIssue created by migration from https://trac.sagemath.org/ticket/12177\n\n",
    "created_at": "2011-12-18T10:35:20Z",
    "labels": [
        "linear algebra",
        "major",
        "enhancement"
    ],
    "title": "Prime slicing for matrix multiplication",
    "type": "issue",
    "url": "https://github.com/sagemath/sagetest/issues/12005",
    "user": "SimonKing"
}
```
Assignee: jason, was

CC:  malb burcin robertwb boothby

Keywords: prime slicing, Karatsuba

In Sage, matrix arithmetic over finite fields is fast in the following cases:

* Prime fields, using linbox (#4260) resp. `M4RI` over `GF(2)`
* `GF(2^e)`, using `M4RIE` (#9562)

In all other cases, it sucks. There is the suggestion to use a wrapper of a fork of `C-MeatAxe` (#12103), but this would only work up to field size 255 and might have further disadvantages.

Martin Albrecht suggested to use "prime slicing" instead: 

* Represent matrices over `GF(p^n)` by a list of n matrices over `GF(p)` (with Linbox as backend)
* Matrix multiplication over `GF(p^n)` is implemented by a series of multiplications over `GF(p)`
* Karatsuba type formulae reduce the number of "prime" multiplications involved.

On [sage-devel](http://groups.google.com/group/sage-devel/browse_thread/thread/8a988a2f5d997a5a/ebcde393f8ef5240), Martin gave a couple of references:

* [Boothby and Bradshaw](http://www.google.com/url?sa=D&q=http://arxiv.org/abs/0901.1413&usg=AFQjCNFg5cUrXjBTqkon1V6n4Yl_CVQ9yA) Bitslicing and the Method of Four Russians Over Larger Finite Fields
* [Montgomery](http://bit.ly/r5PVgv) Five, Six, and Seven-Term
Karatsuba-Like Formulae

On another occasion, Martin also pointed out how to compute echelon forms in that setting.

The purpose of this ticket is to make that idea real.

Issue created by migration from https://trac.sagemath.org/ticket/12177





---

archive/issue_comments_138677.json:
```json
{
    "body": "Hi Simon, great that you're getting the ball rolling. Two comments:\n\n(a) I think the this code shouldn't be in Sage but on a lower level, preferably LinBox. But perhaps we can write proof of concept in Sage first and then port to C++?\n\n(b)I figure we should get an idea about what performance to expect, so:\n\n```python\np = 17\nK.<a> = GF(p^2)\nA = [random_matrix(GF(p),2000,2000) for _ in range(2)]\nB = [random_matrix(GF(p),2000,2000) for _ in range(2)]\nC = [matrix(GF(p),2000,2000) for _ in range(2)]\n\ndef mul2(K, C,A,B):\n   poly = K.polynomial()\n   T0 = A[0]*B[0]\n   C[0] += T0\n   C[1] -= T0\n   C[1] += (A[0] + A[1])*(B[0]+B[1])\n   T0 = A[1]*B[1]\n   C[1] -= T0\n   for i,c in enumerate(list(poly)):\n       if i == 2:\n           break\n       C[i] -= c*T0\n   return C\n```\n\n\nmul2 (I didn't check for bugs) with these inputs takes about 3 seconds on my computer, which was to be expected since a product over GF(p) takes about 1 second on my computer. For comparison Magma is a bit better:\n\n\n```\nMagma V2.15-10    Sun Dec 18 2011 12:53:05 on road     [Seed = 1273554698]\nType ? for help.  Type <Ctrl>-D to quit.\n> K:=GF(17^2);\n> A:=RandomMatrix(K,2000,2000);\n> B:=RandomMatrix(K,2000,2000);\n> time C:=A*B;\nTime: 2.320\n```\n\n\nI'm not sure we can do much about it, since the performance essentially depends on the speed of mod-p matrices.",
    "created_at": "2011-12-18T12:55:39Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138677",
    "user": "malb"
}
```

Hi Simon, great that you're getting the ball rolling. Two comments:

(a) I think the this code shouldn't be in Sage but on a lower level, preferably LinBox. But perhaps we can write proof of concept in Sage first and then port to C++?

(b)I figure we should get an idea about what performance to expect, so:

```python
p = 17
K.<a> = GF(p^2)
A = [random_matrix(GF(p),2000,2000) for _ in range(2)]
B = [random_matrix(GF(p),2000,2000) for _ in range(2)]
C = [matrix(GF(p),2000,2000) for _ in range(2)]

def mul2(K, C,A,B):
   poly = K.polynomial()
   T0 = A[0]*B[0]
   C[0] += T0
   C[1] -= T0
   C[1] += (A[0] + A[1])*(B[0]+B[1])
   T0 = A[1]*B[1]
   C[1] -= T0
   for i,c in enumerate(list(poly)):
       if i == 2:
           break
       C[i] -= c*T0
   return C
```


mul2 (I didn't check for bugs) with these inputs takes about 3 seconds on my computer, which was to be expected since a product over GF(p) takes about 1 second on my computer. For comparison Magma is a bit better:


```
Magma V2.15-10    Sun Dec 18 2011 12:53:05 on road     [Seed = 1273554698]
Type ? for help.  Type <Ctrl>-D to quit.
> K:=GF(17^2);
> A:=RandomMatrix(K,2000,2000);
> B:=RandomMatrix(K,2000,2000);
> time C:=A*B;
Time: 2.320
```


I'm not sure we can do much about it, since the performance essentially depends on the speed of mod-p matrices.



---

archive/issue_comments_138678.json:
```json
{
    "body": "I think we have to do two things:\n\n#. Have some code that deals with lists of matrices. I understand that Burcin has such code.\n#. Find a way to find a Karatsuba type formula that reduces the number of mod-p multiplications.\n\nI think the second point should actually done not by Karatsuba, but by a modification of Level-n Toom multiplication (where n is the degree of our field extension). See [Wikipedia](http://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication) for a method to compute the level-n formula.\n\nIdeally, we would also merge the defining polynomial of our field extension into the Toom formula.",
    "created_at": "2011-12-18T15:42:23Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138678",
    "user": "SimonKing"
}
```

I think we have to do two things:

#. Have some code that deals with lists of matrices. I understand that Burcin has such code.
#. Find a way to find a Karatsuba type formula that reduces the number of mod-p multiplications.

I think the second point should actually done not by Karatsuba, but by a modification of Level-n Toom multiplication (where n is the degree of our field extension). See [Wikipedia](http://en.wikipedia.org/wiki/Toom%E2%80%93Cook_multiplication) for a method to compute the level-n formula.

Ideally, we would also merge the defining polynomial of our field extension into the Toom formula.



---

archive/issue_comments_138679.json:
```json
{
    "body": "Here are some thoughts - I am not sure if this is all correct, but I think it is a starting point:\n\nElements of `K=GF(p^n)` are represented by polynomials of degree `n-1` over `k=GF(p)`.\n\nAssumption: `p>>n`.\n\nEach polynomial of degree `n-1` (hence, any element of K) is determined by evaluation at n points. When multipliying two polynomials then (modulo the defining polynomial of K) they also give rise to a polynomial of degree `n-1`. Hence, again, n evaluation points are enough.\n\nI am not sure, though, whether we can choose *any* n-tuple of elements of k.\n\nFirst step:\n\nChoose n points from k. Evalutation of a generic polynomial of degree n-1 gives rise to n linear combinations of the polynomial's coefficients, described by some invertible square matrix. Let A be the inverse of that matrix.\n\nMultiplying two polynomials of degree n modulo the defining polynomial now means: Evaluate the two factors at the given n points. For each point, multiply the two evaluations. Multiply this with A, and read off the coefficients of the product.\n\nSo, this can be used for our polynomials of matrices. If my arguments work, then we would be down to n multiplications over GF(p) for one multiplication over `GF(p^n)`. And comparing the figures of Linbox over `GF(17)` and Magma over `GF(17^2)`, we would be competitive with Magma.",
    "created_at": "2011-12-18T15:54:19Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138679",
    "user": "SimonKing"
}
```

Here are some thoughts - I am not sure if this is all correct, but I think it is a starting point:

Elements of `K=GF(p^n)` are represented by polynomials of degree `n-1` over `k=GF(p)`.

Assumption: `p>>n`.

Each polynomial of degree `n-1` (hence, any element of K) is determined by evaluation at n points. When multipliying two polynomials then (modulo the defining polynomial of K) they also give rise to a polynomial of degree `n-1`. Hence, again, n evaluation points are enough.

I am not sure, though, whether we can choose *any* n-tuple of elements of k.

First step:

Choose n points from k. Evalutation of a generic polynomial of degree n-1 gives rise to n linear combinations of the polynomial's coefficients, described by some invertible square matrix. Let A be the inverse of that matrix.

Multiplying two polynomials of degree n modulo the defining polynomial now means: Evaluate the two factors at the given n points. For each point, multiply the two evaluations. Multiply this with A, and read off the coefficients of the product.

So, this can be used for our polynomials of matrices. If my arguments work, then we would be down to n multiplications over GF(p) for one multiplication over `GF(p^n)`. And comparing the figures of Linbox over `GF(17)` and Magma over `GF(17^2)`, we would be competitive with Magma.



---

archive/issue_comments_138680.json:
```json
{
    "body": "Sorry, in my previous post, I totally forgot to include \"reduction modulo defining polynomial\". Anyway, I am now trying to produce some code.",
    "created_at": "2011-12-18T16:21:02Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138680",
    "user": "SimonKing"
}
```

Sorry, in my previous post, I totally forgot to include "reduction modulo defining polynomial". Anyway, I am now trying to produce some code.



---

archive/issue_comments_138681.json:
```json
{
    "body": "Attachment [toom.py](tarball://root/attachments/some-uuid/ticket12177/toom.py) by malb created at 2011-12-19 14:54:48\n\nproof of concept",
    "created_at": "2011-12-19T14:54:48Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138681",
    "user": "malb"
}
```

Attachment [toom.py](tarball://root/attachments/some-uuid/ticket12177/toom.py) by malb created at 2011-12-19 14:54:48

proof of concept



---

archive/issue_comments_138682.json:
```json
{
    "body": "I attached a proof of concept that the Toom thingy works.",
    "created_at": "2011-12-19T14:55:26Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138682",
    "user": "malb"
}
```

I attached a proof of concept that the Toom thingy works.



---

archive/issue_comments_138683.json:
```json
{
    "body": "I attached a patch with template classes implementing this polynomial with matrix coefficients representation. There is also an implementation of the naive multiplication algorithm for GF(p<sup>k</sup>) to demonstrate FFLAS calls.\n\nTimings for the naive multiplication from Martin's example above (comment:2) for p=17 are:\n\n\n```\nk     time\n2     4.51    \n3    10.28\n4    19.17\n```\n\n\nThat's n<sup>2</sup> coefficient matrix multiplications with some overhead to handle the rollover with the minimal polynomial. We should look at a better algorithm. :)",
    "created_at": "2011-12-19T15:56:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138683",
    "user": "burcin"
}
```

I attached a patch with template classes implementing this polynomial with matrix coefficients representation. There is also an implementation of the naive multiplication algorithm for GF(p<sup>k</sup>) to demonstrate FFLAS calls.

Timings for the naive multiplication from Martin's example above (comment:2) for p=17 are:


```
k     time
2     4.51    
3    10.28
4    19.17
```


That's n<sup>2</sup> coefficient matrix multiplications with some overhead to handle the rollover with the minimal polynomial. We should look at a better algorithm. :)



---

archive/issue_comments_138684.json:
```json
{
    "body": "Replying to [comment:7 burcin]:\n> That's n<sup>2</sup> coefficient matrix multiplications with some overhead to handle the rollover with the minimal polynomial. We should look at a better algorithm. :)\n\n... which probably is Toom. Martin, do you have code for Toom multiplication?",
    "created_at": "2011-12-19T16:02:19Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138684",
    "user": "SimonKing"
}
```

Replying to [comment:7 burcin]:
> That's n<sup>2</sup> coefficient matrix multiplications with some overhead to handle the rollover with the minimal polynomial. We should look at a better algorithm. :)

... which probably is Toom. Martin, do you have code for Toom multiplication?



---

archive/issue_comments_138685.json:
```json
{
    "body": "I've attached it, but it's going to be slow if you try it because a*A for a in the field there is no special code in Sage yet.",
    "created_at": "2011-12-19T17:12:27Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138685",
    "user": "malb"
}
```

I've attached it, but it's going to be slow if you try it because a*A for a in the field there is no special code in Sage yet.



---

archive/issue_comments_138686.json:
```json
{
    "body": "Attachment [toom_matrix.py](tarball://root/attachments/some-uuid/ticket12177/toom_matrix.py) by SimonKing created at 2011-12-20 14:15:25\n\nAnother proof of concept",
    "created_at": "2011-12-20T14:15:25Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138686",
    "user": "SimonKing"
}
```

Attachment [toom_matrix.py](tarball://root/attachments/some-uuid/ticket12177/toom_matrix.py) by SimonKing created at 2011-12-20 14:15:25

Another proof of concept



---

archive/issue_comments_138687.json:
```json
{
    "body": "Martin, in your Toom proof of concept, aren't you evaluating at too few points? We start with polynomials of degree less than the degree of the field extension. But the product will have (before reduction) twice that degree.\n\nHence, instead of `FWD = Matrix(K, l, l, [K(i**j) for i in range(l) for j in range(l)])`, I think we need\n\n```\n    FWD = Matrix(K, l, l, [K(i**j) for i in range(l) for j in range(2*l-1)])\n```\n\nThe following lines are to be changed accordingly.\n\nWe could of course include the reduction to degree l-1 into the matrix `BCK`. I did something along these lines in [attachment:toom_matrix.py], which returns the equivalent of `FWD` and `BCK` - perhaps you can plug it into your code?",
    "created_at": "2011-12-20T14:16:32Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138687",
    "user": "SimonKing"
}
```

Martin, in your Toom proof of concept, aren't you evaluating at too few points? We start with polynomials of degree less than the degree of the field extension. But the product will have (before reduction) twice that degree.

Hence, instead of `FWD = Matrix(K, l, l, [K(i**j) for i in range(l) for j in range(l)])`, I think we need

```
    FWD = Matrix(K, l, l, [K(i**j) for i in range(l) for j in range(2*l-1)])
```

The following lines are to be changed accordingly.

We could of course include the reduction to degree l-1 into the matrix `BCK`. I did something along these lines in [attachment:toom_matrix.py], which returns the equivalent of `FWD` and `BCK` - perhaps you can plug it into your code?



---

archive/issue_comments_138688.json:
```json
{
    "body": "Simon, `l = len(A)+len(B)-1` i.e., l is *not* the degree of the inputs. Merging the modular reduction with the interpolation might be a good idea though.",
    "created_at": "2011-12-20T14:28:12Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138688",
    "user": "malb"
}
```

Simon, `l = len(A)+len(B)-1` i.e., l is *not* the degree of the inputs. Merging the modular reduction with the interpolation might be a good idea though.



---

archive/issue_comments_138689.json:
```json
{
    "body": "Replying to [comment:11 malb]:\n> Simon, `l = len(A)+len(B)-1` i.e., l is *not* the degree of the inputs. Merging the modular reduction with the interpolation might be a good idea though.\n\nSorry! I was somehow misreading it as `l = len(A)` - what happened to my eyes?",
    "created_at": "2011-12-20T14:34:07Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138689",
    "user": "SimonKing"
}
```

Replying to [comment:11 malb]:
> Simon, `l = len(A)+len(B)-1` i.e., l is *not* the degree of the inputs. Merging the modular reduction with the interpolation might be a good idea though.

Sorry! I was somehow misreading it as `l = len(A)` - what happened to my eyes?



---

archive/issue_comments_138690.json:
```json
{
    "body": "Attachment [trac_12177-coeff_matrices_template.patch](tarball://root/attachments/some-uuid/ticket12177/trac_12177-coeff_matrices_template.patch) by burcin created at 2011-12-21 11:58:39",
    "created_at": "2011-12-21T11:58:39Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138690",
    "user": "burcin"
}
```

Attachment [trac_12177-coeff_matrices_template.patch](tarball://root/attachments/some-uuid/ticket12177/trac_12177-coeff_matrices_template.patch) by burcin created at 2011-12-21 11:58:39



---

archive/issue_comments_138691.json:
```json
{
    "body": "I refreshed my patch with a version that implements the multi point evaluation approach using FFLAS. It can be reached via the `_multiply_toom()` function:\n\n\n```\nsage: K.<a> = GF(17^6)\nsage: MS = MatrixSpace(K, 2000, 2000)\nsage: from sage.matrix.matrix_modq_dense_float import Matrix_modq_dense_float\nsage: M = Matrix_modq_dense_float(MS, a^5)\nsage: res = M._multiply_toom(M)\n<some debugging output>\n```\n",
    "created_at": "2011-12-21T12:01:38Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138691",
    "user": "burcin"
}
```

I refreshed my patch with a version that implements the multi point evaluation approach using FFLAS. It can be reached via the `_multiply_toom()` function:


```
sage: K.<a> = GF(17^6)
sage: MS = MatrixSpace(K, 2000, 2000)
sage: from sage.matrix.matrix_modq_dense_float import Matrix_modq_dense_float
sage: M = Matrix_modq_dense_float(MS, a^5)
sage: res = M._multiply_toom(M)
<some debugging output>
```




---

archive/issue_comments_138692.json:
```json
{
    "body": "See #9888 for a related discussion.",
    "created_at": "2016-08-17T01:08:37Z",
    "issue": "https://github.com/sagemath/sagetest/issues/12005",
    "type": "issue_comment",
    "url": "https://github.com/sagemath/sagetest/issues/12005#issuecomment-138692",
    "user": "kedlaya"
}
```

See #9888 for a related discussion.
