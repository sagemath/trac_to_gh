# Issue 32658: Improve `_matrix_times_vector_` and `_vector_times_matrix_` for dense rational and integer matrices

Issue created by migration from https://trac.sagemath.org/ticket/32895

Original creator: @kliem

Original creation time: 2021-11-18 16:17:22

CC:  vdelecroix slabbe mjo

We improve multiplication of matrix and vector for flint matrices by using that the rows of the matrix are already stored as vectors.


---

Comment by git created at 2021-11-18 16:17:45

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by nbruin created at 2021-11-18 17:27:07

Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.


---

Comment by @kliem created at 2021-11-18 21:35:15

Thanks for the suggestion. I'll look into it.


---

Comment by git created at 2021-11-18 21:47:23

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2021-11-18 21:52:17

Replying to [comment:5 nbruin]:
> Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.

It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.

I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.

Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.


---

Comment by @kliem created at 2021-11-18 21:53:17

Changing status from new to needs_review.


---

Comment by @kliem created at 2021-11-18 22:30:38

Just for the fun, I played around a bit with parallelization:

This is the `ZZ` part with up to 4 threads, depening on the size (I only have 2 cores):

```
7.11 µs ± 12.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
2.5 µs ± 21.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
170 µs ± 437 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
104 µs ± 539 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 15.7 ms, sys: 0 ns, total: 15.7 ms
Wall time: 9.78 ms
CPU times: user 26.3 ms, sys: 10 µs, total: 26.3 ms
Wall time: 7.69 ms
CPU times: user 2.59 s, sys: 8.09 ms, total: 2.6 s
Wall time: 660 ms
CPU times: user 2.87 s, sys: 3.85 ms, total: 2.87 s
Wall time: 737 ms
```


This could be done with just a few changes:


```diff
diff --git a/src/sage/libs/flint/fmpz.pxd b/src/sage/libs/flint/fmpz.pxd
index 01058d0f13..e44a5b4595 100644
--- a/src/sage/libs/flint/fmpz.pxd
+++ b/src/sage/libs/flint/fmpz.pxd
@@ -97,7 +97,7 @@ cdef extern from "flint_wrap.h":
     void fmpz_mul2_uiui(fmpz_t, fmpz_t, ulong, ulong)
     void fmpz_mul_2exp(fmpz_t, fmpz_t, ulong)
 
-    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t)
+    void fmpz_addmul(fmpz_t, fmpz_t, fmpz_t) nogil
     void fmpz_addmul_ui(fmpz_t, fmpz_t, ulong)
 
     void fmpz_submul(fmpz_t, fmpz_t, fmpz_t)
diff --git a/src/sage/libs/flint/fmpz_vec.pxd b/src/sage/libs/flint/fmpz_vec.pxd
index 87614f973c..813d59672e 100644
--- a/src/sage/libs/flint/fmpz_vec.pxd
+++ b/src/sage/libs/flint/fmpz_vec.pxd
@@ -10,4 +10,4 @@ cdef extern from "flint_wrap.h":
     ulong _fmpz_vec_max_limbs(const fmpz *, slong)
     void _fmpz_vec_scalar_mod_fmpz(fmpz *, fmpz *, long, fmpz_t)
     void _fmpz_vec_scalar_smod_fmpz(fmpz *, fmpz *, long, fmpz_t)
-    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong)
+    void _fmpz_vec_dot(fmpz_t, const fmpz *, const fmpz *, slong) nogil
diff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx
index 9872b02338..b6afa85df0 100644
--- a/src/sage/matrix/matrix_integer_dense.pyx
+++ b/src/sage/matrix/matrix_integer_dense.pyx
@@ -1,8 +1,8 @@
 # -*- coding: utf-8 -*-
-# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS
+# distutils: extra_compile_args = NTL_CFLAGS M4RI_CFLAGS OPENMP_CFLAGS
 # distutils: libraries = iml NTL_LIBRARIES gmp m CBLAS_LIBRARIES
 # distutils: library_dirs = NTL_LIBDIR CBLAS_LIBDIR
-# distutils: extra_link_args = NTL_LIBEXTRA
+# distutils: extra_link_args = NTL_LIBEXTRA OPENMP_CFLAGS
 # distutils: include_dirs = NTL_INCDIR M4RI_INCDIR CBLAS_INCDIR
 """
 Dense matrices over the integer ring
@@ -131,6 +131,7 @@ from .matrix2 import decomp_seq
 from .matrix cimport Matrix
 
 cimport sage.structure.element
+from cython.parallel cimport prange
 
 import sage.matrix.matrix_space as matrix_space
 
@@ -1068,6 +1069,13 @@ cdef class Matrix_integer_dense(Matrix_dense):
         cdef fmpz* w_flint
         cdef fmpz* ans_flint
 
+        cdef int num_threads = 1
+        if self._ncols > 500:
+            num_threads = 2
+        if self._ncols > 5000:
+            num_threads = 4
+
+
         M = self._row_ambient_module()
         w = <Vector_integer_dense> v
         ans = M.zero_vector()
@@ -1089,9 +1097,15 @@ cdef class Matrix_integer_dense(Matrix_dense):
             # So in the inner loop we have very little pointer movement.
             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has
             # no depence on the previous step.
-            for j in range(self._nrows):
-                for i in range(self._ncols):
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+            if num_threads == 1:
+                for j in range(self._nrows):
+                    for i in range(self._ncols):
+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+            else:
+                for j in range(self._nrows):
+                    #for i in range(self._ncols):
+                    for i in prange(self._ncols, num_threads=num_threads, nogil=True):
+                        fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
 
             for i in range(self._ncols):
                 fmpz_get_mpz(ans._entries[i], ans_flint + i)
@@ -1124,6 +1138,13 @@ cdef class Matrix_integer_dense(Matrix_dense):
         cdef Py_ssize_t i, j
         cdef fmpz_t x
         cdef fmpz* w_flint
+        cdef fmpz* ans_flint
+
+        cdef int num_threads = 1
+        if self._ncols > 20:
+            num_threads = 2
+        if self._ncols > 500:
+            num_threads = 4
 
         M = self._column_ambient_module()
         w = <Vector_integer_dense> v
@@ -1131,20 +1152,28 @@ cdef class Matrix_integer_dense(Matrix_dense):
 
         fmpz_init(x)
         w_flint = _fmpz_vec_init(self._ncols)
+        ans_flint = _fmpz_vec_init(self._nrows)
 
         try:
             sig_on()
             for j in range(self._ncols):
                 fmpz_set_mpz(w_flint + j, w._entries[j])
 
+            if num_threads == 1:
+                for i in range(self._nrows):
+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)
+            else:
+                for i in prange(self._nrows, num_threads=num_threads, nogil=True):
+                    _fmpz_vec_dot(ans_flint + i, self._matrix.rows[i], w_flint, self._ncols)
+
             for i in range(self._nrows):
-                _fmpz_vec_dot(x, self._matrix.rows[i], w_flint, self._ncols)
-                fmpz_get_mpz(ans._entries[i], x)
+                fmpz_get_mpz(ans._entries[i], ans_flint + i)
 
             sig_off()
         finally:
             fmpz_clear(x)
             _fmpz_vec_clear(w_flint, self._ncols)
+            _fmpz_vec_clear(ans_flint, self._nrows)
 
         return ans
```


This is probably just to advertize that `prange` can easily be used nowadays.


---

Comment by nbruin created at 2021-11-19 01:13:13

Replying to [comment:9 gh-kliem]:
> Replying to [comment:5 nbruin]:
> > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.
> 
> It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.
> 
> I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.
> 
> Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.

In that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already "hot" because it is still in cache from the computation that produced it?

Parallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.


---

Comment by @kliem created at 2021-11-19 07:19:17

Now here are some timings of `_vector_times_matrix_` with some changes that we have some data:

As it is:


```
7.31 µs ± 30.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
174 µs ± 346 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 16.2 ms, sys: 2 µs, total: 16.2 ms
Wall time: 16.3 ms
CPU times: user 1.54 s, sys: 0 ns, total: 1.54 s
Wall time: 1.54 s
```


Reading in the vectors as if they were transposed


```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[i] + j)
```



```
7.5 µs ± 63 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
171 µs ± 256 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 19.7 ms, sys: 0 ns, total: 19.7 ms
Wall time: 19.7 ms
CPU times: user 3.36 s, sys: 60 µs, total: 3.36 s
Wall time: 3.36 s
```


Also moving in the vector:


```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + i, w_flint + i, self._matrix.rows[i] + j)
```



```
7.02 µs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
204 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 25.6 ms, sys: 0 ns, total: 25.6 ms
Wall time: 25.6 ms
CPU times: user 3.91 s, sys: 1e+03 ns, total: 3.91 s
Wall time: 3.91 s
```


Having small movements, but lots of dependencies:


```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + j, w_flint + j, self._matrix.rows[j] + i)
```



```
7.37 µs ± 34.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
146 µs ± 497 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 12.8 ms, sys: 0 ns, total: 12.8 ms
Wall time: 12.8 ms
CPU times: user 1.24 s, sys: 3.94 ms, total: 1.24 s
Wall time: 1.24 s
```


A bit more movement with dependency:


```diff
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
+                    fmpz_addmul(ans_flint + j, w_flint + i, self._matrix.rows[j] + i)
```



```
6.87 µs ± 31.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
202 µs ± 242 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
CPU times: user 21.1 ms, sys: 17 µs, total: 21.1 ms
Wall time: 21.1 ms
CPU times: user 2.09 s, sys: 0 ns, total: 2.09 s
Wall time: 2.09 s
```


Now using `_fmpz_vec_dot` instead of our manual version:


```
7.19 µs ± 34 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
188 µs ± 2.71 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms
Wall time: 19.5 ms
CPU times: user 1.9 s, sys: 43 µs, total: 1.9 s
Wall time: 1.9 s
```


Apparently using `fmpz_mat_entry(self._matrix, j, i)` instead of `self._matrix.rows[j] + i` makes no difference.

And here are timings, that jump around more but avoid creating the answer vector:


```diff
diff --git a/src/sage/matrix/matrix_integer_dense.pyx b/src/sage/matrix/matrix_integer_dense.pyx
index 9872b02338..f2c6d0b01e 100644
--- a/src/sage/matrix/matrix_integer_dense.pyx
+++ b/src/sage/matrix/matrix_integer_dense.pyx
@@ -1073,33 +1073,29 @@ cdef class Matrix_integer_dense(Matrix_dense):
         ans = M.zero_vector()
 
         w_flint = _fmpz_vec_init(self._nrows)
-        ans_flint = _fmpz_vec_init(self._ncols)
+        fmpz_init(x)
 
         try:
             sig_on()
             for j in range(self._nrows):
                 fmpz_set_mpz(w_flint + j, w._entries[j])
 
-            for i in range(self._ncols):
-                fmpz_zero(ans_flint + i)
-
             # The order is crucial:
             # ``self._matrix.rows[j] + i`` is right next to ``self._matrix[j] + i + 1``
             # but far away from ``self._matrix[j + 1] + i``.
             # So in the inner loop we have very little pointer movement.
             # Even better: Unlike with ``_matrix_times_vector`` the inner loop has
             # no depence on the previous step.
-            for j in range(self._nrows):
-                for i in range(self._ncols):
-                    fmpz_addmul(ans_flint + i, w_flint + j, self._matrix.rows[j] + i)
-
             for i in range(self._ncols):
-                fmpz_get_mpz(ans._entries[i], ans_flint + i)
+                fmpz_zero(x)
+                for j in range(self._nrows):
+                    fmpz_addmul(x, w_flint + j, self._matrix.rows[j] + i)
+                fmpz_get_mpz(ans._entries[i], x)
 
             sig_off()
         finally:
             _fmpz_vec_clear(w_flint, self._nrows)
-            _fmpz_vec_clear(ans_flint, self._ncols)
+            fmpz_clear(x)
```



```
7.02 µs ± 19.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
199 µs ± 4.07 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
CPU times: user 24.8 ms, sys: 5 µs, total: 24.8 ms
Wall time: 24.8 ms
CPU times: user 3.8 s, sys: 16 µs, total: 3.8 s
Wall time: 3.8 s
```


Except for the first and last version, all other versions are very incorrect and just here to show what effect different changes have.

What really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:


```
sage: M = random_matrix(ZZ, 10, 10)
sage: %timeit M._column_ambient_module().zero_vector()
612 ns ± 2.81 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
sage: %timeit M._row_ambient_module().zero_vector()
4.49 µs ± 32.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```



---

Comment by @kliem created at 2021-11-19 07:28:00

Replying to [comment:12 nbruin]:
> Replying to [comment:9 gh-kliem]:
> > Replying to [comment:5 nbruin]:
> > > Your timings suggest that for a very important case: matrix/vector products over ZZ of dimension 100 (and presumably smaller?) get MUCH slower than it was before. That's actually something that ends up in inner loops quite a bit, so it would be worth timing that case more carefully and, if it is indeed slower, use a cross-over to use a different, faster, implementation for small dimensions.
> > 
> > It's now almost up to the original speed. About 1 percent slow down. I would say this is not worth having a backup version.
> > 
> > I was thinking a bit this evening that doing first a transposition can't be it. In the end, flint does not do anything special. It just calculates it plainly. Even the dot product does not have any secrets. The reason `_vector_times_matrix_` took so long is apparently only because of the messed up order. The entries of the matrix are stored in rows. You don't want to jump around in memory in tight loops, so it makes sense to change the order to avoid this.
> > 
> > Now `_vector_times_matrix_` is even faster than `_matrix_times_vector_`, because in the tight loop there is no dependency. The loop can just be executed regardless and there is no waiting on results from previous steps.
> 
> In that case it gets even more important to do careful timings: what kind of cache locality are you aiming for? Are you hoping the matrix and/or vector get prefetched? Do you expect the matrix to already be in cache and then the vector to get fetched? or the other way around? Or is the vector already "hot" because it is still in cache from the computation that produced it?
> 
> Parallelism would of course be nice, but I think it's also important to have an uncompromised single-thread implementation: in a larger computational project it's often much more advantageous to use some embarrassingly parallelism (technical term) higher up to get a better speed-up factor.

For the small cases the caching and jumping around doesn't matter, as you can see above.
For the big cases it matters a lot. The really fast caches aren't as big as a matrix. So it would be good to reuse stuff in the cache, before loading new things. If we do things in a nice order, the cache can be filled while we are going. If we do it in a bad order, it cannot be refilled while we are going.

Yes, I know that parallelisism is usually better higher up. I was just playing around. Usually you want to treat lots of vectors, matrices or other objects. A matrix and vector in which the product is hard to compute by itself hardly fits in memory.


---

Comment by @kliem created at 2021-11-19 08:04:44

Changing status from needs_review to needs_work.


---

Comment by git created at 2021-11-19 18:57:05

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2021-11-19 18:58:49

Changing status from needs_work to needs_review.


---

Comment by nbruin created at 2021-11-19 22:32:48

Replying to [comment:13 gh-kliem]:
> What really bothers me, is that the second version is so much faster for the small case. But the problems are never were you think they are:
> 
> {{{
> sage: M = random_matrix(ZZ, 10, 10)
> sage: %timeit M._column_ambient_module().zero_vector()
> 612 ns ± 2.81 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
> sage: %timeit M._row_ambient_module().zero_vector()
> 4.49 µs ± 32.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
> }}}

Yes, that's ridiculously bad, but we can see where that's coming from: there's a difference in implementation between the two:

```
sage: M._row_ambient_module??                                                                                                              
Source:
    def _row_ambient_module(self, base_ring=None):
        if base_ring is None:
            base_ring = self.base_ring()
        x = self.fetch('row_ambient_module_%s'%base_ring)
        if not x is None:
            return x
        x = sage.modules.free_module.FreeModule(base_ring, self.ncols(), sparse=self.is_sparse())
        self.cache('row_ambient_module',x)
        return x
File:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx
Type:   builtin_function_or_method
sage: M._column_ambient_module??                                                                                                           
Source:
    def _column_ambient_module(self):
        x = self.fetch('column_ambient_module')
        if not x is None:
            return x
        x = sage.modules.free_module.FreeModule(self.base_ring(), self.nrows(),
                                                sparse=self.is_sparse())
        self.cache('column_ambient_module',x)
        return x
File:   /usr/local/sage/sage-git/local/lib64/python3.9/site-packages/sage/matrix/matrix2.pyx
Type:   builtin_function_or_method
```

As you can see, `_row_ambient_module` is doing string formatting; even in its fast path! So, I think what needs to happen here:
 - identify if `_row_ambient_module` even needs a `base_ring` optional argument. It seems to me this is not desirable. If it can simply be removed, that should happen.
 - determine testing the `None` argument (or having it!) is measurable. If so, split out between a fast path routine without a base ring optional parameter and another one that does allow it. It looks like the only place where this gets used is in `row_module` just below it, so that could just do what it needs to do there (cached or not)
 - make sure the caching actually WORKS! not that the code presently writes a different cache entry than what it uses. So I think the main time difference you're measuring at the moment is the expensive `UniqueRepresentation` lookup  that happens in the `FreeModule` invocation that happens every time.

I suspect that fixing the third one will bring it much more in line already but that working with a string constant for cache entry lookup will still be measurable. Certainly, we should not be using string formatting if we're just concatenating two strings either (and note that getting a string representation of a base ring could be quite expensive too)

Furthermore, I think these routines should be `cdef` accessible as well.

Finally, it might be worth looking into cached_method lookups for these. Especially for parameterless methods, those should be pretty efficient.


---

Comment by @kliem created at 2021-11-19 22:38:16

Let's move this discussion to #32901.


---

Comment by git created at 2021-11-30 08:14:18

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2021-12-07 11:17:52

There are failing doctest. Not directly related to this ticket, but appearently something goes wrong:

On develop:


```
sage: dP6 = toric_varieties.dP6()
sage: Kc = dP6.Kaehler_cone()
sage: D = Kc.ray(1)
sage: p = D.parent()
sage: M = p._lift_matrix
sage: M*D
(1, 0, 0, 0, 0, 1)
sage: D*M.transpose()
(1, 2, 0, 0, -1, 0)
sage: vector(list(D))*M.transpose()
(1, 0, 0, 0, 0, 1)
```


Now I implement an innocent method for `Matrix_integer_dense`:


```
    cdef _matrix_times_vector_(self, Vector v):
        cdef Matrix_integer_dense transposed = self.transpose()
        return transposed._vector_times_matrix_(v)
```


And the result is different now:


```
sage: dP6 = toric_varieties.dP6()
sage: Kc = dP6.Kaehler_cone()
sage: D = Kc.ray(1)
sage: p = D.parent()
sage: M = p._lift_matrix
sage: M*D
(1, 2, 0, 0, -1, 0)
```


Apparently this is the problem:


```
sage: D.base_ring()
Integer Ring
```


So `D` is set up wrong. It inherits from `Vector_rational_dense` but the base ring is wrong and the parent is at least strange. I think the parent should have the same base ring as the element.

The above failure is due to an incorrect type case.


---

Comment by @kliem created at 2021-12-07 11:17:52

Changing status from needs_review to needs_work.


---

Comment by @kliem created at 2021-12-07 13:25:56

Fixing this failure is the job of #32991.

For this ticket, I'll just do a workaround for the doctest.


---

Comment by git created at 2021-12-07 13:32:29

Branch pushed to git repo; I updated commit sha1. This was a forced push. New commits:


---

Comment by @kliem created at 2021-12-07 13:32:35

Changing status from needs_work to needs_review.


---

Comment by @kliem created at 2021-12-10 07:29:09

Ready for review. The bots are morally green now.


---

Comment by vdelecroix created at 2021-12-27 15:45:36

Could you rebase on #32984?


---

Comment by vdelecroix created at 2021-12-27 15:46:48

Shouldn't the `sig_off` be inside the `finally`?


---

Comment by @kliem created at 2021-12-27 16:49:11

Replying to [comment:29 vdelecroix]:
> Shouldn't the `sig_off` be inside the `finally`?

I don't think so. If there is an error, it will jump back to `sig_on` and raise an error there. In this case it should not call `sig_off`.


---

Comment by @kliem created at 2021-12-27 16:51:42

Replying to [comment:28 vdelecroix]:
> Could you rebase on #32984?

Can't do that ATM. Once #32984 is positively reviewed, feel free to do so.


---

Comment by @kliem created at 2021-12-27 16:52:54

Or just put this here on needs work. Whatever works.


---

Comment by vdelecroix created at 2021-12-28 13:08:25

This following tests

```
            sage: A = random_matrix(QQ, 100)
            sage: v = random_vector(QQ, 100)
            sage: A*v == v*A.transpose()
            True
```

would better be part of a generic `TestSuite` rather than inside doctests. Though, in this case it would make sense to parametrize the size. `100 x 100` over rationals is fine but over `QQ[x]` this is unlikely to finish.


---

Comment by vdelecroix created at 2021-12-28 16:29:40

Changing status from needs_review to needs_info.


---

Comment by vdelecroix created at 2021-12-28 16:29:40

I don't think that having this kind of optimization on flint data structures inside sage is a good idea. The underlying design of flint is to only expose type names (ie `fmpz_mat_t` and `fmpq_mat_t`) and interfaces (ie functions `fmpz_mat_mul` and `fmpq_mat_mul`). It would make more sense to make a merge request to [flint developers](https://github.com/wbhart/flint2) and just use plain flint code inside sage. Doing so would
- benefit to all flint users and not only sagemath users
- only use the public API of flint (so that flint upgrades will remain smooth)


---

Comment by git created at 2022-01-03 21:03:04

Branch pushed to git repo; I updated commit sha1. New commits:


---

Comment by @kliem created at 2022-01-03 21:10:36

I removed the explicit dependence on the flint structures.

I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.

If we use a brand new feature of flint, it means lots of compile time for many people.

Putting this into flint, might not be what we desire:
- We use GMP-vectors instead of FLINT-vectors.
- The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.

Those are some of the issues, I can think of. I sure can try putting it in FLINT.

The doctest is mainly there for coverage (and of course as sanity check).


---

Comment by @kliem created at 2022-01-03 21:10:36

Changing status from needs_info to needs_review.


---

Comment by vdelecroix created at 2022-01-04 09:40:22

Replying to [comment:36 gh-kliem]:
> I removed the explicit dependence on the flint structures.
> 
> I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.
> 
> If we use a brand new feature of flint, it means lots of compile time for many people.

Why is that ? The inclusion in sage might wait for a flint release to appear.
 
> Putting this into flint, might not be what we desire:
> - We use GMP-vectors instead of FLINT-vectors

Might make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.

Alternatively, the following could directly be proposed upstream

```
void fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)
```


> - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.

`fmpz_set_mpz` might allocate memory...

> Those are some of the issues, I can think of. I sure can try putting it in FLINT.
> 
> The doctest is mainly there for coverage (and of course as sanity check).


---

Comment by @kliem created at 2022-01-05 07:11:01

Replying to [comment:37 vdelecroix]:
> Replying to [comment:36 gh-kliem]:
> > I removed the explicit dependence on the flint structures.
> > 
> > I don't know how good this would fit into flint and whether we should force and brand new flint version because of it.
> > 
> > If we use a brand new feature of flint, it means lots of compile time for many people.
> 
> Why is that ? The inclusion in sage might wait for a flint release to appear.

If we use new functions that were introduced in the latest version of flint, we will have to raise the requirements in `spkg-configure`. Currently, debian stable has flint 2.6.3, which will be accepted by sage (and the situation should be similar for other systems). If we raise this to the most recent version, almost everyone building sage will have to build flint as well for a while. That is fine, if there is some great new feature, but `matrix_times_vector` can be achieved with the existing FLINT Api without much work. This would be what I would expect `matrix_times_vector` to take care of:


```diff
+            for i in range(self._nrows):
+                fmpz_zero(x)
+                for j in range(self._ncols):
+                    fmpz_addmul(x, w_flint + j, fmpz_mat_entry(self._matrix, i, j))
+                fmpz_get_mpz(ans._entries[i], x)
```


>  
> > Putting this into flint, might not be what we desire:
> > - We use GMP-vectors instead of FLINT-vectors
> 
> Might make sense to actually switch to flint vectors. Or at least have an alternative implementation using flint vectors inside sage. That way we could directly call flint functions for the product : https://flintlib.org/doc/fmpz_mat.html#c.fmpz_mat_mul_fmpz_vec.
> 
> Alternatively, the following could directly be proposed upstream
> {{{
> void fmpz_mat_mul_mpz_vec(fmpz * c, const fmpz_mat_t A, const mpz * b, slong blen)
> }}}
> 
> > - The methods allocate memory. Allocating memory within `sig_on` and `sig_off` is not a good idea.
> 
> `fmpz_set_mpz` might allocate memory...

That is different though. The memory is accounted for, and when calling `mpz_clear` it will be released (this requires that allocating memory and accounting for it is atomic, I don't know if this is the case but even if not and interrupt in between is not very likely). Whereas something as


```
sig_on()
void* foo = malloc(100)
# Do something
free(foo)
sig_off()
```

is not clean at all. `free(foo)` will not be called in case of interrupt. If we wrap a function that behaves like this with `sig_on`, `sig_off` an interrupt will usually result in memory leak and there is nothing we can do about it.

However, if all flint does is multiplication of a flint matrix and a flint vector and assign this to a flint vector, it will work just fine (or as fine as other code).
> 
> > Those are some of the issues, I can think of. I sure can try putting it in FLINT.
> > 
> > The doctest is mainly there for coverage (and of course as sanity check).
